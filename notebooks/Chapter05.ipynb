{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch verison: 2.5.1\n",
      "numpy version: 2.2.3\n",
      "MPS backend: True\n",
      "Using device: mps\n",
      "Matrix multiplication took 0.1227 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f'torch verison: {torch.__version__}')\n",
    "import numpy\n",
    "print(f'numpy version: {numpy.__version__}')\n",
    "print(f'MPS backend: {torch.backends.mps.is_available()}')\n",
    "\n",
    "import time\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Simple matrix multiplication test\n",
    "x = torch.rand(1000, 1000).to(device)\n",
    "y = torch.rand(1000, 1000).to(device)\n",
    "\n",
    "start = time.time()\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"Matrix multiplication took {time.time() - start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Temperature (°Celsius)')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(t_u, t_c)\n",
    "plt.xlabel('Measurement')\n",
    "plt.ylabel('Temperature (°Celsius)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.001\n",
    "loss_rate_of_change_w = \\\n",
    "    (loss_fn(model(t_u, w + delta, b), t_c) -\n",
    "     loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = \\\n",
    "    (loss_fn(model(t_u, w, b + delta), t_c) -\n",
    "     loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3156.7336, -4947.1113, -5150.9663, -7251.5581, -4982.5645, -4326.6836,\n",
       "        -2997.1951, -1924.7407, -4282.3672, -5345.9580, -6055.0186])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "\n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "        print(f'    Params: {params}')\n",
    "        print(f'    Grad:   {grad}')\n",
    "\n",
    "    return params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "    Params: tensor([-44.1730,  -0.8260])\n",
      "    Grad:   tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 5802484.500000\n",
      "    Params: tensor([2568.4011,   45.1637])\n",
      "    Grad:   tensor([-261257.4062,   -4598.9702])\n",
      "Epoch 3, Loss 19408029696.000000\n",
      "    Params: tensor([-148527.7344,   -2616.3931])\n",
      "    Grad:   tensor([15109614.0000,   266155.6875])\n",
      "Epoch 4, Loss 64915905708032.000000\n",
      "    Params: tensor([8589999.0000,  151310.8906])\n",
      "    Grad:   tensor([-8.7385e+08, -1.5393e+07])\n",
      "Epoch 5, Loss 217130525461053440.000000\n",
      "    Params: tensor([-4.9680e+08, -8.7510e+06])\n",
      "    Grad:   tensor([5.0539e+10, 8.9023e+08])\n",
      "Epoch 6, Loss 726257583152928129024.000000\n",
      "    Params: tensor([2.8732e+10, 5.0610e+08])\n",
      "    Grad:   tensor([-2.9229e+12, -5.1486e+10])\n",
      "Epoch 7, Loss 2429183416467662896627712.000000\n",
      "    Params: tensor([-1.6617e+12, -2.9270e+10])\n",
      "    Grad:   tensor([1.6904e+14, 2.9776e+12])\n",
      "Epoch 8, Loss 8125122549611731432050262016.000000\n",
      "    Params: tensor([9.6102e+13, 1.6928e+12])\n",
      "    Grad:   tensor([-9.7764e+15, -1.7221e+14])\n",
      "Epoch 9, Loss 27176882120842590626938030653440.000000\n",
      "    Params: tensor([-5.5580e+15, -9.7903e+13])\n",
      "    Grad:   tensor([5.6541e+17, 9.9596e+15])\n",
      "Epoch 10, Loss 90901105189019073810297959556841472.000000\n",
      "    Params: tensor([3.2144e+17, 5.6621e+15])\n",
      "    Grad:   tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 11, Loss inf\n",
      "    Params: tensor([-1.8590e+19, -3.2746e+17])\n",
      "    Grad:   tensor([1.8912e+21, 3.3313e+19])\n",
      "Epoch 12, Loss inf\n",
      "    Params: tensor([1.0752e+21, 1.8939e+19])\n",
      "    Grad:   tensor([-1.0937e+23, -1.9266e+21])\n",
      "Epoch 13, Loss inf\n",
      "    Params: tensor([-6.2181e+22, -1.0953e+21])\n",
      "    Grad:   tensor([6.3256e+24, 1.1142e+23])\n",
      "Epoch 14, Loss inf\n",
      "    Params: tensor([3.5962e+24, 6.3346e+22])\n",
      "    Grad:   tensor([-3.6584e+26, -6.4441e+24])\n",
      "Epoch 15, Loss inf\n",
      "    Params: tensor([-2.0798e+26, -3.6636e+24])\n",
      "    Grad:   tensor([2.1158e+28, 3.7269e+26])\n",
      "Epoch 16, Loss inf\n",
      "    Params: tensor([1.2028e+28, 2.1188e+26])\n",
      "    Grad:   tensor([-1.2236e+30, -2.1554e+28])\n",
      "Epoch 17, Loss inf\n",
      "    Params: tensor([-6.9566e+29, -1.2254e+28])\n",
      "    Grad:   tensor([7.0769e+31, 1.2466e+30])\n",
      "Epoch 18, Loss inf\n",
      "    Params: tensor([4.0233e+31, 7.0869e+29])\n",
      "    Grad:   tensor([-4.0929e+33, -7.2095e+31])\n",
      "Epoch 19, Loss inf\n",
      "    Params: tensor([-2.3268e+33, -4.0987e+31])\n",
      "    Grad:   tensor([2.3671e+35, 4.1695e+33])\n",
      "Epoch 20, Loss inf\n",
      "    Params: tensor([1.3457e+35, 2.3704e+33])\n",
      "    Grad:   tensor([-1.3690e+37, -2.4114e+35])\n",
      "Epoch 21, Loss inf\n",
      "    Params: tensor([       -inf, -1.3709e+35])\n",
      "    Grad:   tensor([       inf, 1.3946e+37])\n",
      "Epoch 22, Loss inf\n",
      "    Params: tensor([nan, inf])\n",
      "    Grad:   tensor([-inf, -inf])\n",
      "Epoch 23, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 24, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 25, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 26, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 27, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 28, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 29, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 30, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 31, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 32, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 33, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 34, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 35, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 36, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 37, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 38, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 39, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 40, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 41, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 42, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 43, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 44, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 45, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 46, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 47, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 48, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 49, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 50, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 51, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 52, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 53, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 54, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 55, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 56, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 57, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 58, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 59, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 60, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 61, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 62, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 63, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 64, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 65, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 66, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 67, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 68, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 69, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 70, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 71, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 72, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 73, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 74, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 75, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 76, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 77, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 78, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 79, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 80, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 81, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 82, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 83, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 84, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 85, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 86, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 87, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 88, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 89, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 90, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 91, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 92, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 93, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 94, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 95, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 96, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 97, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 98, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 99, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n",
      "Epoch 100, Loss nan\n",
      "    Params: tensor([nan, nan])\n",
      "    Grad:   tensor([nan, nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "    Params: tensor([ 0.5483, -0.0083])\n",
      "    Grad:   tensor([4517.2964,   82.6000])\n",
      "Epoch 2, Loss 323.090515\n",
      "    Params: tensor([ 0.3623, -0.0118])\n",
      "    Grad:   tensor([1859.5493,   35.7843])\n",
      "Epoch 3, Loss 78.929634\n",
      "    Params: tensor([ 0.2858, -0.0135])\n",
      "    Grad:   tensor([765.4666,  16.5122])\n",
      "Epoch 4, Loss 37.552845\n",
      "    Params: tensor([ 0.2543, -0.0143])\n",
      "    Grad:   tensor([315.0790,   8.5787])\n",
      "Epoch 5, Loss 30.540283\n",
      "    Params: tensor([ 0.2413, -0.0149])\n",
      "    Grad:   tensor([129.6733,   5.3127])\n",
      "Epoch 6, Loss 29.351154\n",
      "    Params: tensor([ 0.2360, -0.0153])\n",
      "    Grad:   tensor([53.3495,  3.9682])\n",
      "Epoch 7, Loss 29.148884\n",
      "    Params: tensor([ 0.2338, -0.0156])\n",
      "    Grad:   tensor([21.9304,  3.4148])\n",
      "Epoch 8, Loss 29.113848\n",
      "    Params: tensor([ 0.2329, -0.0159])\n",
      "    Grad:   tensor([8.9964, 3.1869])\n",
      "Epoch 9, Loss 29.107145\n",
      "    Params: tensor([ 0.2325, -0.0162])\n",
      "    Grad:   tensor([3.6721, 3.0930])\n",
      "Epoch 10, Loss 29.105247\n",
      "    Params: tensor([ 0.2324, -0.0166])\n",
      "    Grad:   tensor([1.4803, 3.0544])\n",
      "Epoch 11, Loss 29.104168\n",
      "    Params: tensor([ 0.2323, -0.0169])\n",
      "    Grad:   tensor([0.5781, 3.0384])\n",
      "Epoch 12, Loss 29.103222\n",
      "    Params: tensor([ 0.2323, -0.0172])\n",
      "    Grad:   tensor([0.2066, 3.0318])\n",
      "Epoch 13, Loss 29.102295\n",
      "    Params: tensor([ 0.2323, -0.0175])\n",
      "    Grad:   tensor([0.0537, 3.0291])\n",
      "Epoch 14, Loss 29.101379\n",
      "    Params: tensor([ 0.2323, -0.0178])\n",
      "    Grad:   tensor([-0.0093,  3.0279])\n",
      "Epoch 15, Loss 29.100466\n",
      "    Params: tensor([ 0.2323, -0.0181])\n",
      "    Grad:   tensor([-0.0353,  3.0274])\n",
      "Epoch 16, Loss 29.099548\n",
      "    Params: tensor([ 0.2323, -0.0184])\n",
      "    Grad:   tensor([-0.0459,  3.0272])\n",
      "Epoch 17, Loss 29.098631\n",
      "    Params: tensor([ 0.2323, -0.0187])\n",
      "    Grad:   tensor([-0.0502,  3.0270])\n",
      "Epoch 18, Loss 29.097717\n",
      "    Params: tensor([ 0.2323, -0.0190])\n",
      "    Grad:   tensor([-0.0520,  3.0270])\n",
      "Epoch 19, Loss 29.096796\n",
      "    Params: tensor([ 0.2323, -0.0193])\n",
      "    Grad:   tensor([-0.0528,  3.0269])\n",
      "Epoch 20, Loss 29.095881\n",
      "    Params: tensor([ 0.2323, -0.0196])\n",
      "    Grad:   tensor([-0.0531,  3.0268])\n",
      "Epoch 21, Loss 29.094959\n",
      "    Params: tensor([ 0.2323, -0.0199])\n",
      "    Grad:   tensor([-0.0533,  3.0268])\n",
      "Epoch 22, Loss 29.094049\n",
      "    Params: tensor([ 0.2323, -0.0202])\n",
      "    Grad:   tensor([-0.0533,  3.0267])\n",
      "Epoch 23, Loss 29.093134\n",
      "    Params: tensor([ 0.2323, -0.0205])\n",
      "    Grad:   tensor([-0.0533,  3.0267])\n",
      "Epoch 24, Loss 29.092216\n",
      "    Params: tensor([ 0.2323, -0.0208])\n",
      "    Grad:   tensor([-0.0533,  3.0266])\n",
      "Epoch 25, Loss 29.091301\n",
      "    Params: tensor([ 0.2323, -0.0211])\n",
      "    Grad:   tensor([-0.0533,  3.0266])\n",
      "Epoch 26, Loss 29.090385\n",
      "    Params: tensor([ 0.2323, -0.0214])\n",
      "    Grad:   tensor([-0.0533,  3.0265])\n",
      "Epoch 27, Loss 29.089464\n",
      "    Params: tensor([ 0.2323, -0.0217])\n",
      "    Grad:   tensor([-0.0533,  3.0265])\n",
      "Epoch 28, Loss 29.088551\n",
      "    Params: tensor([ 0.2323, -0.0220])\n",
      "    Grad:   tensor([-0.0532,  3.0264])\n",
      "Epoch 29, Loss 29.087635\n",
      "    Params: tensor([ 0.2323, -0.0223])\n",
      "    Grad:   tensor([-0.0533,  3.0264])\n",
      "Epoch 30, Loss 29.086714\n",
      "    Params: tensor([ 0.2323, -0.0226])\n",
      "    Grad:   tensor([-0.0533,  3.0263])\n",
      "Epoch 31, Loss 29.085804\n",
      "    Params: tensor([ 0.2324, -0.0229])\n",
      "    Grad:   tensor([-0.0532,  3.0262])\n",
      "Epoch 32, Loss 29.084888\n",
      "    Params: tensor([ 0.2324, -0.0232])\n",
      "    Grad:   tensor([-0.0533,  3.0262])\n",
      "Epoch 33, Loss 29.083967\n",
      "    Params: tensor([ 0.2324, -0.0235])\n",
      "    Grad:   tensor([-0.0533,  3.0261])\n",
      "Epoch 34, Loss 29.083057\n",
      "    Params: tensor([ 0.2324, -0.0238])\n",
      "    Grad:   tensor([-0.0533,  3.0261])\n",
      "Epoch 35, Loss 29.082142\n",
      "    Params: tensor([ 0.2324, -0.0241])\n",
      "    Grad:   tensor([-0.0532,  3.0260])\n",
      "Epoch 36, Loss 29.081221\n",
      "    Params: tensor([ 0.2324, -0.0244])\n",
      "    Grad:   tensor([-0.0533,  3.0260])\n",
      "Epoch 37, Loss 29.080309\n",
      "    Params: tensor([ 0.2324, -0.0247])\n",
      "    Grad:   tensor([-0.0533,  3.0259])\n",
      "Epoch 38, Loss 29.079390\n",
      "    Params: tensor([ 0.2324, -0.0250])\n",
      "    Grad:   tensor([-0.0532,  3.0259])\n",
      "Epoch 39, Loss 29.078474\n",
      "    Params: tensor([ 0.2324, -0.0253])\n",
      "    Grad:   tensor([-0.0533,  3.0258])\n",
      "Epoch 40, Loss 29.077562\n",
      "    Params: tensor([ 0.2324, -0.0256])\n",
      "    Grad:   tensor([-0.0533,  3.0258])\n",
      "Epoch 41, Loss 29.076649\n",
      "    Params: tensor([ 0.2324, -0.0259])\n",
      "    Grad:   tensor([-0.0533,  3.0257])\n",
      "Epoch 42, Loss 29.075731\n",
      "    Params: tensor([ 0.2324, -0.0262])\n",
      "    Grad:   tensor([-0.0532,  3.0257])\n",
      "Epoch 43, Loss 29.074812\n",
      "    Params: tensor([ 0.2324, -0.0265])\n",
      "    Grad:   tensor([-0.0533,  3.0256])\n",
      "Epoch 44, Loss 29.073895\n",
      "    Params: tensor([ 0.2324, -0.0268])\n",
      "    Grad:   tensor([-0.0533,  3.0256])\n",
      "Epoch 45, Loss 29.072981\n",
      "    Params: tensor([ 0.2324, -0.0271])\n",
      "    Grad:   tensor([-0.0533,  3.0255])\n",
      "Epoch 46, Loss 29.072069\n",
      "    Params: tensor([ 0.2324, -0.0274])\n",
      "    Grad:   tensor([-0.0533,  3.0254])\n",
      "Epoch 47, Loss 29.071148\n",
      "    Params: tensor([ 0.2324, -0.0277])\n",
      "    Grad:   tensor([-0.0533,  3.0254])\n",
      "Epoch 48, Loss 29.070234\n",
      "    Params: tensor([ 0.2324, -0.0281])\n",
      "    Grad:   tensor([-0.0533,  3.0253])\n",
      "Epoch 49, Loss 29.069323\n",
      "    Params: tensor([ 0.2325, -0.0284])\n",
      "    Grad:   tensor([-0.0533,  3.0253])\n",
      "Epoch 50, Loss 29.068401\n",
      "    Params: tensor([ 0.2325, -0.0287])\n",
      "    Grad:   tensor([-0.0532,  3.0252])\n",
      "Epoch 51, Loss 29.067486\n",
      "    Params: tensor([ 0.2325, -0.0290])\n",
      "    Grad:   tensor([-0.0533,  3.0252])\n",
      "Epoch 52, Loss 29.066566\n",
      "    Params: tensor([ 0.2325, -0.0293])\n",
      "    Grad:   tensor([-0.0533,  3.0251])\n",
      "Epoch 53, Loss 29.065657\n",
      "    Params: tensor([ 0.2325, -0.0296])\n",
      "    Grad:   tensor([-0.0533,  3.0251])\n",
      "Epoch 54, Loss 29.064741\n",
      "    Params: tensor([ 0.2325, -0.0299])\n",
      "    Grad:   tensor([-0.0533,  3.0250])\n",
      "Epoch 55, Loss 29.063826\n",
      "    Params: tensor([ 0.2325, -0.0302])\n",
      "    Grad:   tensor([-0.0532,  3.0250])\n",
      "Epoch 56, Loss 29.062910\n",
      "    Params: tensor([ 0.2325, -0.0305])\n",
      "    Grad:   tensor([-0.0533,  3.0249])\n",
      "Epoch 57, Loss 29.061995\n",
      "    Params: tensor([ 0.2325, -0.0308])\n",
      "    Grad:   tensor([-0.0532,  3.0249])\n",
      "Epoch 58, Loss 29.061079\n",
      "    Params: tensor([ 0.2325, -0.0311])\n",
      "    Grad:   tensor([-0.0533,  3.0248])\n",
      "Epoch 59, Loss 29.060169\n",
      "    Params: tensor([ 0.2325, -0.0314])\n",
      "    Grad:   tensor([-0.0533,  3.0248])\n",
      "Epoch 60, Loss 29.059248\n",
      "    Params: tensor([ 0.2325, -0.0317])\n",
      "    Grad:   tensor([-0.0533,  3.0247])\n",
      "Epoch 61, Loss 29.058336\n",
      "    Params: tensor([ 0.2325, -0.0320])\n",
      "    Grad:   tensor([-0.0533,  3.0247])\n",
      "Epoch 62, Loss 29.057415\n",
      "    Params: tensor([ 0.2325, -0.0323])\n",
      "    Grad:   tensor([-0.0534,  3.0246])\n",
      "Epoch 63, Loss 29.056507\n",
      "    Params: tensor([ 0.2325, -0.0326])\n",
      "    Grad:   tensor([-0.0533,  3.0245])\n",
      "Epoch 64, Loss 29.055586\n",
      "    Params: tensor([ 0.2325, -0.0329])\n",
      "    Grad:   tensor([-0.0532,  3.0245])\n",
      "Epoch 65, Loss 29.054674\n",
      "    Params: tensor([ 0.2325, -0.0332])\n",
      "    Grad:   tensor([-0.0533,  3.0244])\n",
      "Epoch 66, Loss 29.053761\n",
      "    Params: tensor([ 0.2325, -0.0335])\n",
      "    Grad:   tensor([-0.0533,  3.0244])\n",
      "Epoch 67, Loss 29.052843\n",
      "    Params: tensor([ 0.2325, -0.0338])\n",
      "    Grad:   tensor([-0.0533,  3.0243])\n",
      "Epoch 68, Loss 29.051929\n",
      "    Params: tensor([ 0.2326, -0.0341])\n",
      "    Grad:   tensor([-0.0532,  3.0243])\n",
      "Epoch 69, Loss 29.051012\n",
      "    Params: tensor([ 0.2326, -0.0344])\n",
      "    Grad:   tensor([-0.0533,  3.0242])\n",
      "Epoch 70, Loss 29.050098\n",
      "    Params: tensor([ 0.2326, -0.0347])\n",
      "    Grad:   tensor([-0.0532,  3.0242])\n",
      "Epoch 71, Loss 29.049183\n",
      "    Params: tensor([ 0.2326, -0.0350])\n",
      "    Grad:   tensor([-0.0533,  3.0241])\n",
      "Epoch 72, Loss 29.048273\n",
      "    Params: tensor([ 0.2326, -0.0353])\n",
      "    Grad:   tensor([-0.0533,  3.0241])\n",
      "Epoch 73, Loss 29.047350\n",
      "    Params: tensor([ 0.2326, -0.0356])\n",
      "    Grad:   tensor([-0.0532,  3.0240])\n",
      "Epoch 74, Loss 29.046442\n",
      "    Params: tensor([ 0.2326, -0.0359])\n",
      "    Grad:   tensor([-0.0533,  3.0240])\n",
      "Epoch 75, Loss 29.045530\n",
      "    Params: tensor([ 0.2326, -0.0362])\n",
      "    Grad:   tensor([-0.0532,  3.0239])\n",
      "Epoch 76, Loss 29.044611\n",
      "    Params: tensor([ 0.2326, -0.0365])\n",
      "    Grad:   tensor([-0.0533,  3.0239])\n",
      "Epoch 77, Loss 29.043699\n",
      "    Params: tensor([ 0.2326, -0.0368])\n",
      "    Grad:   tensor([-0.0533,  3.0238])\n",
      "Epoch 78, Loss 29.042784\n",
      "    Params: tensor([ 0.2326, -0.0371])\n",
      "    Grad:   tensor([-0.0533,  3.0238])\n",
      "Epoch 79, Loss 29.041870\n",
      "    Params: tensor([ 0.2326, -0.0374])\n",
      "    Grad:   tensor([-0.0533,  3.0237])\n",
      "Epoch 80, Loss 29.040955\n",
      "    Params: tensor([ 0.2326, -0.0377])\n",
      "    Grad:   tensor([-0.0532,  3.0236])\n",
      "Epoch 81, Loss 29.040039\n",
      "    Params: tensor([ 0.2326, -0.0380])\n",
      "    Grad:   tensor([-0.0534,  3.0236])\n",
      "Epoch 82, Loss 29.039122\n",
      "    Params: tensor([ 0.2326, -0.0383])\n",
      "    Grad:   tensor([-0.0533,  3.0235])\n",
      "Epoch 83, Loss 29.038210\n",
      "    Params: tensor([ 0.2326, -0.0386])\n",
      "    Grad:   tensor([-0.0532,  3.0235])\n",
      "Epoch 84, Loss 29.037294\n",
      "    Params: tensor([ 0.2326, -0.0389])\n",
      "    Grad:   tensor([-0.0533,  3.0234])\n",
      "Epoch 85, Loss 29.036379\n",
      "    Params: tensor([ 0.2326, -0.0392])\n",
      "    Grad:   tensor([-0.0533,  3.0234])\n",
      "Epoch 86, Loss 29.035463\n",
      "    Params: tensor([ 0.2326, -0.0395])\n",
      "    Grad:   tensor([-0.0532,  3.0233])\n",
      "Epoch 87, Loss 29.034554\n",
      "    Params: tensor([ 0.2327, -0.0398])\n",
      "    Grad:   tensor([-0.0533,  3.0233])\n",
      "Epoch 88, Loss 29.033636\n",
      "    Params: tensor([ 0.2327, -0.0401])\n",
      "    Grad:   tensor([-0.0532,  3.0232])\n",
      "Epoch 89, Loss 29.032722\n",
      "    Params: tensor([ 0.2327, -0.0405])\n",
      "    Grad:   tensor([-0.0533,  3.0232])\n",
      "Epoch 90, Loss 29.031811\n",
      "    Params: tensor([ 0.2327, -0.0408])\n",
      "    Grad:   tensor([-0.0533,  3.0231])\n",
      "Epoch 91, Loss 29.030895\n",
      "    Params: tensor([ 0.2327, -0.0411])\n",
      "    Grad:   tensor([-0.0532,  3.0231])\n",
      "Epoch 92, Loss 29.029976\n",
      "    Params: tensor([ 0.2327, -0.0414])\n",
      "    Grad:   tensor([-0.0532,  3.0230])\n",
      "Epoch 93, Loss 29.029066\n",
      "    Params: tensor([ 0.2327, -0.0417])\n",
      "    Grad:   tensor([-0.0533,  3.0230])\n",
      "Epoch 94, Loss 29.028151\n",
      "    Params: tensor([ 0.2327, -0.0420])\n",
      "    Grad:   tensor([-0.0532,  3.0229])\n",
      "Epoch 95, Loss 29.027235\n",
      "    Params: tensor([ 0.2327, -0.0423])\n",
      "    Grad:   tensor([-0.0533,  3.0229])\n",
      "Epoch 96, Loss 29.026323\n",
      "    Params: tensor([ 0.2327, -0.0426])\n",
      "    Grad:   tensor([-0.0533,  3.0228])\n",
      "Epoch 97, Loss 29.025410\n",
      "    Params: tensor([ 0.2327, -0.0429])\n",
      "    Grad:   tensor([-0.0532,  3.0227])\n",
      "Epoch 98, Loss 29.024492\n",
      "    Params: tensor([ 0.2327, -0.0432])\n",
      "    Grad:   tensor([-0.0532,  3.0227])\n",
      "Epoch 99, Loss 29.023582\n",
      "    Params: tensor([ 0.2327, -0.0435])\n",
      "    Grad:   tensor([-0.0533,  3.0226])\n",
      "Epoch 100, Loss 29.022667\n",
      "    Params: tensor([ 0.2327, -0.0438])\n",
      "    Grad:   tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 1e-4,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574913\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587])\n",
      "Epoch 4, Loss 29.756193\n",
      "    Params: tensor([2.2616, 0.1004])\n",
      "    Grad:   tensor([-5.2218,  2.1327])\n",
      "Epoch 5, Loss 29.507153\n",
      "    Params: tensor([2.2853, 0.0740])\n",
      "    Grad:   tensor([-2.3715,  2.6310])\n",
      "Epoch 6, Loss 29.392456\n",
      "    Params: tensor([2.2978, 0.0458])\n",
      "    Grad:   tensor([-1.2492,  2.8241])\n",
      "Epoch 7, Loss 29.298828\n",
      "    Params: tensor([2.3059, 0.0168])\n",
      "    Grad:   tensor([-0.8071,  2.8970])\n",
      "Epoch 8, Loss 29.208717\n",
      "    Params: tensor([ 2.3122, -0.0124])\n",
      "    Grad:   tensor([-0.6325,  2.9227])\n",
      "Epoch 9, Loss 29.119415\n",
      "    Params: tensor([ 2.3178, -0.0417])\n",
      "    Grad:   tensor([-0.5633,  2.9298])\n",
      "Epoch 10, Loss 29.030489\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941877\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264])\n",
      "Epoch 12, Loss 28.853565\n",
      "    Params: tensor([ 2.3336, -0.1295])\n",
      "    Grad:   tensor([-0.5190,  2.9222])\n",
      "Epoch 13, Loss 28.765553\n",
      "    Params: tensor([ 2.3388, -0.1587])\n",
      "    Grad:   tensor([-0.5165,  2.9175])\n",
      "Epoch 14, Loss 28.677851\n",
      "    Params: tensor([ 2.3439, -0.1878])\n",
      "    Grad:   tensor([-0.5150,  2.9126])\n",
      "Epoch 15, Loss 28.590431\n",
      "    Params: tensor([ 2.3491, -0.2169])\n",
      "    Grad:   tensor([-0.5138,  2.9077])\n",
      "Epoch 16, Loss 28.503319\n",
      "    Params: tensor([ 2.3542, -0.2459])\n",
      "    Grad:   tensor([-0.5129,  2.9028])\n",
      "Epoch 17, Loss 28.416498\n",
      "    Params: tensor([ 2.3593, -0.2749])\n",
      "    Grad:   tensor([-0.5120,  2.8979])\n",
      "Epoch 18, Loss 28.329973\n",
      "    Params: tensor([ 2.3644, -0.3038])\n",
      "    Grad:   tensor([-0.5111,  2.8930])\n",
      "Epoch 19, Loss 28.243742\n",
      "    Params: tensor([ 2.3695, -0.3327])\n",
      "    Grad:   tensor([-0.5102,  2.8881])\n",
      "Epoch 20, Loss 28.157804\n",
      "    Params: tensor([ 2.3746, -0.3615])\n",
      "    Grad:   tensor([-0.5093,  2.8832])\n",
      "Epoch 21, Loss 28.072151\n",
      "    Params: tensor([ 2.3797, -0.3903])\n",
      "    Grad:   tensor([-0.5084,  2.8783])\n",
      "Epoch 22, Loss 27.986797\n",
      "    Params: tensor([ 2.3848, -0.4190])\n",
      "    Grad:   tensor([-0.5076,  2.8734])\n",
      "Epoch 23, Loss 27.901728\n",
      "    Params: tensor([ 2.3899, -0.4477])\n",
      "    Grad:   tensor([-0.5067,  2.8685])\n",
      "Epoch 24, Loss 27.816950\n",
      "    Params: tensor([ 2.3949, -0.4763])\n",
      "    Grad:   tensor([-0.5059,  2.8636])\n",
      "Epoch 25, Loss 27.732464\n",
      "    Params: tensor([ 2.4000, -0.5049])\n",
      "    Grad:   tensor([-0.5050,  2.8588])\n",
      "Epoch 26, Loss 27.648256\n",
      "    Params: tensor([ 2.4050, -0.5335])\n",
      "    Grad:   tensor([-0.5042,  2.8539])\n",
      "Epoch 27, Loss 27.564344\n",
      "    Params: tensor([ 2.4101, -0.5620])\n",
      "    Grad:   tensor([-0.5033,  2.8490])\n",
      "Epoch 28, Loss 27.480707\n",
      "    Params: tensor([ 2.4151, -0.5904])\n",
      "    Grad:   tensor([-0.5024,  2.8442])\n",
      "Epoch 29, Loss 27.397362\n",
      "    Params: tensor([ 2.4201, -0.6188])\n",
      "    Grad:   tensor([-0.5016,  2.8394])\n",
      "Epoch 30, Loss 27.314295\n",
      "    Params: tensor([ 2.4251, -0.6471])\n",
      "    Grad:   tensor([-0.5007,  2.8346])\n",
      "Epoch 31, Loss 27.231512\n",
      "    Params: tensor([ 2.4301, -0.6754])\n",
      "    Grad:   tensor([-0.4999,  2.8297])\n",
      "Epoch 32, Loss 27.149010\n",
      "    Params: tensor([ 2.4351, -0.7037])\n",
      "    Grad:   tensor([-0.4990,  2.8249])\n",
      "Epoch 33, Loss 27.066790\n",
      "    Params: tensor([ 2.4401, -0.7319])\n",
      "    Grad:   tensor([-0.4982,  2.8201])\n",
      "Epoch 34, Loss 26.984844\n",
      "    Params: tensor([ 2.4450, -0.7600])\n",
      "    Grad:   tensor([-0.4973,  2.8153])\n",
      "Epoch 35, Loss 26.903175\n",
      "    Params: tensor([ 2.4500, -0.7881])\n",
      "    Grad:   tensor([-0.4965,  2.8106])\n",
      "Epoch 36, Loss 26.821791\n",
      "    Params: tensor([ 2.4550, -0.8162])\n",
      "    Grad:   tensor([-0.4957,  2.8058])\n",
      "Epoch 37, Loss 26.740679\n",
      "    Params: tensor([ 2.4599, -0.8442])\n",
      "    Grad:   tensor([-0.4948,  2.8010])\n",
      "Epoch 38, Loss 26.659838\n",
      "    Params: tensor([ 2.4649, -0.8722])\n",
      "    Grad:   tensor([-0.4940,  2.7963])\n",
      "Epoch 39, Loss 26.579279\n",
      "    Params: tensor([ 2.4698, -0.9001])\n",
      "    Grad:   tensor([-0.4931,  2.7915])\n",
      "Epoch 40, Loss 26.498987\n",
      "    Params: tensor([ 2.4747, -0.9280])\n",
      "    Grad:   tensor([-0.4923,  2.7868])\n",
      "Epoch 41, Loss 26.418974\n",
      "    Params: tensor([ 2.4796, -0.9558])\n",
      "    Grad:   tensor([-0.4915,  2.7820])\n",
      "Epoch 42, Loss 26.339228\n",
      "    Params: tensor([ 2.4845, -0.9836])\n",
      "    Grad:   tensor([-0.4906,  2.7773])\n",
      "Epoch 43, Loss 26.259754\n",
      "    Params: tensor([ 2.4894, -1.0113])\n",
      "    Grad:   tensor([-0.4898,  2.7726])\n",
      "Epoch 44, Loss 26.180548\n",
      "    Params: tensor([ 2.4943, -1.0390])\n",
      "    Grad:   tensor([-0.4890,  2.7679])\n",
      "Epoch 45, Loss 26.101616\n",
      "    Params: tensor([ 2.4992, -1.0666])\n",
      "    Grad:   tensor([-0.4881,  2.7632])\n",
      "Epoch 46, Loss 26.022947\n",
      "    Params: tensor([ 2.5041, -1.0942])\n",
      "    Grad:   tensor([-0.4873,  2.7585])\n",
      "Epoch 47, Loss 25.944544\n",
      "    Params: tensor([ 2.5089, -1.1217])\n",
      "    Grad:   tensor([-0.4865,  2.7538])\n",
      "Epoch 48, Loss 25.866417\n",
      "    Params: tensor([ 2.5138, -1.1492])\n",
      "    Grad:   tensor([-0.4856,  2.7491])\n",
      "Epoch 49, Loss 25.788549\n",
      "    Params: tensor([ 2.5186, -1.1766])\n",
      "    Grad:   tensor([-0.4848,  2.7444])\n",
      "Epoch 50, Loss 25.710938\n",
      "    Params: tensor([ 2.5235, -1.2040])\n",
      "    Grad:   tensor([-0.4840,  2.7398])\n",
      "Epoch 51, Loss 25.633600\n",
      "    Params: tensor([ 2.5283, -1.2314])\n",
      "    Grad:   tensor([-0.4832,  2.7351])\n",
      "Epoch 52, Loss 25.556524\n",
      "    Params: tensor([ 2.5331, -1.2587])\n",
      "    Grad:   tensor([-0.4823,  2.7305])\n",
      "Epoch 53, Loss 25.479700\n",
      "    Params: tensor([ 2.5379, -1.2860])\n",
      "    Grad:   tensor([-0.4815,  2.7258])\n",
      "Epoch 54, Loss 25.403149\n",
      "    Params: tensor([ 2.5428, -1.3132])\n",
      "    Grad:   tensor([-0.4807,  2.7212])\n",
      "Epoch 55, Loss 25.326851\n",
      "    Params: tensor([ 2.5476, -1.3403])\n",
      "    Grad:   tensor([-0.4799,  2.7166])\n",
      "Epoch 56, Loss 25.250811\n",
      "    Params: tensor([ 2.5523, -1.3675])\n",
      "    Grad:   tensor([-0.4791,  2.7120])\n",
      "Epoch 57, Loss 25.175035\n",
      "    Params: tensor([ 2.5571, -1.3945])\n",
      "    Grad:   tensor([-0.4783,  2.7074])\n",
      "Epoch 58, Loss 25.099512\n",
      "    Params: tensor([ 2.5619, -1.4216])\n",
      "    Grad:   tensor([-0.4775,  2.7028])\n",
      "Epoch 59, Loss 25.024248\n",
      "    Params: tensor([ 2.5667, -1.4485])\n",
      "    Grad:   tensor([-0.4766,  2.6982])\n",
      "Epoch 60, Loss 24.949236\n",
      "    Params: tensor([ 2.5714, -1.4755])\n",
      "    Grad:   tensor([-0.4758,  2.6936])\n",
      "Epoch 61, Loss 24.874483\n",
      "    Params: tensor([ 2.5762, -1.5024])\n",
      "    Grad:   tensor([-0.4750,  2.6890])\n",
      "Epoch 62, Loss 24.799976\n",
      "    Params: tensor([ 2.5809, -1.5292])\n",
      "    Grad:   tensor([-0.4742,  2.6845])\n",
      "Epoch 63, Loss 24.725737\n",
      "    Params: tensor([ 2.5857, -1.5560])\n",
      "    Grad:   tensor([-0.4734,  2.6799])\n",
      "Epoch 64, Loss 24.651739\n",
      "    Params: tensor([ 2.5904, -1.5828])\n",
      "    Grad:   tensor([-0.4726,  2.6753])\n",
      "Epoch 65, Loss 24.577986\n",
      "    Params: tensor([ 2.5951, -1.6095])\n",
      "    Grad:   tensor([-0.4718,  2.6708])\n",
      "Epoch 66, Loss 24.504494\n",
      "    Params: tensor([ 2.5998, -1.6361])\n",
      "    Grad:   tensor([-0.4710,  2.6663])\n",
      "Epoch 67, Loss 24.431252\n",
      "    Params: tensor([ 2.6045, -1.6628])\n",
      "    Grad:   tensor([-0.4702,  2.6617])\n",
      "Epoch 68, Loss 24.358257\n",
      "    Params: tensor([ 2.6092, -1.6893])\n",
      "    Grad:   tensor([-0.4694,  2.6572])\n",
      "Epoch 69, Loss 24.285505\n",
      "    Params: tensor([ 2.6139, -1.7159])\n",
      "    Grad:   tensor([-0.4686,  2.6527])\n",
      "Epoch 70, Loss 24.212999\n",
      "    Params: tensor([ 2.6186, -1.7423])\n",
      "    Grad:   tensor([-0.4678,  2.6482])\n",
      "Epoch 71, Loss 24.140741\n",
      "    Params: tensor([ 2.6232, -1.7688])\n",
      "    Grad:   tensor([-0.4670,  2.6437])\n",
      "Epoch 72, Loss 24.068733\n",
      "    Params: tensor([ 2.6279, -1.7952])\n",
      "    Grad:   tensor([-0.4662,  2.6392])\n",
      "Epoch 73, Loss 23.996971\n",
      "    Params: tensor([ 2.6326, -1.8215])\n",
      "    Grad:   tensor([-0.4654,  2.6347])\n",
      "Epoch 74, Loss 23.925446\n",
      "    Params: tensor([ 2.6372, -1.8478])\n",
      "    Grad:   tensor([-0.4646,  2.6302])\n",
      "Epoch 75, Loss 23.854168\n",
      "    Params: tensor([ 2.6418, -1.8741])\n",
      "    Grad:   tensor([-0.4638,  2.6258])\n",
      "Epoch 76, Loss 23.783125\n",
      "    Params: tensor([ 2.6465, -1.9003])\n",
      "    Grad:   tensor([-0.4631,  2.6213])\n",
      "Epoch 77, Loss 23.712328\n",
      "    Params: tensor([ 2.6511, -1.9265])\n",
      "    Grad:   tensor([-0.4623,  2.6169])\n",
      "Epoch 78, Loss 23.641773\n",
      "    Params: tensor([ 2.6557, -1.9526])\n",
      "    Grad:   tensor([-0.4615,  2.6124])\n",
      "Epoch 79, Loss 23.571455\n",
      "    Params: tensor([ 2.6603, -1.9787])\n",
      "    Grad:   tensor([-0.4607,  2.6080])\n",
      "Epoch 80, Loss 23.501379\n",
      "    Params: tensor([ 2.6649, -2.0047])\n",
      "    Grad:   tensor([-0.4599,  2.6035])\n",
      "Epoch 81, Loss 23.431538\n",
      "    Params: tensor([ 2.6695, -2.0307])\n",
      "    Grad:   tensor([-0.4591,  2.5991])\n",
      "Epoch 82, Loss 23.361937\n",
      "    Params: tensor([ 2.6741, -2.0566])\n",
      "    Grad:   tensor([-0.4584,  2.5947])\n",
      "Epoch 83, Loss 23.292570\n",
      "    Params: tensor([ 2.6787, -2.0825])\n",
      "    Grad:   tensor([-0.4576,  2.5903])\n",
      "Epoch 84, Loss 23.223436\n",
      "    Params: tensor([ 2.6832, -2.1084])\n",
      "    Grad:   tensor([-0.4568,  2.5859])\n",
      "Epoch 85, Loss 23.154541\n",
      "    Params: tensor([ 2.6878, -2.1342])\n",
      "    Grad:   tensor([-0.4560,  2.5815])\n",
      "Epoch 86, Loss 23.085882\n",
      "    Params: tensor([ 2.6923, -2.1600])\n",
      "    Grad:   tensor([-0.4553,  2.5771])\n",
      "Epoch 87, Loss 23.017447\n",
      "    Params: tensor([ 2.6969, -2.1857])\n",
      "    Grad:   tensor([-0.4545,  2.5727])\n",
      "Epoch 88, Loss 22.949251\n",
      "    Params: tensor([ 2.7014, -2.2114])\n",
      "    Grad:   tensor([-0.4537,  2.5684])\n",
      "Epoch 89, Loss 22.881283\n",
      "    Params: tensor([ 2.7060, -2.2370])\n",
      "    Grad:   tensor([-0.4529,  2.5640])\n",
      "Epoch 90, Loss 22.813549\n",
      "    Params: tensor([ 2.7105, -2.2626])\n",
      "    Grad:   tensor([-0.4522,  2.5597])\n",
      "Epoch 91, Loss 22.746044\n",
      "    Params: tensor([ 2.7150, -2.2882])\n",
      "    Grad:   tensor([-0.4514,  2.5553])\n",
      "Epoch 92, Loss 22.678766\n",
      "    Params: tensor([ 2.7195, -2.3137])\n",
      "    Grad:   tensor([-0.4506,  2.5510])\n",
      "Epoch 93, Loss 22.611717\n",
      "    Params: tensor([ 2.7240, -2.3392])\n",
      "    Grad:   tensor([-0.4499,  2.5466])\n",
      "Epoch 94, Loss 22.544899\n",
      "    Params: tensor([ 2.7285, -2.3646])\n",
      "    Grad:   tensor([-0.4491,  2.5423])\n",
      "Epoch 95, Loss 22.478306\n",
      "    Params: tensor([ 2.7330, -2.3900])\n",
      "    Grad:   tensor([-0.4483,  2.5380])\n",
      "Epoch 96, Loss 22.411934\n",
      "    Params: tensor([ 2.7374, -2.4153])\n",
      "    Grad:   tensor([-0.4476,  2.5337])\n",
      "Epoch 97, Loss 22.345793\n",
      "    Params: tensor([ 2.7419, -2.4406])\n",
      "    Grad:   tensor([-0.4468,  2.5294])\n",
      "Epoch 98, Loss 22.279875\n",
      "    Params: tensor([ 2.7464, -2.4658])\n",
      "    Grad:   tensor([-0.4461,  2.5251])\n",
      "Epoch 99, Loss 22.214186\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 100,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "    Params: tensor([1.7761, 0.1064])\n",
      "    Grad:   tensor([-77.6140, -10.6400])\n",
      "Epoch 2, Loss 37.574913\n",
      "    Params: tensor([2.0848, 0.1303])\n",
      "    Grad:   tensor([-30.8623,  -2.3864])\n",
      "Epoch 3, Loss 30.871077\n",
      "    Params: tensor([2.2094, 0.1217])\n",
      "    Grad:   tensor([-12.4631,   0.8587])\n",
      "Epoch 4, Loss 29.756193\n",
      "    Params: tensor([2.2616, 0.1004])\n",
      "    Grad:   tensor([-5.2218,  2.1327])\n",
      "Epoch 5, Loss 29.507153\n",
      "    Params: tensor([2.2853, 0.0740])\n",
      "    Grad:   tensor([-2.3715,  2.6310])\n",
      "Epoch 6, Loss 29.392456\n",
      "    Params: tensor([2.2978, 0.0458])\n",
      "    Grad:   tensor([-1.2492,  2.8241])\n",
      "Epoch 7, Loss 29.298828\n",
      "    Params: tensor([2.3059, 0.0168])\n",
      "    Grad:   tensor([-0.8071,  2.8970])\n",
      "Epoch 8, Loss 29.208717\n",
      "    Params: tensor([ 2.3122, -0.0124])\n",
      "    Grad:   tensor([-0.6325,  2.9227])\n",
      "Epoch 9, Loss 29.119415\n",
      "    Params: tensor([ 2.3178, -0.0417])\n",
      "    Grad:   tensor([-0.5633,  2.9298])\n",
      "Epoch 10, Loss 29.030489\n",
      "    Params: tensor([ 2.3232, -0.0710])\n",
      "    Grad:   tensor([-0.5355,  2.9295])\n",
      "Epoch 11, Loss 28.941877\n",
      "    Params: tensor([ 2.3284, -0.1003])\n",
      "    Grad:   tensor([-0.5240,  2.9264])\n",
      "Epoch 12, Loss 28.853565\n",
      "    Params: tensor([ 2.3336, -0.1295])\n",
      "    Grad:   tensor([-0.5190,  2.9222])\n",
      "Epoch 13, Loss 28.765553\n",
      "    Params: tensor([ 2.3388, -0.1587])\n",
      "    Grad:   tensor([-0.5165,  2.9175])\n",
      "Epoch 14, Loss 28.677851\n",
      "    Params: tensor([ 2.3439, -0.1878])\n",
      "    Grad:   tensor([-0.5150,  2.9126])\n",
      "Epoch 15, Loss 28.590431\n",
      "    Params: tensor([ 2.3491, -0.2169])\n",
      "    Grad:   tensor([-0.5138,  2.9077])\n",
      "Epoch 16, Loss 28.503319\n",
      "    Params: tensor([ 2.3542, -0.2459])\n",
      "    Grad:   tensor([-0.5129,  2.9028])\n",
      "Epoch 17, Loss 28.416498\n",
      "    Params: tensor([ 2.3593, -0.2749])\n",
      "    Grad:   tensor([-0.5120,  2.8979])\n",
      "Epoch 18, Loss 28.329973\n",
      "    Params: tensor([ 2.3644, -0.3038])\n",
      "    Grad:   tensor([-0.5111,  2.8930])\n",
      "Epoch 19, Loss 28.243742\n",
      "    Params: tensor([ 2.3695, -0.3327])\n",
      "    Grad:   tensor([-0.5102,  2.8881])\n",
      "Epoch 20, Loss 28.157804\n",
      "    Params: tensor([ 2.3746, -0.3615])\n",
      "    Grad:   tensor([-0.5093,  2.8832])\n",
      "Epoch 21, Loss 28.072151\n",
      "    Params: tensor([ 2.3797, -0.3903])\n",
      "    Grad:   tensor([-0.5084,  2.8783])\n",
      "Epoch 22, Loss 27.986797\n",
      "    Params: tensor([ 2.3848, -0.4190])\n",
      "    Grad:   tensor([-0.5076,  2.8734])\n",
      "Epoch 23, Loss 27.901728\n",
      "    Params: tensor([ 2.3899, -0.4477])\n",
      "    Grad:   tensor([-0.5067,  2.8685])\n",
      "Epoch 24, Loss 27.816950\n",
      "    Params: tensor([ 2.3949, -0.4763])\n",
      "    Grad:   tensor([-0.5059,  2.8636])\n",
      "Epoch 25, Loss 27.732464\n",
      "    Params: tensor([ 2.4000, -0.5049])\n",
      "    Grad:   tensor([-0.5050,  2.8588])\n",
      "Epoch 26, Loss 27.648256\n",
      "    Params: tensor([ 2.4050, -0.5335])\n",
      "    Grad:   tensor([-0.5042,  2.8539])\n",
      "Epoch 27, Loss 27.564344\n",
      "    Params: tensor([ 2.4101, -0.5620])\n",
      "    Grad:   tensor([-0.5033,  2.8490])\n",
      "Epoch 28, Loss 27.480707\n",
      "    Params: tensor([ 2.4151, -0.5904])\n",
      "    Grad:   tensor([-0.5024,  2.8442])\n",
      "Epoch 29, Loss 27.397362\n",
      "    Params: tensor([ 2.4201, -0.6188])\n",
      "    Grad:   tensor([-0.5016,  2.8394])\n",
      "Epoch 30, Loss 27.314295\n",
      "    Params: tensor([ 2.4251, -0.6471])\n",
      "    Grad:   tensor([-0.5007,  2.8346])\n",
      "Epoch 31, Loss 27.231512\n",
      "    Params: tensor([ 2.4301, -0.6754])\n",
      "    Grad:   tensor([-0.4999,  2.8297])\n",
      "Epoch 32, Loss 27.149010\n",
      "    Params: tensor([ 2.4351, -0.7037])\n",
      "    Grad:   tensor([-0.4990,  2.8249])\n",
      "Epoch 33, Loss 27.066790\n",
      "    Params: tensor([ 2.4401, -0.7319])\n",
      "    Grad:   tensor([-0.4982,  2.8201])\n",
      "Epoch 34, Loss 26.984844\n",
      "    Params: tensor([ 2.4450, -0.7600])\n",
      "    Grad:   tensor([-0.4973,  2.8153])\n",
      "Epoch 35, Loss 26.903175\n",
      "    Params: tensor([ 2.4500, -0.7881])\n",
      "    Grad:   tensor([-0.4965,  2.8106])\n",
      "Epoch 36, Loss 26.821791\n",
      "    Params: tensor([ 2.4550, -0.8162])\n",
      "    Grad:   tensor([-0.4957,  2.8058])\n",
      "Epoch 37, Loss 26.740679\n",
      "    Params: tensor([ 2.4599, -0.8442])\n",
      "    Grad:   tensor([-0.4948,  2.8010])\n",
      "Epoch 38, Loss 26.659838\n",
      "    Params: tensor([ 2.4649, -0.8722])\n",
      "    Grad:   tensor([-0.4940,  2.7963])\n",
      "Epoch 39, Loss 26.579279\n",
      "    Params: tensor([ 2.4698, -0.9001])\n",
      "    Grad:   tensor([-0.4931,  2.7915])\n",
      "Epoch 40, Loss 26.498987\n",
      "    Params: tensor([ 2.4747, -0.9280])\n",
      "    Grad:   tensor([-0.4923,  2.7868])\n",
      "Epoch 41, Loss 26.418974\n",
      "    Params: tensor([ 2.4796, -0.9558])\n",
      "    Grad:   tensor([-0.4915,  2.7820])\n",
      "Epoch 42, Loss 26.339228\n",
      "    Params: tensor([ 2.4845, -0.9836])\n",
      "    Grad:   tensor([-0.4906,  2.7773])\n",
      "Epoch 43, Loss 26.259754\n",
      "    Params: tensor([ 2.4894, -1.0113])\n",
      "    Grad:   tensor([-0.4898,  2.7726])\n",
      "Epoch 44, Loss 26.180548\n",
      "    Params: tensor([ 2.4943, -1.0390])\n",
      "    Grad:   tensor([-0.4890,  2.7679])\n",
      "Epoch 45, Loss 26.101616\n",
      "    Params: tensor([ 2.4992, -1.0666])\n",
      "    Grad:   tensor([-0.4881,  2.7632])\n",
      "Epoch 46, Loss 26.022947\n",
      "    Params: tensor([ 2.5041, -1.0942])\n",
      "    Grad:   tensor([-0.4873,  2.7585])\n",
      "Epoch 47, Loss 25.944544\n",
      "    Params: tensor([ 2.5089, -1.1217])\n",
      "    Grad:   tensor([-0.4865,  2.7538])\n",
      "Epoch 48, Loss 25.866417\n",
      "    Params: tensor([ 2.5138, -1.1492])\n",
      "    Grad:   tensor([-0.4856,  2.7491])\n",
      "Epoch 49, Loss 25.788549\n",
      "    Params: tensor([ 2.5186, -1.1766])\n",
      "    Grad:   tensor([-0.4848,  2.7444])\n",
      "Epoch 50, Loss 25.710938\n",
      "    Params: tensor([ 2.5235, -1.2040])\n",
      "    Grad:   tensor([-0.4840,  2.7398])\n",
      "Epoch 51, Loss 25.633600\n",
      "    Params: tensor([ 2.5283, -1.2314])\n",
      "    Grad:   tensor([-0.4832,  2.7351])\n",
      "Epoch 52, Loss 25.556524\n",
      "    Params: tensor([ 2.5331, -1.2587])\n",
      "    Grad:   tensor([-0.4823,  2.7305])\n",
      "Epoch 53, Loss 25.479700\n",
      "    Params: tensor([ 2.5379, -1.2860])\n",
      "    Grad:   tensor([-0.4815,  2.7258])\n",
      "Epoch 54, Loss 25.403149\n",
      "    Params: tensor([ 2.5428, -1.3132])\n",
      "    Grad:   tensor([-0.4807,  2.7212])\n",
      "Epoch 55, Loss 25.326851\n",
      "    Params: tensor([ 2.5476, -1.3403])\n",
      "    Grad:   tensor([-0.4799,  2.7166])\n",
      "Epoch 56, Loss 25.250811\n",
      "    Params: tensor([ 2.5523, -1.3675])\n",
      "    Grad:   tensor([-0.4791,  2.7120])\n",
      "Epoch 57, Loss 25.175035\n",
      "    Params: tensor([ 2.5571, -1.3945])\n",
      "    Grad:   tensor([-0.4783,  2.7074])\n",
      "Epoch 58, Loss 25.099512\n",
      "    Params: tensor([ 2.5619, -1.4216])\n",
      "    Grad:   tensor([-0.4775,  2.7028])\n",
      "Epoch 59, Loss 25.024248\n",
      "    Params: tensor([ 2.5667, -1.4485])\n",
      "    Grad:   tensor([-0.4766,  2.6982])\n",
      "Epoch 60, Loss 24.949236\n",
      "    Params: tensor([ 2.5714, -1.4755])\n",
      "    Grad:   tensor([-0.4758,  2.6936])\n",
      "Epoch 61, Loss 24.874483\n",
      "    Params: tensor([ 2.5762, -1.5024])\n",
      "    Grad:   tensor([-0.4750,  2.6890])\n",
      "Epoch 62, Loss 24.799976\n",
      "    Params: tensor([ 2.5809, -1.5292])\n",
      "    Grad:   tensor([-0.4742,  2.6845])\n",
      "Epoch 63, Loss 24.725737\n",
      "    Params: tensor([ 2.5857, -1.5560])\n",
      "    Grad:   tensor([-0.4734,  2.6799])\n",
      "Epoch 64, Loss 24.651739\n",
      "    Params: tensor([ 2.5904, -1.5828])\n",
      "    Grad:   tensor([-0.4726,  2.6753])\n",
      "Epoch 65, Loss 24.577986\n",
      "    Params: tensor([ 2.5951, -1.6095])\n",
      "    Grad:   tensor([-0.4718,  2.6708])\n",
      "Epoch 66, Loss 24.504494\n",
      "    Params: tensor([ 2.5998, -1.6361])\n",
      "    Grad:   tensor([-0.4710,  2.6663])\n",
      "Epoch 67, Loss 24.431252\n",
      "    Params: tensor([ 2.6045, -1.6628])\n",
      "    Grad:   tensor([-0.4702,  2.6617])\n",
      "Epoch 68, Loss 24.358257\n",
      "    Params: tensor([ 2.6092, -1.6893])\n",
      "    Grad:   tensor([-0.4694,  2.6572])\n",
      "Epoch 69, Loss 24.285505\n",
      "    Params: tensor([ 2.6139, -1.7159])\n",
      "    Grad:   tensor([-0.4686,  2.6527])\n",
      "Epoch 70, Loss 24.212999\n",
      "    Params: tensor([ 2.6186, -1.7423])\n",
      "    Grad:   tensor([-0.4678,  2.6482])\n",
      "Epoch 71, Loss 24.140741\n",
      "    Params: tensor([ 2.6232, -1.7688])\n",
      "    Grad:   tensor([-0.4670,  2.6437])\n",
      "Epoch 72, Loss 24.068733\n",
      "    Params: tensor([ 2.6279, -1.7952])\n",
      "    Grad:   tensor([-0.4662,  2.6392])\n",
      "Epoch 73, Loss 23.996971\n",
      "    Params: tensor([ 2.6326, -1.8215])\n",
      "    Grad:   tensor([-0.4654,  2.6347])\n",
      "Epoch 74, Loss 23.925446\n",
      "    Params: tensor([ 2.6372, -1.8478])\n",
      "    Grad:   tensor([-0.4646,  2.6302])\n",
      "Epoch 75, Loss 23.854168\n",
      "    Params: tensor([ 2.6418, -1.8741])\n",
      "    Grad:   tensor([-0.4638,  2.6258])\n",
      "Epoch 76, Loss 23.783125\n",
      "    Params: tensor([ 2.6465, -1.9003])\n",
      "    Grad:   tensor([-0.4631,  2.6213])\n",
      "Epoch 77, Loss 23.712328\n",
      "    Params: tensor([ 2.6511, -1.9265])\n",
      "    Grad:   tensor([-0.4623,  2.6169])\n",
      "Epoch 78, Loss 23.641773\n",
      "    Params: tensor([ 2.6557, -1.9526])\n",
      "    Grad:   tensor([-0.4615,  2.6124])\n",
      "Epoch 79, Loss 23.571455\n",
      "    Params: tensor([ 2.6603, -1.9787])\n",
      "    Grad:   tensor([-0.4607,  2.6080])\n",
      "Epoch 80, Loss 23.501379\n",
      "    Params: tensor([ 2.6649, -2.0047])\n",
      "    Grad:   tensor([-0.4599,  2.6035])\n",
      "Epoch 81, Loss 23.431538\n",
      "    Params: tensor([ 2.6695, -2.0307])\n",
      "    Grad:   tensor([-0.4591,  2.5991])\n",
      "Epoch 82, Loss 23.361937\n",
      "    Params: tensor([ 2.6741, -2.0566])\n",
      "    Grad:   tensor([-0.4584,  2.5947])\n",
      "Epoch 83, Loss 23.292570\n",
      "    Params: tensor([ 2.6787, -2.0825])\n",
      "    Grad:   tensor([-0.4576,  2.5903])\n",
      "Epoch 84, Loss 23.223436\n",
      "    Params: tensor([ 2.6832, -2.1084])\n",
      "    Grad:   tensor([-0.4568,  2.5859])\n",
      "Epoch 85, Loss 23.154541\n",
      "    Params: tensor([ 2.6878, -2.1342])\n",
      "    Grad:   tensor([-0.4560,  2.5815])\n",
      "Epoch 86, Loss 23.085882\n",
      "    Params: tensor([ 2.6923, -2.1600])\n",
      "    Grad:   tensor([-0.4553,  2.5771])\n",
      "Epoch 87, Loss 23.017447\n",
      "    Params: tensor([ 2.6969, -2.1857])\n",
      "    Grad:   tensor([-0.4545,  2.5727])\n",
      "Epoch 88, Loss 22.949251\n",
      "    Params: tensor([ 2.7014, -2.2114])\n",
      "    Grad:   tensor([-0.4537,  2.5684])\n",
      "Epoch 89, Loss 22.881283\n",
      "    Params: tensor([ 2.7060, -2.2370])\n",
      "    Grad:   tensor([-0.4529,  2.5640])\n",
      "Epoch 90, Loss 22.813549\n",
      "    Params: tensor([ 2.7105, -2.2626])\n",
      "    Grad:   tensor([-0.4522,  2.5597])\n",
      "Epoch 91, Loss 22.746044\n",
      "    Params: tensor([ 2.7150, -2.2882])\n",
      "    Grad:   tensor([-0.4514,  2.5553])\n",
      "Epoch 92, Loss 22.678766\n",
      "    Params: tensor([ 2.7195, -2.3137])\n",
      "    Grad:   tensor([-0.4506,  2.5510])\n",
      "Epoch 93, Loss 22.611717\n",
      "    Params: tensor([ 2.7240, -2.3392])\n",
      "    Grad:   tensor([-0.4499,  2.5466])\n",
      "Epoch 94, Loss 22.544899\n",
      "    Params: tensor([ 2.7285, -2.3646])\n",
      "    Grad:   tensor([-0.4491,  2.5423])\n",
      "Epoch 95, Loss 22.478306\n",
      "    Params: tensor([ 2.7330, -2.3900])\n",
      "    Grad:   tensor([-0.4483,  2.5380])\n",
      "Epoch 96, Loss 22.411934\n",
      "    Params: tensor([ 2.7374, -2.4153])\n",
      "    Grad:   tensor([-0.4476,  2.5337])\n",
      "Epoch 97, Loss 22.345793\n",
      "    Params: tensor([ 2.7419, -2.4406])\n",
      "    Grad:   tensor([-0.4468,  2.5294])\n",
      "Epoch 98, Loss 22.279875\n",
      "    Params: tensor([ 2.7464, -2.4658])\n",
      "    Grad:   tensor([-0.4461,  2.5251])\n",
      "Epoch 99, Loss 22.214186\n",
      "    Params: tensor([ 2.7508, -2.4910])\n",
      "    Grad:   tensor([-0.4453,  2.5208])\n",
      "Epoch 100, Loss 22.148710\n",
      "    Params: tensor([ 2.7553, -2.5162])\n",
      "    Grad:   tensor([-0.4446,  2.5165])\n",
      "Epoch 101, Loss 22.083464\n",
      "    Params: tensor([ 2.7597, -2.5413])\n",
      "    Grad:   tensor([-0.4438,  2.5122])\n",
      "Epoch 102, Loss 22.018436\n",
      "    Params: tensor([ 2.7641, -2.5664])\n",
      "    Grad:   tensor([-0.4430,  2.5080])\n",
      "Epoch 103, Loss 21.953632\n",
      "    Params: tensor([ 2.7686, -2.5914])\n",
      "    Grad:   tensor([-0.4423,  2.5037])\n",
      "Epoch 104, Loss 21.889046\n",
      "    Params: tensor([ 2.7730, -2.6164])\n",
      "    Grad:   tensor([-0.4415,  2.4994])\n",
      "Epoch 105, Loss 21.824677\n",
      "    Params: tensor([ 2.7774, -2.6414])\n",
      "    Grad:   tensor([-0.4408,  2.4952])\n",
      "Epoch 106, Loss 21.760529\n",
      "    Params: tensor([ 2.7818, -2.6663])\n",
      "    Grad:   tensor([-0.4400,  2.4910])\n",
      "Epoch 107, Loss 21.696600\n",
      "    Params: tensor([ 2.7862, -2.6912])\n",
      "    Grad:   tensor([-0.4393,  2.4867])\n",
      "Epoch 108, Loss 21.632883\n",
      "    Params: tensor([ 2.7906, -2.7160])\n",
      "    Grad:   tensor([-0.4385,  2.4825])\n",
      "Epoch 109, Loss 21.569389\n",
      "    Params: tensor([ 2.7949, -2.7408])\n",
      "    Grad:   tensor([-0.4378,  2.4783])\n",
      "Epoch 110, Loss 21.506102\n",
      "    Params: tensor([ 2.7993, -2.7655])\n",
      "    Grad:   tensor([-0.4370,  2.4741])\n",
      "Epoch 111, Loss 21.443037\n",
      "    Params: tensor([ 2.8037, -2.7902])\n",
      "    Grad:   tensor([-0.4363,  2.4699])\n",
      "Epoch 112, Loss 21.380186\n",
      "    Params: tensor([ 2.8080, -2.8149])\n",
      "    Grad:   tensor([-0.4356,  2.4657])\n",
      "Epoch 113, Loss 21.317549\n",
      "    Params: tensor([ 2.8124, -2.8395])\n",
      "    Grad:   tensor([-0.4348,  2.4615])\n",
      "Epoch 114, Loss 21.255117\n",
      "    Params: tensor([ 2.8167, -2.8641])\n",
      "    Grad:   tensor([-0.4341,  2.4573])\n",
      "Epoch 115, Loss 21.192907\n",
      "    Params: tensor([ 2.8211, -2.8886])\n",
      "    Grad:   tensor([-0.4334,  2.4531])\n",
      "Epoch 116, Loss 21.130898\n",
      "    Params: tensor([ 2.8254, -2.9131])\n",
      "    Grad:   tensor([-0.4326,  2.4490])\n",
      "Epoch 117, Loss 21.069105\n",
      "    Params: tensor([ 2.8297, -2.9375])\n",
      "    Grad:   tensor([-0.4319,  2.4448])\n",
      "Epoch 118, Loss 21.007526\n",
      "    Params: tensor([ 2.8340, -2.9619])\n",
      "    Grad:   tensor([-0.4311,  2.4407])\n",
      "Epoch 119, Loss 20.946150\n",
      "    Params: tensor([ 2.8383, -2.9863])\n",
      "    Grad:   tensor([-0.4304,  2.4365])\n",
      "Epoch 120, Loss 20.884981\n",
      "    Params: tensor([ 2.8426, -3.0106])\n",
      "    Grad:   tensor([-0.4297,  2.4324])\n",
      "Epoch 121, Loss 20.824024\n",
      "    Params: tensor([ 2.8469, -3.0349])\n",
      "    Grad:   tensor([-0.4290,  2.4282])\n",
      "Epoch 122, Loss 20.763273\n",
      "    Params: tensor([ 2.8512, -3.0592])\n",
      "    Grad:   tensor([-0.4282,  2.4241])\n",
      "Epoch 123, Loss 20.702728\n",
      "    Params: tensor([ 2.8555, -3.0834])\n",
      "    Grad:   tensor([-0.4275,  2.4200])\n",
      "Epoch 124, Loss 20.642384\n",
      "    Params: tensor([ 2.8597, -3.1075])\n",
      "    Grad:   tensor([-0.4268,  2.4159])\n",
      "Epoch 125, Loss 20.582249\n",
      "    Params: tensor([ 2.8640, -3.1316])\n",
      "    Grad:   tensor([-0.4260,  2.4118])\n",
      "Epoch 126, Loss 20.522322\n",
      "    Params: tensor([ 2.8682, -3.1557])\n",
      "    Grad:   tensor([-0.4253,  2.4077])\n",
      "Epoch 127, Loss 20.462593\n",
      "    Params: tensor([ 2.8725, -3.1797])\n",
      "    Grad:   tensor([-0.4246,  2.4036])\n",
      "Epoch 128, Loss 20.403069\n",
      "    Params: tensor([ 2.8767, -3.2037])\n",
      "    Grad:   tensor([-0.4239,  2.3995])\n",
      "Epoch 129, Loss 20.343742\n",
      "    Params: tensor([ 2.8810, -3.2277])\n",
      "    Grad:   tensor([-0.4232,  2.3954])\n",
      "Epoch 130, Loss 20.284624\n",
      "    Params: tensor([ 2.8852, -3.2516])\n",
      "    Grad:   tensor([-0.4224,  2.3914])\n",
      "Epoch 131, Loss 20.225702\n",
      "    Params: tensor([ 2.8894, -3.2755])\n",
      "    Grad:   tensor([-0.4217,  2.3873])\n",
      "Epoch 132, Loss 20.166981\n",
      "    Params: tensor([ 2.8936, -3.2993])\n",
      "    Grad:   tensor([-0.4210,  2.3832])\n",
      "Epoch 133, Loss 20.108461\n",
      "    Params: tensor([ 2.8978, -3.3231])\n",
      "    Grad:   tensor([-0.4203,  2.3792])\n",
      "Epoch 134, Loss 20.050137\n",
      "    Params: tensor([ 2.9020, -3.3469])\n",
      "    Grad:   tensor([-0.4196,  2.3752])\n",
      "Epoch 135, Loss 19.992016\n",
      "    Params: tensor([ 2.9062, -3.3706])\n",
      "    Grad:   tensor([-0.4189,  2.3711])\n",
      "Epoch 136, Loss 19.934086\n",
      "    Params: tensor([ 2.9104, -3.3942])\n",
      "    Grad:   tensor([-0.4182,  2.3671])\n",
      "Epoch 137, Loss 19.876352\n",
      "    Params: tensor([ 2.9146, -3.4179])\n",
      "    Grad:   tensor([-0.4174,  2.3631])\n",
      "Epoch 138, Loss 19.818823\n",
      "    Params: tensor([ 2.9187, -3.4415])\n",
      "    Grad:   tensor([-0.4167,  2.3591])\n",
      "Epoch 139, Loss 19.761480\n",
      "    Params: tensor([ 2.9229, -3.4650])\n",
      "    Grad:   tensor([-0.4160,  2.3550])\n",
      "Epoch 140, Loss 19.704336\n",
      "    Params: tensor([ 2.9270, -3.4885])\n",
      "    Grad:   tensor([-0.4153,  2.3510])\n",
      "Epoch 141, Loss 19.647385\n",
      "    Params: tensor([ 2.9312, -3.5120])\n",
      "    Grad:   tensor([-0.4146,  2.3471])\n",
      "Epoch 142, Loss 19.590626\n",
      "    Params: tensor([ 2.9353, -3.5354])\n",
      "    Grad:   tensor([-0.4139,  2.3431])\n",
      "Epoch 143, Loss 19.534061\n",
      "    Params: tensor([ 2.9395, -3.5588])\n",
      "    Grad:   tensor([-0.4132,  2.3391])\n",
      "Epoch 144, Loss 19.477690\n",
      "    Params: tensor([ 2.9436, -3.5822])\n",
      "    Grad:   tensor([-0.4125,  2.3351])\n",
      "Epoch 145, Loss 19.421507\n",
      "    Params: tensor([ 2.9477, -3.6055])\n",
      "    Grad:   tensor([-0.4118,  2.3311])\n",
      "Epoch 146, Loss 19.365515\n",
      "    Params: tensor([ 2.9518, -3.6287])\n",
      "    Grad:   tensor([-0.4111,  2.3272])\n",
      "Epoch 147, Loss 19.309715\n",
      "    Params: tensor([ 2.9559, -3.6520])\n",
      "    Grad:   tensor([-0.4104,  2.3232])\n",
      "Epoch 148, Loss 19.254107\n",
      "    Params: tensor([ 2.9600, -3.6752])\n",
      "    Grad:   tensor([-0.4097,  2.3193])\n",
      "Epoch 149, Loss 19.198685\n",
      "    Params: tensor([ 2.9641, -3.6983])\n",
      "    Grad:   tensor([-0.4090,  2.3153])\n",
      "Epoch 150, Loss 19.143446\n",
      "    Params: tensor([ 2.9682, -3.7214])\n",
      "    Grad:   tensor([-0.4083,  2.3114])\n",
      "Epoch 151, Loss 19.088402\n",
      "    Params: tensor([ 2.9723, -3.7445])\n",
      "    Grad:   tensor([-0.4076,  2.3075])\n",
      "Epoch 152, Loss 19.033543\n",
      "    Params: tensor([ 2.9763, -3.7675])\n",
      "    Grad:   tensor([-0.4069,  2.3036])\n",
      "Epoch 153, Loss 18.978868\n",
      "    Params: tensor([ 2.9804, -3.7905])\n",
      "    Grad:   tensor([-0.4062,  2.2997])\n",
      "Epoch 154, Loss 18.924377\n",
      "    Params: tensor([ 2.9844, -3.8135])\n",
      "    Grad:   tensor([-0.4056,  2.2957])\n",
      "Epoch 155, Loss 18.870081\n",
      "    Params: tensor([ 2.9885, -3.8364])\n",
      "    Grad:   tensor([-0.4049,  2.2918])\n",
      "Epoch 156, Loss 18.815960\n",
      "    Params: tensor([ 2.9925, -3.8593])\n",
      "    Grad:   tensor([-0.4042,  2.2880])\n",
      "Epoch 157, Loss 18.762022\n",
      "    Params: tensor([ 2.9966, -3.8821])\n",
      "    Grad:   tensor([-0.4035,  2.2841])\n",
      "Epoch 158, Loss 18.708271\n",
      "    Params: tensor([ 3.0006, -3.9049])\n",
      "    Grad:   tensor([-0.4028,  2.2802])\n",
      "Epoch 159, Loss 18.654699\n",
      "    Params: tensor([ 3.0046, -3.9277])\n",
      "    Grad:   tensor([-0.4021,  2.2763])\n",
      "Epoch 160, Loss 18.601313\n",
      "    Params: tensor([ 3.0086, -3.9504])\n",
      "    Grad:   tensor([-0.4014,  2.2724])\n",
      "Epoch 161, Loss 18.548109\n",
      "    Params: tensor([ 3.0126, -3.9731])\n",
      "    Grad:   tensor([-0.4007,  2.2686])\n",
      "Epoch 162, Loss 18.495085\n",
      "    Params: tensor([ 3.0166, -3.9958])\n",
      "    Grad:   tensor([-0.4001,  2.2647])\n",
      "Epoch 163, Loss 18.442236\n",
      "    Params: tensor([ 3.0206, -4.0184])\n",
      "    Grad:   tensor([-0.3994,  2.2609])\n",
      "Epoch 164, Loss 18.389570\n",
      "    Params: tensor([ 3.0246, -4.0409])\n",
      "    Grad:   tensor([-0.3987,  2.2570])\n",
      "Epoch 165, Loss 18.337080\n",
      "    Params: tensor([ 3.0286, -4.0635])\n",
      "    Grad:   tensor([-0.3980,  2.2532])\n",
      "Epoch 166, Loss 18.284777\n",
      "    Params: tensor([ 3.0326, -4.0860])\n",
      "    Grad:   tensor([-0.3974,  2.2494])\n",
      "Epoch 167, Loss 18.232641\n",
      "    Params: tensor([ 3.0365, -4.1084])\n",
      "    Grad:   tensor([-0.3967,  2.2456])\n",
      "Epoch 168, Loss 18.180685\n",
      "    Params: tensor([ 3.0405, -4.1308])\n",
      "    Grad:   tensor([-0.3960,  2.2417])\n",
      "Epoch 169, Loss 18.128906\n",
      "    Params: tensor([ 3.0445, -4.1532])\n",
      "    Grad:   tensor([-0.3953,  2.2379])\n",
      "Epoch 170, Loss 18.077301\n",
      "    Params: tensor([ 3.0484, -4.1756])\n",
      "    Grad:   tensor([-0.3947,  2.2341])\n",
      "Epoch 171, Loss 18.025877\n",
      "    Params: tensor([ 3.0523, -4.1979])\n",
      "    Grad:   tensor([-0.3940,  2.2303])\n",
      "Epoch 172, Loss 17.974623\n",
      "    Params: tensor([ 3.0563, -4.2201])\n",
      "    Grad:   tensor([-0.3933,  2.2266])\n",
      "Epoch 173, Loss 17.923546\n",
      "    Params: tensor([ 3.0602, -4.2424])\n",
      "    Grad:   tensor([-0.3927,  2.2228])\n",
      "Epoch 174, Loss 17.872643\n",
      "    Params: tensor([ 3.0641, -4.2646])\n",
      "    Grad:   tensor([-0.3920,  2.2190])\n",
      "Epoch 175, Loss 17.821909\n",
      "    Params: tensor([ 3.0680, -4.2867])\n",
      "    Grad:   tensor([-0.3913,  2.2152])\n",
      "Epoch 176, Loss 17.771345\n",
      "    Params: tensor([ 3.0719, -4.3088])\n",
      "    Grad:   tensor([-0.3907,  2.2115])\n",
      "Epoch 177, Loss 17.720955\n",
      "    Params: tensor([ 3.0758, -4.3309])\n",
      "    Grad:   tensor([-0.3900,  2.2077])\n",
      "Epoch 178, Loss 17.670738\n",
      "    Params: tensor([ 3.0797, -4.3529])\n",
      "    Grad:   tensor([-0.3893,  2.2040])\n",
      "Epoch 179, Loss 17.620689\n",
      "    Params: tensor([ 3.0836, -4.3749])\n",
      "    Grad:   tensor([-0.3887,  2.2002])\n",
      "Epoch 180, Loss 17.570814\n",
      "    Params: tensor([ 3.0875, -4.3969])\n",
      "    Grad:   tensor([-0.3880,  2.1965])\n",
      "Epoch 181, Loss 17.521103\n",
      "    Params: tensor([ 3.0914, -4.4188])\n",
      "    Grad:   tensor([-0.3873,  2.1927])\n",
      "Epoch 182, Loss 17.471565\n",
      "    Params: tensor([ 3.0952, -4.4407])\n",
      "    Grad:   tensor([-0.3867,  2.1890])\n",
      "Epoch 183, Loss 17.422192\n",
      "    Params: tensor([ 3.0991, -4.4626])\n",
      "    Grad:   tensor([-0.3860,  2.1853])\n",
      "Epoch 184, Loss 17.372993\n",
      "    Params: tensor([ 3.1030, -4.4844])\n",
      "    Grad:   tensor([-0.3854,  2.1816])\n",
      "Epoch 185, Loss 17.323954\n",
      "    Params: tensor([ 3.1068, -4.5062])\n",
      "    Grad:   tensor([-0.3847,  2.1779])\n",
      "Epoch 186, Loss 17.275084\n",
      "    Params: tensor([ 3.1106, -4.5279])\n",
      "    Grad:   tensor([-0.3841,  2.1742])\n",
      "Epoch 187, Loss 17.226379\n",
      "    Params: tensor([ 3.1145, -4.5496])\n",
      "    Grad:   tensor([-0.3834,  2.1705])\n",
      "Epoch 188, Loss 17.177839\n",
      "    Params: tensor([ 3.1183, -4.5713])\n",
      "    Grad:   tensor([-0.3828,  2.1668])\n",
      "Epoch 189, Loss 17.129463\n",
      "    Params: tensor([ 3.1221, -4.5929])\n",
      "    Grad:   tensor([-0.3821,  2.1631])\n",
      "Epoch 190, Loss 17.081255\n",
      "    Params: tensor([ 3.1259, -4.6145])\n",
      "    Grad:   tensor([-0.3815,  2.1594])\n",
      "Epoch 191, Loss 17.033209\n",
      "    Params: tensor([ 3.1298, -4.6361])\n",
      "    Grad:   tensor([-0.3808,  2.1558])\n",
      "Epoch 192, Loss 16.985327\n",
      "    Params: tensor([ 3.1336, -4.6576])\n",
      "    Grad:   tensor([-0.3802,  2.1521])\n",
      "Epoch 193, Loss 16.937605\n",
      "    Params: tensor([ 3.1374, -4.6791])\n",
      "    Grad:   tensor([-0.3795,  2.1485])\n",
      "Epoch 194, Loss 16.890047\n",
      "    Params: tensor([ 3.1411, -4.7005])\n",
      "    Grad:   tensor([-0.3789,  2.1448])\n",
      "Epoch 195, Loss 16.842649\n",
      "    Params: tensor([ 3.1449, -4.7219])\n",
      "    Grad:   tensor([-0.3782,  2.1412])\n",
      "Epoch 196, Loss 16.795412\n",
      "    Params: tensor([ 3.1487, -4.7433])\n",
      "    Grad:   tensor([-0.3776,  2.1375])\n",
      "Epoch 197, Loss 16.748339\n",
      "    Params: tensor([ 3.1525, -4.7646])\n",
      "    Grad:   tensor([-0.3770,  2.1339])\n",
      "Epoch 198, Loss 16.701422\n",
      "    Params: tensor([ 3.1562, -4.7859])\n",
      "    Grad:   tensor([-0.3763,  2.1303])\n",
      "Epoch 199, Loss 16.654661\n",
      "    Params: tensor([ 3.1600, -4.8072])\n",
      "    Grad:   tensor([-0.3757,  2.1267])\n",
      "Epoch 200, Loss 16.608067\n",
      "    Params: tensor([ 3.1637, -4.8284])\n",
      "    Grad:   tensor([-0.3750,  2.1230])\n",
      "Epoch 201, Loss 16.561623\n",
      "    Params: tensor([ 3.1675, -4.8496])\n",
      "    Grad:   tensor([-0.3744,  2.1194])\n",
      "Epoch 202, Loss 16.515343\n",
      "    Params: tensor([ 3.1712, -4.8708])\n",
      "    Grad:   tensor([-0.3738,  2.1158])\n",
      "Epoch 203, Loss 16.469219\n",
      "    Params: tensor([ 3.1750, -4.8919])\n",
      "    Grad:   tensor([-0.3731,  2.1122])\n",
      "Epoch 204, Loss 16.423248\n",
      "    Params: tensor([ 3.1787, -4.9130])\n",
      "    Grad:   tensor([-0.3725,  2.1087])\n",
      "Epoch 205, Loss 16.377434\n",
      "    Params: tensor([ 3.1824, -4.9341])\n",
      "    Grad:   tensor([-0.3719,  2.1051])\n",
      "Epoch 206, Loss 16.331776\n",
      "    Params: tensor([ 3.1861, -4.9551])\n",
      "    Grad:   tensor([-0.3712,  2.1015])\n",
      "Epoch 207, Loss 16.286276\n",
      "    Params: tensor([ 3.1898, -4.9760])\n",
      "    Grad:   tensor([-0.3706,  2.0979])\n",
      "Epoch 208, Loss 16.240929\n",
      "    Params: tensor([ 3.1935, -4.9970])\n",
      "    Grad:   tensor([-0.3700,  2.0944])\n",
      "Epoch 209, Loss 16.195732\n",
      "    Params: tensor([ 3.1972, -5.0179])\n",
      "    Grad:   tensor([-0.3694,  2.0908])\n",
      "Epoch 210, Loss 16.150694\n",
      "    Params: tensor([ 3.2009, -5.0388])\n",
      "    Grad:   tensor([-0.3687,  2.0873])\n",
      "Epoch 211, Loss 16.105806\n",
      "    Params: tensor([ 3.2046, -5.0596])\n",
      "    Grad:   tensor([-0.3681,  2.0837])\n",
      "Epoch 212, Loss 16.061073\n",
      "    Params: tensor([ 3.2082, -5.0804])\n",
      "    Grad:   tensor([-0.3675,  2.0802])\n",
      "Epoch 213, Loss 16.016487\n",
      "    Params: tensor([ 3.2119, -5.1012])\n",
      "    Grad:   tensor([-0.3668,  2.0766])\n",
      "Epoch 214, Loss 15.972058\n",
      "    Params: tensor([ 3.2156, -5.1219])\n",
      "    Grad:   tensor([-0.3662,  2.0731])\n",
      "Epoch 215, Loss 15.927776\n",
      "    Params: tensor([ 3.2192, -5.1426])\n",
      "    Grad:   tensor([-0.3656,  2.0696])\n",
      "Epoch 216, Loss 15.883645\n",
      "    Params: tensor([ 3.2229, -5.1633])\n",
      "    Grad:   tensor([-0.3650,  2.0661])\n",
      "Epoch 217, Loss 15.839664\n",
      "    Params: tensor([ 3.2265, -5.1839])\n",
      "    Grad:   tensor([-0.3644,  2.0626])\n",
      "Epoch 218, Loss 15.795832\n",
      "    Params: tensor([ 3.2302, -5.2045])\n",
      "    Grad:   tensor([-0.3637,  2.0591])\n",
      "Epoch 219, Loss 15.752152\n",
      "    Params: tensor([ 3.2338, -5.2250])\n",
      "    Grad:   tensor([-0.3631,  2.0556])\n",
      "Epoch 220, Loss 15.708612\n",
      "    Params: tensor([ 3.2374, -5.2456])\n",
      "    Grad:   tensor([-0.3625,  2.0521])\n",
      "Epoch 221, Loss 15.665226\n",
      "    Params: tensor([ 3.2410, -5.2660])\n",
      "    Grad:   tensor([-0.3619,  2.0486])\n",
      "Epoch 222, Loss 15.621990\n",
      "    Params: tensor([ 3.2447, -5.2865])\n",
      "    Grad:   tensor([-0.3613,  2.0451])\n",
      "Epoch 223, Loss 15.578897\n",
      "    Params: tensor([ 3.2483, -5.3069])\n",
      "    Grad:   tensor([-0.3607,  2.0416])\n",
      "Epoch 224, Loss 15.535950\n",
      "    Params: tensor([ 3.2519, -5.3273])\n",
      "    Grad:   tensor([-0.3601,  2.0382])\n",
      "Epoch 225, Loss 15.493150\n",
      "    Params: tensor([ 3.2555, -5.3476])\n",
      "    Grad:   tensor([-0.3594,  2.0347])\n",
      "Epoch 226, Loss 15.450495\n",
      "    Params: tensor([ 3.2590, -5.3680])\n",
      "    Grad:   tensor([-0.3588,  2.0312])\n",
      "Epoch 227, Loss 15.407981\n",
      "    Params: tensor([ 3.2626, -5.3882])\n",
      "    Grad:   tensor([-0.3582,  2.0278])\n",
      "Epoch 228, Loss 15.365616\n",
      "    Params: tensor([ 3.2662, -5.4085])\n",
      "    Grad:   tensor([-0.3576,  2.0243])\n",
      "Epoch 229, Loss 15.323396\n",
      "    Params: tensor([ 3.2698, -5.4287])\n",
      "    Grad:   tensor([-0.3570,  2.0209])\n",
      "Epoch 230, Loss 15.281317\n",
      "    Params: tensor([ 3.2733, -5.4489])\n",
      "    Grad:   tensor([-0.3564,  2.0175])\n",
      "Epoch 231, Loss 15.239380\n",
      "    Params: tensor([ 3.2769, -5.4690])\n",
      "    Grad:   tensor([-0.3558,  2.0140])\n",
      "Epoch 232, Loss 15.197585\n",
      "    Params: tensor([ 3.2804, -5.4891])\n",
      "    Grad:   tensor([-0.3552,  2.0106])\n",
      "Epoch 233, Loss 15.155932\n",
      "    Params: tensor([ 3.2840, -5.5092])\n",
      "    Grad:   tensor([-0.3546,  2.0072])\n",
      "Epoch 234, Loss 15.114425\n",
      "    Params: tensor([ 3.2875, -5.5292])\n",
      "    Grad:   tensor([-0.3540,  2.0038])\n",
      "Epoch 235, Loss 15.073055\n",
      "    Params: tensor([ 3.2911, -5.5492])\n",
      "    Grad:   tensor([-0.3534,  2.0004])\n",
      "Epoch 236, Loss 15.031823\n",
      "    Params: tensor([ 3.2946, -5.5692])\n",
      "    Grad:   tensor([-0.3528,  1.9970])\n",
      "Epoch 237, Loss 14.990734\n",
      "    Params: tensor([ 3.2981, -5.5891])\n",
      "    Grad:   tensor([-0.3522,  1.9936])\n",
      "Epoch 238, Loss 14.949784\n",
      "    Params: tensor([ 3.3016, -5.6090])\n",
      "    Grad:   tensor([-0.3516,  1.9902])\n",
      "Epoch 239, Loss 14.908973\n",
      "    Params: tensor([ 3.3051, -5.6289])\n",
      "    Grad:   tensor([-0.3510,  1.9868])\n",
      "Epoch 240, Loss 14.868304\n",
      "    Params: tensor([ 3.3086, -5.6487])\n",
      "    Grad:   tensor([-0.3504,  1.9835])\n",
      "Epoch 241, Loss 14.827767\n",
      "    Params: tensor([ 3.3121, -5.6685])\n",
      "    Grad:   tensor([-0.3498,  1.9801])\n",
      "Epoch 242, Loss 14.787370\n",
      "    Params: tensor([ 3.3156, -5.6883])\n",
      "    Grad:   tensor([-0.3492,  1.9767])\n",
      "Epoch 243, Loss 14.747109\n",
      "    Params: tensor([ 3.3191, -5.7080])\n",
      "    Grad:   tensor([-0.3486,  1.9734])\n",
      "Epoch 244, Loss 14.706989\n",
      "    Params: tensor([ 3.3226, -5.7277])\n",
      "    Grad:   tensor([-0.3480,  1.9700])\n",
      "Epoch 245, Loss 14.667002\n",
      "    Params: tensor([ 3.3261, -5.7474])\n",
      "    Grad:   tensor([-0.3474,  1.9667])\n",
      "Epoch 246, Loss 14.627151\n",
      "    Params: tensor([ 3.3295, -5.7670])\n",
      "    Grad:   tensor([-0.3468,  1.9633])\n",
      "Epoch 247, Loss 14.587436\n",
      "    Params: tensor([ 3.3330, -5.7866])\n",
      "    Grad:   tensor([-0.3462,  1.9600])\n",
      "Epoch 248, Loss 14.547855\n",
      "    Params: tensor([ 3.3365, -5.8062])\n",
      "    Grad:   tensor([-0.3456,  1.9567])\n",
      "Epoch 249, Loss 14.508409\n",
      "    Params: tensor([ 3.3399, -5.8257])\n",
      "    Grad:   tensor([-0.3451,  1.9533])\n",
      "Epoch 250, Loss 14.469097\n",
      "    Params: tensor([ 3.3434, -5.8452])\n",
      "    Grad:   tensor([-0.3445,  1.9500])\n",
      "Epoch 251, Loss 14.429920\n",
      "    Params: tensor([ 3.3468, -5.8647])\n",
      "    Grad:   tensor([-0.3439,  1.9467])\n",
      "Epoch 252, Loss 14.390870\n",
      "    Params: tensor([ 3.3502, -5.8841])\n",
      "    Grad:   tensor([-0.3433,  1.9434])\n",
      "Epoch 253, Loss 14.351956\n",
      "    Params: tensor([ 3.3537, -5.9035])\n",
      "    Grad:   tensor([-0.3427,  1.9401])\n",
      "Epoch 254, Loss 14.313177\n",
      "    Params: tensor([ 3.3571, -5.9229])\n",
      "    Grad:   tensor([-0.3421,  1.9368])\n",
      "Epoch 255, Loss 14.274529\n",
      "    Params: tensor([ 3.3605, -5.9422])\n",
      "    Grad:   tensor([-0.3416,  1.9335])\n",
      "Epoch 256, Loss 14.236009\n",
      "    Params: tensor([ 3.3639, -5.9615])\n",
      "    Grad:   tensor([-0.3410,  1.9302])\n",
      "Epoch 257, Loss 14.197620\n",
      "    Params: tensor([ 3.3673, -5.9808])\n",
      "    Grad:   tensor([-0.3404,  1.9269])\n",
      "Epoch 258, Loss 14.159363\n",
      "    Params: tensor([ 3.3707, -6.0000])\n",
      "    Grad:   tensor([-0.3398,  1.9237])\n",
      "Epoch 259, Loss 14.121234\n",
      "    Params: tensor([ 3.3741, -6.0192])\n",
      "    Grad:   tensor([-0.3392,  1.9204])\n",
      "Epoch 260, Loss 14.083236\n",
      "    Params: tensor([ 3.3775, -6.0384])\n",
      "    Grad:   tensor([-0.3387,  1.9171])\n",
      "Epoch 261, Loss 14.045367\n",
      "    Params: tensor([ 3.3809, -6.0576])\n",
      "    Grad:   tensor([-0.3381,  1.9139])\n",
      "Epoch 262, Loss 14.007627\n",
      "    Params: tensor([ 3.3842, -6.0767])\n",
      "    Grad:   tensor([-0.3375,  1.9106])\n",
      "Epoch 263, Loss 13.970016\n",
      "    Params: tensor([ 3.3876, -6.0957])\n",
      "    Grad:   tensor([-0.3369,  1.9074])\n",
      "Epoch 264, Loss 13.932531\n",
      "    Params: tensor([ 3.3910, -6.1148])\n",
      "    Grad:   tensor([-0.3364,  1.9041])\n",
      "Epoch 265, Loss 13.895172\n",
      "    Params: tensor([ 3.3943, -6.1338])\n",
      "    Grad:   tensor([-0.3358,  1.9009])\n",
      "Epoch 266, Loss 13.857944\n",
      "    Params: tensor([ 3.3977, -6.1528])\n",
      "    Grad:   tensor([-0.3352,  1.8977])\n",
      "Epoch 267, Loss 13.820837\n",
      "    Params: tensor([ 3.4010, -6.1717])\n",
      "    Grad:   tensor([-0.3347,  1.8945])\n",
      "Epoch 268, Loss 13.783858\n",
      "    Params: tensor([ 3.4044, -6.1906])\n",
      "    Grad:   tensor([-0.3341,  1.8912])\n",
      "Epoch 269, Loss 13.747006\n",
      "    Params: tensor([ 3.4077, -6.2095])\n",
      "    Grad:   tensor([-0.3335,  1.8880])\n",
      "Epoch 270, Loss 13.710278\n",
      "    Params: tensor([ 3.4110, -6.2284])\n",
      "    Grad:   tensor([-0.3330,  1.8848])\n",
      "Epoch 271, Loss 13.673676\n",
      "    Params: tensor([ 3.4144, -6.2472])\n",
      "    Grad:   tensor([-0.3324,  1.8816])\n",
      "Epoch 272, Loss 13.637196\n",
      "    Params: tensor([ 3.4177, -6.2660])\n",
      "    Grad:   tensor([-0.3318,  1.8784])\n",
      "Epoch 273, Loss 13.600842\n",
      "    Params: tensor([ 3.4210, -6.2847])\n",
      "    Grad:   tensor([-0.3313,  1.8752])\n",
      "Epoch 274, Loss 13.564609\n",
      "    Params: tensor([ 3.4243, -6.3034])\n",
      "    Grad:   tensor([-0.3307,  1.8720])\n",
      "Epoch 275, Loss 13.528501\n",
      "    Params: tensor([ 3.4276, -6.3221])\n",
      "    Grad:   tensor([-0.3301,  1.8689])\n",
      "Epoch 276, Loss 13.492514\n",
      "    Params: tensor([ 3.4309, -6.3408])\n",
      "    Grad:   tensor([-0.3296,  1.8657])\n",
      "Epoch 277, Loss 13.456651\n",
      "    Params: tensor([ 3.4342, -6.3594])\n",
      "    Grad:   tensor([-0.3290,  1.8625])\n",
      "Epoch 278, Loss 13.420910\n",
      "    Params: tensor([ 3.4375, -6.3780])\n",
      "    Grad:   tensor([-0.3285,  1.8594])\n",
      "Epoch 279, Loss 13.385287\n",
      "    Params: tensor([ 3.4407, -6.3966])\n",
      "    Grad:   tensor([-0.3279,  1.8562])\n",
      "Epoch 280, Loss 13.349789\n",
      "    Params: tensor([ 3.4440, -6.4151])\n",
      "    Grad:   tensor([-0.3274,  1.8530])\n",
      "Epoch 281, Loss 13.314407\n",
      "    Params: tensor([ 3.4473, -6.4336])\n",
      "    Grad:   tensor([-0.3268,  1.8499])\n",
      "Epoch 282, Loss 13.279150\n",
      "    Params: tensor([ 3.4506, -6.4520])\n",
      "    Grad:   tensor([-0.3262,  1.8468])\n",
      "Epoch 283, Loss 13.244009\n",
      "    Params: tensor([ 3.4538, -6.4705])\n",
      "    Grad:   tensor([-0.3257,  1.8436])\n",
      "Epoch 284, Loss 13.208991\n",
      "    Params: tensor([ 3.4571, -6.4889])\n",
      "    Grad:   tensor([-0.3251,  1.8405])\n",
      "Epoch 285, Loss 13.174088\n",
      "    Params: tensor([ 3.4603, -6.5073])\n",
      "    Grad:   tensor([-0.3246,  1.8374])\n",
      "Epoch 286, Loss 13.139307\n",
      "    Params: tensor([ 3.4635, -6.5256])\n",
      "    Grad:   tensor([-0.3240,  1.8342])\n",
      "Epoch 287, Loss 13.104639\n",
      "    Params: tensor([ 3.4668, -6.5439])\n",
      "    Grad:   tensor([-0.3235,  1.8311])\n",
      "Epoch 288, Loss 13.070092\n",
      "    Params: tensor([ 3.4700, -6.5622])\n",
      "    Grad:   tensor([-0.3229,  1.8280])\n",
      "Epoch 289, Loss 13.035664\n",
      "    Params: tensor([ 3.4732, -6.5804])\n",
      "    Grad:   tensor([-0.3224,  1.8249])\n",
      "Epoch 290, Loss 13.001349\n",
      "    Params: tensor([ 3.4765, -6.5987])\n",
      "    Grad:   tensor([-0.3218,  1.8218])\n",
      "Epoch 291, Loss 12.967152\n",
      "    Params: tensor([ 3.4797, -6.6169])\n",
      "    Grad:   tensor([-0.3213,  1.8187])\n",
      "Epoch 292, Loss 12.933075\n",
      "    Params: tensor([ 3.4829, -6.6350])\n",
      "    Grad:   tensor([-0.3207,  1.8156])\n",
      "Epoch 293, Loss 12.899109\n",
      "    Params: tensor([ 3.4861, -6.6531])\n",
      "    Grad:   tensor([-0.3202,  1.8125])\n",
      "Epoch 294, Loss 12.865259\n",
      "    Params: tensor([ 3.4893, -6.6712])\n",
      "    Grad:   tensor([-0.3196,  1.8095])\n",
      "Epoch 295, Loss 12.831525\n",
      "    Params: tensor([ 3.4925, -6.6893])\n",
      "    Grad:   tensor([-0.3191,  1.8064])\n",
      "Epoch 296, Loss 12.797904\n",
      "    Params: tensor([ 3.4956, -6.7073])\n",
      "    Grad:   tensor([-0.3186,  1.8033])\n",
      "Epoch 297, Loss 12.764399\n",
      "    Params: tensor([ 3.4988, -6.7253])\n",
      "    Grad:   tensor([-0.3180,  1.8003])\n",
      "Epoch 298, Loss 12.731007\n",
      "    Params: tensor([ 3.5020, -6.7433])\n",
      "    Grad:   tensor([-0.3175,  1.7972])\n",
      "Epoch 299, Loss 12.697727\n",
      "    Params: tensor([ 3.5052, -6.7612])\n",
      "    Grad:   tensor([-0.3169,  1.7941])\n",
      "Epoch 300, Loss 12.664559\n",
      "    Params: tensor([ 3.5083, -6.7792])\n",
      "    Grad:   tensor([-0.3164,  1.7911])\n",
      "Epoch 301, Loss 12.631507\n",
      "    Params: tensor([ 3.5115, -6.7970])\n",
      "    Grad:   tensor([-0.3159,  1.7881])\n",
      "Epoch 302, Loss 12.598568\n",
      "    Params: tensor([ 3.5146, -6.8149])\n",
      "    Grad:   tensor([-0.3153,  1.7850])\n",
      "Epoch 303, Loss 12.565738\n",
      "    Params: tensor([ 3.5178, -6.8327])\n",
      "    Grad:   tensor([-0.3148,  1.7820])\n",
      "Epoch 304, Loss 12.533021\n",
      "    Params: tensor([ 3.5209, -6.8505])\n",
      "    Grad:   tensor([-0.3143,  1.7790])\n",
      "Epoch 305, Loss 12.500413\n",
      "    Params: tensor([ 3.5241, -6.8683])\n",
      "    Grad:   tensor([-0.3137,  1.7759])\n",
      "Epoch 306, Loss 12.467919\n",
      "    Params: tensor([ 3.5272, -6.8860])\n",
      "    Grad:   tensor([-0.3132,  1.7729])\n",
      "Epoch 307, Loss 12.435532\n",
      "    Params: tensor([ 3.5303, -6.9037])\n",
      "    Grad:   tensor([-0.3127,  1.7699])\n",
      "Epoch 308, Loss 12.403256\n",
      "    Params: tensor([ 3.5335, -6.9213])\n",
      "    Grad:   tensor([-0.3121,  1.7669])\n",
      "Epoch 309, Loss 12.371090\n",
      "    Params: tensor([ 3.5366, -6.9390])\n",
      "    Grad:   tensor([-0.3116,  1.7639])\n",
      "Epoch 310, Loss 12.339031\n",
      "    Params: tensor([ 3.5397, -6.9566])\n",
      "    Grad:   tensor([-0.3111,  1.7609])\n",
      "Epoch 311, Loss 12.307082\n",
      "    Params: tensor([ 3.5428, -6.9742])\n",
      "    Grad:   tensor([-0.3105,  1.7579])\n",
      "Epoch 312, Loss 12.275247\n",
      "    Params: tensor([ 3.5459, -6.9917])\n",
      "    Grad:   tensor([-0.3100,  1.7549])\n",
      "Epoch 313, Loss 12.243509\n",
      "    Params: tensor([ 3.5490, -7.0092])\n",
      "    Grad:   tensor([-0.3095,  1.7519])\n",
      "Epoch 314, Loss 12.211887\n",
      "    Params: tensor([ 3.5521, -7.0267])\n",
      "    Grad:   tensor([-0.3090,  1.7490])\n",
      "Epoch 315, Loss 12.180370\n",
      "    Params: tensor([ 3.5552, -7.0442])\n",
      "    Grad:   tensor([-0.3084,  1.7460])\n",
      "Epoch 316, Loss 12.148962\n",
      "    Params: tensor([ 3.5582, -7.0616])\n",
      "    Grad:   tensor([-0.3079,  1.7430])\n",
      "Epoch 317, Loss 12.117657\n",
      "    Params: tensor([ 3.5613, -7.0790])\n",
      "    Grad:   tensor([-0.3074,  1.7401])\n",
      "Epoch 318, Loss 12.086462\n",
      "    Params: tensor([ 3.5644, -7.0964])\n",
      "    Grad:   tensor([-0.3069,  1.7371])\n",
      "Epoch 319, Loss 12.055373\n",
      "    Params: tensor([ 3.5674, -7.1137])\n",
      "    Grad:   tensor([-0.3063,  1.7342])\n",
      "Epoch 320, Loss 12.024384\n",
      "    Params: tensor([ 3.5705, -7.1310])\n",
      "    Grad:   tensor([-0.3058,  1.7312])\n",
      "Epoch 321, Loss 11.993508\n",
      "    Params: tensor([ 3.5736, -7.1483])\n",
      "    Grad:   tensor([-0.3053,  1.7283])\n",
      "Epoch 322, Loss 11.962731\n",
      "    Params: tensor([ 3.5766, -7.1656])\n",
      "    Grad:   tensor([-0.3048,  1.7253])\n",
      "Epoch 323, Loss 11.932056\n",
      "    Params: tensor([ 3.5796, -7.1828])\n",
      "    Grad:   tensor([-0.3043,  1.7224])\n",
      "Epoch 324, Loss 11.901492\n",
      "    Params: tensor([ 3.5827, -7.2000])\n",
      "    Grad:   tensor([-0.3037,  1.7195])\n",
      "Epoch 325, Loss 11.871029\n",
      "    Params: tensor([ 3.5857, -7.2172])\n",
      "    Grad:   tensor([-0.3032,  1.7166])\n",
      "Epoch 326, Loss 11.840671\n",
      "    Params: tensor([ 3.5887, -7.2343])\n",
      "    Grad:   tensor([-0.3027,  1.7136])\n",
      "Epoch 327, Loss 11.810413\n",
      "    Params: tensor([ 3.5918, -7.2514])\n",
      "    Grad:   tensor([-0.3022,  1.7107])\n",
      "Epoch 328, Loss 11.780257\n",
      "    Params: tensor([ 3.5948, -7.2685])\n",
      "    Grad:   tensor([-0.3017,  1.7078])\n",
      "Epoch 329, Loss 11.750208\n",
      "    Params: tensor([ 3.5978, -7.2855])\n",
      "    Grad:   tensor([-0.3012,  1.7049])\n",
      "Epoch 330, Loss 11.720258\n",
      "    Params: tensor([ 3.6008, -7.3026])\n",
      "    Grad:   tensor([-0.3007,  1.7020])\n",
      "Epoch 331, Loss 11.690412\n",
      "    Params: tensor([ 3.6038, -7.3196])\n",
      "    Grad:   tensor([-0.3002,  1.6991])\n",
      "Epoch 332, Loss 11.660664\n",
      "    Params: tensor([ 3.6068, -7.3365])\n",
      "    Grad:   tensor([-0.2996,  1.6963])\n",
      "Epoch 333, Loss 11.631015\n",
      "    Params: tensor([ 3.6098, -7.3535])\n",
      "    Grad:   tensor([-0.2991,  1.6934])\n",
      "Epoch 334, Loss 11.601473\n",
      "    Params: tensor([ 3.6128, -7.3704])\n",
      "    Grad:   tensor([-0.2986,  1.6905])\n",
      "Epoch 335, Loss 11.572030\n",
      "    Params: tensor([ 3.6158, -7.3872])\n",
      "    Grad:   tensor([-0.2981,  1.6876])\n",
      "Epoch 336, Loss 11.542686\n",
      "    Params: tensor([ 3.6187, -7.4041])\n",
      "    Grad:   tensor([-0.2976,  1.6848])\n",
      "Epoch 337, Loss 11.513440\n",
      "    Params: tensor([ 3.6217, -7.4209])\n",
      "    Grad:   tensor([-0.2971,  1.6819])\n",
      "Epoch 338, Loss 11.484293\n",
      "    Params: tensor([ 3.6247, -7.4377])\n",
      "    Grad:   tensor([-0.2966,  1.6790])\n",
      "Epoch 339, Loss 11.455246\n",
      "    Params: tensor([ 3.6276, -7.4545])\n",
      "    Grad:   tensor([-0.2961,  1.6762])\n",
      "Epoch 340, Loss 11.426300\n",
      "    Params: tensor([ 3.6306, -7.4712])\n",
      "    Grad:   tensor([-0.2956,  1.6733])\n",
      "Epoch 341, Loss 11.397448\n",
      "    Params: tensor([ 3.6335, -7.4879])\n",
      "    Grad:   tensor([-0.2951,  1.6705])\n",
      "Epoch 342, Loss 11.368696\n",
      "    Params: tensor([ 3.6365, -7.5046])\n",
      "    Grad:   tensor([-0.2946,  1.6677])\n",
      "Epoch 343, Loss 11.340043\n",
      "    Params: tensor([ 3.6394, -7.5212])\n",
      "    Grad:   tensor([-0.2941,  1.6648])\n",
      "Epoch 344, Loss 11.311487\n",
      "    Params: tensor([ 3.6424, -7.5378])\n",
      "    Grad:   tensor([-0.2936,  1.6620])\n",
      "Epoch 345, Loss 11.283028\n",
      "    Params: tensor([ 3.6453, -7.5544])\n",
      "    Grad:   tensor([-0.2931,  1.6592])\n",
      "Epoch 346, Loss 11.254662\n",
      "    Params: tensor([ 3.6482, -7.5710])\n",
      "    Grad:   tensor([-0.2926,  1.6564])\n",
      "Epoch 347, Loss 11.226396\n",
      "    Params: tensor([ 3.6511, -7.5875])\n",
      "    Grad:   tensor([-0.2921,  1.6535])\n",
      "Epoch 348, Loss 11.198220\n",
      "    Params: tensor([ 3.6541, -7.6040])\n",
      "    Grad:   tensor([-0.2916,  1.6507])\n",
      "Epoch 349, Loss 11.170150\n",
      "    Params: tensor([ 3.6570, -7.6205])\n",
      "    Grad:   tensor([-0.2911,  1.6479])\n",
      "Epoch 350, Loss 11.142170\n",
      "    Params: tensor([ 3.6599, -7.6370])\n",
      "    Grad:   tensor([-0.2906,  1.6451])\n",
      "Epoch 351, Loss 11.114282\n",
      "    Params: tensor([ 3.6628, -7.6534])\n",
      "    Grad:   tensor([-0.2901,  1.6423])\n",
      "Epoch 352, Loss 11.086491\n",
      "    Params: tensor([ 3.6657, -7.6698])\n",
      "    Grad:   tensor([-0.2896,  1.6395])\n",
      "Epoch 353, Loss 11.058797\n",
      "    Params: tensor([ 3.6686, -7.6861])\n",
      "    Grad:   tensor([-0.2892,  1.6368])\n",
      "Epoch 354, Loss 11.031193\n",
      "    Params: tensor([ 3.6714, -7.7025])\n",
      "    Grad:   tensor([-0.2886,  1.6340])\n",
      "Epoch 355, Loss 11.003686\n",
      "    Params: tensor([ 3.6743, -7.7188])\n",
      "    Grad:   tensor([-0.2882,  1.6312])\n",
      "Epoch 356, Loss 10.976270\n",
      "    Params: tensor([ 3.6772, -7.7351])\n",
      "    Grad:   tensor([-0.2877,  1.6284])\n",
      "Epoch 357, Loss 10.948948\n",
      "    Params: tensor([ 3.6801, -7.7513])\n",
      "    Grad:   tensor([-0.2872,  1.6257])\n",
      "Epoch 358, Loss 10.921719\n",
      "    Params: tensor([ 3.6829, -7.7676])\n",
      "    Grad:   tensor([-0.2867,  1.6229])\n",
      "Epoch 359, Loss 10.894581\n",
      "    Params: tensor([ 3.6858, -7.7838])\n",
      "    Grad:   tensor([-0.2862,  1.6201])\n",
      "Epoch 360, Loss 10.867537\n",
      "    Params: tensor([ 3.6887, -7.7999])\n",
      "    Grad:   tensor([-0.2857,  1.6174])\n",
      "Epoch 361, Loss 10.840583\n",
      "    Params: tensor([ 3.6915, -7.8161])\n",
      "    Grad:   tensor([-0.2852,  1.6146])\n",
      "Epoch 362, Loss 10.813721\n",
      "    Params: tensor([ 3.6944, -7.8322])\n",
      "    Grad:   tensor([-0.2847,  1.6119])\n",
      "Epoch 363, Loss 10.786950\n",
      "    Params: tensor([ 3.6972, -7.8483])\n",
      "    Grad:   tensor([-0.2843,  1.6092])\n",
      "Epoch 364, Loss 10.760270\n",
      "    Params: tensor([ 3.7000, -7.8644])\n",
      "    Grad:   tensor([-0.2838,  1.6064])\n",
      "Epoch 365, Loss 10.733681\n",
      "    Params: tensor([ 3.7029, -7.8804])\n",
      "    Grad:   tensor([-0.2833,  1.6037])\n",
      "Epoch 366, Loss 10.707184\n",
      "    Params: tensor([ 3.7057, -7.8964])\n",
      "    Grad:   tensor([-0.2828,  1.6010])\n",
      "Epoch 367, Loss 10.680775\n",
      "    Params: tensor([ 3.7085, -7.9124])\n",
      "    Grad:   tensor([-0.2823,  1.5983])\n",
      "Epoch 368, Loss 10.654454\n",
      "    Params: tensor([ 3.7113, -7.9284])\n",
      "    Grad:   tensor([-0.2819,  1.5955])\n",
      "Epoch 369, Loss 10.628225\n",
      "    Params: tensor([ 3.7142, -7.9443])\n",
      "    Grad:   tensor([-0.2814,  1.5928])\n",
      "Epoch 370, Loss 10.602086\n",
      "    Params: tensor([ 3.7170, -7.9602])\n",
      "    Grad:   tensor([-0.2809,  1.5901])\n",
      "Epoch 371, Loss 10.576034\n",
      "    Params: tensor([ 3.7198, -7.9761])\n",
      "    Grad:   tensor([-0.2804,  1.5874])\n",
      "Epoch 372, Loss 10.550071\n",
      "    Params: tensor([ 3.7226, -7.9919])\n",
      "    Grad:   tensor([-0.2799,  1.5847])\n",
      "Epoch 373, Loss 10.524194\n",
      "    Params: tensor([ 3.7254, -8.0077])\n",
      "    Grad:   tensor([-0.2795,  1.5820])\n",
      "Epoch 374, Loss 10.498409\n",
      "    Params: tensor([ 3.7282, -8.0235])\n",
      "    Grad:   tensor([-0.2790,  1.5794])\n",
      "Epoch 375, Loss 10.472707\n",
      "    Params: tensor([ 3.7309, -8.0393])\n",
      "    Grad:   tensor([-0.2785,  1.5767])\n",
      "Epoch 376, Loss 10.447093\n",
      "    Params: tensor([ 3.7337, -8.0550])\n",
      "    Grad:   tensor([-0.2780,  1.5740])\n",
      "Epoch 377, Loss 10.421569\n",
      "    Params: tensor([ 3.7365, -8.0707])\n",
      "    Grad:   tensor([-0.2776,  1.5713])\n",
      "Epoch 378, Loss 10.396132\n",
      "    Params: tensor([ 3.7393, -8.0864])\n",
      "    Grad:   tensor([-0.2771,  1.5686])\n",
      "Epoch 379, Loss 10.370779\n",
      "    Params: tensor([ 3.7420, -8.1021])\n",
      "    Grad:   tensor([-0.2766,  1.5660])\n",
      "Epoch 380, Loss 10.345510\n",
      "    Params: tensor([ 3.7448, -8.1177])\n",
      "    Grad:   tensor([-0.2762,  1.5633])\n",
      "Epoch 381, Loss 10.320328\n",
      "    Params: tensor([ 3.7476, -8.1333])\n",
      "    Grad:   tensor([-0.2757,  1.5607])\n",
      "Epoch 382, Loss 10.295234\n",
      "    Params: tensor([ 3.7503, -8.1489])\n",
      "    Grad:   tensor([-0.2752,  1.5580])\n",
      "Epoch 383, Loss 10.270224\n",
      "    Params: tensor([ 3.7531, -8.1645])\n",
      "    Grad:   tensor([-0.2748,  1.5554])\n",
      "Epoch 384, Loss 10.245296\n",
      "    Params: tensor([ 3.7558, -8.1800])\n",
      "    Grad:   tensor([-0.2743,  1.5527])\n",
      "Epoch 385, Loss 10.220457\n",
      "    Params: tensor([ 3.7585, -8.1955])\n",
      "    Grad:   tensor([-0.2738,  1.5501])\n",
      "Epoch 386, Loss 10.195701\n",
      "    Params: tensor([ 3.7613, -8.2110])\n",
      "    Grad:   tensor([-0.2734,  1.5475])\n",
      "Epoch 387, Loss 10.171029\n",
      "    Params: tensor([ 3.7640, -8.2264])\n",
      "    Grad:   tensor([-0.2729,  1.5448])\n",
      "Epoch 388, Loss 10.146438\n",
      "    Params: tensor([ 3.7667, -8.2418])\n",
      "    Grad:   tensor([-0.2724,  1.5422])\n",
      "Epoch 389, Loss 10.121935\n",
      "    Params: tensor([ 3.7694, -8.2572])\n",
      "    Grad:   tensor([-0.2720,  1.5396])\n",
      "Epoch 390, Loss 10.097512\n",
      "    Params: tensor([ 3.7722, -8.2726])\n",
      "    Grad:   tensor([-0.2715,  1.5370])\n",
      "Epoch 391, Loss 10.073173\n",
      "    Params: tensor([ 3.7749, -8.2879])\n",
      "    Grad:   tensor([-0.2711,  1.5344])\n",
      "Epoch 392, Loss 10.048919\n",
      "    Params: tensor([ 3.7776, -8.3033])\n",
      "    Grad:   tensor([-0.2706,  1.5317])\n",
      "Epoch 393, Loss 10.024743\n",
      "    Params: tensor([ 3.7803, -8.3185])\n",
      "    Grad:   tensor([-0.2701,  1.5291])\n",
      "Epoch 394, Loss 10.000652\n",
      "    Params: tensor([ 3.7830, -8.3338])\n",
      "    Grad:   tensor([-0.2697,  1.5265])\n",
      "Epoch 395, Loss 9.976640\n",
      "    Params: tensor([ 3.7857, -8.3491])\n",
      "    Grad:   tensor([-0.2692,  1.5240])\n",
      "Epoch 396, Loss 9.952712\n",
      "    Params: tensor([ 3.7884, -8.3643])\n",
      "    Grad:   tensor([-0.2688,  1.5214])\n",
      "Epoch 397, Loss 9.928862\n",
      "    Params: tensor([ 3.7910, -8.3795])\n",
      "    Grad:   tensor([-0.2683,  1.5188])\n",
      "Epoch 398, Loss 9.905093\n",
      "    Params: tensor([ 3.7937, -8.3946])\n",
      "    Grad:   tensor([-0.2678,  1.5162])\n",
      "Epoch 399, Loss 9.881409\n",
      "    Params: tensor([ 3.7964, -8.4098])\n",
      "    Grad:   tensor([-0.2674,  1.5136])\n",
      "Epoch 400, Loss 9.857804\n",
      "    Params: tensor([ 3.7991, -8.4249])\n",
      "    Grad:   tensor([-0.2669,  1.5111])\n",
      "Epoch 401, Loss 9.834277\n",
      "    Params: tensor([ 3.8017, -8.4399])\n",
      "    Grad:   tensor([-0.2665,  1.5085])\n",
      "Epoch 402, Loss 9.810831\n",
      "    Params: tensor([ 3.8044, -8.4550])\n",
      "    Grad:   tensor([-0.2660,  1.5059])\n",
      "Epoch 403, Loss 9.787466\n",
      "    Params: tensor([ 3.8070, -8.4700])\n",
      "    Grad:   tensor([-0.2656,  1.5034])\n",
      "Epoch 404, Loss 9.764176\n",
      "    Params: tensor([ 3.8097, -8.4851])\n",
      "    Grad:   tensor([-0.2651,  1.5008])\n",
      "Epoch 405, Loss 9.740973\n",
      "    Params: tensor([ 3.8123, -8.5000])\n",
      "    Grad:   tensor([-0.2647,  1.4983])\n",
      "Epoch 406, Loss 9.717843\n",
      "    Params: tensor([ 3.8150, -8.5150])\n",
      "    Grad:   tensor([-0.2642,  1.4957])\n",
      "Epoch 407, Loss 9.694793\n",
      "    Params: tensor([ 3.8176, -8.5299])\n",
      "    Grad:   tensor([-0.2638,  1.4932])\n",
      "Epoch 408, Loss 9.671824\n",
      "    Params: tensor([ 3.8202, -8.5448])\n",
      "    Grad:   tensor([-0.2633,  1.4906])\n",
      "Epoch 409, Loss 9.648926\n",
      "    Params: tensor([ 3.8229, -8.5597])\n",
      "    Grad:   tensor([-0.2629,  1.4881])\n",
      "Epoch 410, Loss 9.626110\n",
      "    Params: tensor([ 3.8255, -8.5746])\n",
      "    Grad:   tensor([-0.2624,  1.4856])\n",
      "Epoch 411, Loss 9.603373\n",
      "    Params: tensor([ 3.8281, -8.5894])\n",
      "    Grad:   tensor([-0.2620,  1.4831])\n",
      "Epoch 412, Loss 9.580709\n",
      "    Params: tensor([ 3.8307, -8.6042])\n",
      "    Grad:   tensor([-0.2615,  1.4805])\n",
      "Epoch 413, Loss 9.558125\n",
      "    Params: tensor([ 3.8333, -8.6190])\n",
      "    Grad:   tensor([-0.2611,  1.4780])\n",
      "Epoch 414, Loss 9.535617\n",
      "    Params: tensor([ 3.8360, -8.6337])\n",
      "    Grad:   tensor([-0.2606,  1.4755])\n",
      "Epoch 415, Loss 9.513184\n",
      "    Params: tensor([ 3.8386, -8.6485])\n",
      "    Grad:   tensor([-0.2602,  1.4730])\n",
      "Epoch 416, Loss 9.490829\n",
      "    Params: tensor([ 3.8412, -8.6632])\n",
      "    Grad:   tensor([-0.2598,  1.4705])\n",
      "Epoch 417, Loss 9.468551\n",
      "    Params: tensor([ 3.8437, -8.6779])\n",
      "    Grad:   tensor([-0.2593,  1.4680])\n",
      "Epoch 418, Loss 9.446347\n",
      "    Params: tensor([ 3.8463, -8.6925])\n",
      "    Grad:   tensor([-0.2589,  1.4655])\n",
      "Epoch 419, Loss 9.424216\n",
      "    Params: tensor([ 3.8489, -8.7071])\n",
      "    Grad:   tensor([-0.2584,  1.4630])\n",
      "Epoch 420, Loss 9.402164\n",
      "    Params: tensor([ 3.8515, -8.7217])\n",
      "    Grad:   tensor([-0.2580,  1.4605])\n",
      "Epoch 421, Loss 9.380184\n",
      "    Params: tensor([ 3.8541, -8.7363])\n",
      "    Grad:   tensor([-0.2576,  1.4581])\n",
      "Epoch 422, Loss 9.358282\n",
      "    Params: tensor([ 3.8566, -8.7509])\n",
      "    Grad:   tensor([-0.2571,  1.4556])\n",
      "Epoch 423, Loss 9.336448\n",
      "    Params: tensor([ 3.8592, -8.7654])\n",
      "    Grad:   tensor([-0.2567,  1.4531])\n",
      "Epoch 424, Loss 9.314695\n",
      "    Params: tensor([ 3.8618, -8.7799])\n",
      "    Grad:   tensor([-0.2563,  1.4506])\n",
      "Epoch 425, Loss 9.293012\n",
      "    Params: tensor([ 3.8643, -8.7944])\n",
      "    Grad:   tensor([-0.2558,  1.4482])\n",
      "Epoch 426, Loss 9.271403\n",
      "    Params: tensor([ 3.8669, -8.8089])\n",
      "    Grad:   tensor([-0.2554,  1.4457])\n",
      "Epoch 427, Loss 9.249871\n",
      "    Params: tensor([ 3.8694, -8.8233])\n",
      "    Grad:   tensor([-0.2550,  1.4433])\n",
      "Epoch 428, Loss 9.228410\n",
      "    Params: tensor([ 3.8720, -8.8377])\n",
      "    Grad:   tensor([-0.2545,  1.4408])\n",
      "Epoch 429, Loss 9.207022\n",
      "    Params: tensor([ 3.8745, -8.8521])\n",
      "    Grad:   tensor([-0.2541,  1.4384])\n",
      "Epoch 430, Loss 9.185704\n",
      "    Params: tensor([ 3.8771, -8.8664])\n",
      "    Grad:   tensor([-0.2537,  1.4359])\n",
      "Epoch 431, Loss 9.164462\n",
      "    Params: tensor([ 3.8796, -8.8808])\n",
      "    Grad:   tensor([-0.2532,  1.4335])\n",
      "Epoch 432, Loss 9.143289\n",
      "    Params: tensor([ 3.8821, -8.8951])\n",
      "    Grad:   tensor([-0.2528,  1.4310])\n",
      "Epoch 433, Loss 9.122189\n",
      "    Params: tensor([ 3.8846, -8.9094])\n",
      "    Grad:   tensor([-0.2524,  1.4286])\n",
      "Epoch 434, Loss 9.101160\n",
      "    Params: tensor([ 3.8872, -8.9236])\n",
      "    Grad:   tensor([-0.2519,  1.4262])\n",
      "Epoch 435, Loss 9.080204\n",
      "    Params: tensor([ 3.8897, -8.9379])\n",
      "    Grad:   tensor([-0.2515,  1.4238])\n",
      "Epoch 436, Loss 9.059318\n",
      "    Params: tensor([ 3.8922, -8.9521])\n",
      "    Grad:   tensor([-0.2511,  1.4213])\n",
      "Epoch 437, Loss 9.038502\n",
      "    Params: tensor([ 3.8947, -8.9663])\n",
      "    Grad:   tensor([-0.2507,  1.4189])\n",
      "Epoch 438, Loss 9.017757\n",
      "    Params: tensor([ 3.8972, -8.9804])\n",
      "    Grad:   tensor([-0.2502,  1.4165])\n",
      "Epoch 439, Loss 8.997084\n",
      "    Params: tensor([ 3.8997, -8.9946])\n",
      "    Grad:   tensor([-0.2498,  1.4141])\n",
      "Epoch 440, Loss 8.976479\n",
      "    Params: tensor([ 3.9022, -9.0087])\n",
      "    Grad:   tensor([-0.2494,  1.4117])\n",
      "Epoch 441, Loss 8.955944\n",
      "    Params: tensor([ 3.9047, -9.0228])\n",
      "    Grad:   tensor([-0.2489,  1.4093])\n",
      "Epoch 442, Loss 8.935480\n",
      "    Params: tensor([ 3.9072, -9.0369])\n",
      "    Grad:   tensor([-0.2485,  1.4069])\n",
      "Epoch 443, Loss 8.915089\n",
      "    Params: tensor([ 3.9096, -9.0509])\n",
      "    Grad:   tensor([-0.2481,  1.4045])\n",
      "Epoch 444, Loss 8.894762\n",
      "    Params: tensor([ 3.9121, -9.0649])\n",
      "    Grad:   tensor([-0.2477,  1.4021])\n",
      "Epoch 445, Loss 8.874508\n",
      "    Params: tensor([ 3.9146, -9.0789])\n",
      "    Grad:   tensor([-0.2473,  1.3998])\n",
      "Epoch 446, Loss 8.854318\n",
      "    Params: tensor([ 3.9171, -9.0929])\n",
      "    Grad:   tensor([-0.2468,  1.3974])\n",
      "Epoch 447, Loss 8.834197\n",
      "    Params: tensor([ 3.9195, -9.1068])\n",
      "    Grad:   tensor([-0.2464,  1.3950])\n",
      "Epoch 448, Loss 8.814149\n",
      "    Params: tensor([ 3.9220, -9.1208])\n",
      "    Grad:   tensor([-0.2460,  1.3926])\n",
      "Epoch 449, Loss 8.794162\n",
      "    Params: tensor([ 3.9244, -9.1347])\n",
      "    Grad:   tensor([-0.2456,  1.3903])\n",
      "Epoch 450, Loss 8.774253\n",
      "    Params: tensor([ 3.9269, -9.1486])\n",
      "    Grad:   tensor([-0.2452,  1.3879])\n",
      "Epoch 451, Loss 8.754405\n",
      "    Params: tensor([ 3.9293, -9.1624])\n",
      "    Grad:   tensor([-0.2448,  1.3856])\n",
      "Epoch 452, Loss 8.734623\n",
      "    Params: tensor([ 3.9318, -9.1762])\n",
      "    Grad:   tensor([-0.2443,  1.3832])\n",
      "Epoch 453, Loss 8.714911\n",
      "    Params: tensor([ 3.9342, -9.1901])\n",
      "    Grad:   tensor([-0.2439,  1.3808])\n",
      "Epoch 454, Loss 8.695266\n",
      "    Params: tensor([ 3.9367, -9.2038])\n",
      "    Grad:   tensor([-0.2435,  1.3785])\n",
      "Epoch 455, Loss 8.675688\n",
      "    Params: tensor([ 3.9391, -9.2176])\n",
      "    Grad:   tensor([-0.2431,  1.3762])\n",
      "Epoch 456, Loss 8.656173\n",
      "    Params: tensor([ 3.9415, -9.2313])\n",
      "    Grad:   tensor([-0.2427,  1.3738])\n",
      "Epoch 457, Loss 8.636729\n",
      "    Params: tensor([ 3.9439, -9.2451])\n",
      "    Grad:   tensor([-0.2423,  1.3715])\n",
      "Epoch 458, Loss 8.617347\n",
      "    Params: tensor([ 3.9464, -9.2587])\n",
      "    Grad:   tensor([-0.2419,  1.3692])\n",
      "Epoch 459, Loss 8.598029\n",
      "    Params: tensor([ 3.9488, -9.2724])\n",
      "    Grad:   tensor([-0.2414,  1.3668])\n",
      "Epoch 460, Loss 8.578781\n",
      "    Params: tensor([ 3.9512, -9.2861])\n",
      "    Grad:   tensor([-0.2410,  1.3645])\n",
      "Epoch 461, Loss 8.559597\n",
      "    Params: tensor([ 3.9536, -9.2997])\n",
      "    Grad:   tensor([-0.2406,  1.3622])\n",
      "Epoch 462, Loss 8.540479\n",
      "    Params: tensor([ 3.9560, -9.3133])\n",
      "    Grad:   tensor([-0.2402,  1.3599])\n",
      "Epoch 463, Loss 8.521426\n",
      "    Params: tensor([ 3.9584, -9.3269])\n",
      "    Grad:   tensor([-0.2398,  1.3576])\n",
      "Epoch 464, Loss 8.502437\n",
      "    Params: tensor([ 3.9608, -9.3404])\n",
      "    Grad:   tensor([-0.2394,  1.3553])\n",
      "Epoch 465, Loss 8.483517\n",
      "    Params: tensor([ 3.9632, -9.3539])\n",
      "    Grad:   tensor([-0.2390,  1.3530])\n",
      "Epoch 466, Loss 8.464652\n",
      "    Params: tensor([ 3.9656, -9.3674])\n",
      "    Grad:   tensor([-0.2386,  1.3507])\n",
      "Epoch 467, Loss 8.445858\n",
      "    Params: tensor([ 3.9679, -9.3809])\n",
      "    Grad:   tensor([-0.2382,  1.3484])\n",
      "Epoch 468, Loss 8.427128\n",
      "    Params: tensor([ 3.9703, -9.3944])\n",
      "    Grad:   tensor([-0.2378,  1.3461])\n",
      "Epoch 469, Loss 8.408454\n",
      "    Params: tensor([ 3.9727, -9.4078])\n",
      "    Grad:   tensor([-0.2374,  1.3438])\n",
      "Epoch 470, Loss 8.389848\n",
      "    Params: tensor([ 3.9751, -9.4212])\n",
      "    Grad:   tensor([-0.2370,  1.3415])\n",
      "Epoch 471, Loss 8.371305\n",
      "    Params: tensor([ 3.9774, -9.4346])\n",
      "    Grad:   tensor([-0.2366,  1.3392])\n",
      "Epoch 472, Loss 8.352828\n",
      "    Params: tensor([ 3.9798, -9.4480])\n",
      "    Grad:   tensor([-0.2362,  1.3370])\n",
      "Epoch 473, Loss 8.334409\n",
      "    Params: tensor([ 3.9822, -9.4614])\n",
      "    Grad:   tensor([-0.2358,  1.3347])\n",
      "Epoch 474, Loss 8.316054\n",
      "    Params: tensor([ 3.9845, -9.4747])\n",
      "    Grad:   tensor([-0.2354,  1.3324])\n",
      "Epoch 475, Loss 8.297764\n",
      "    Params: tensor([ 3.9869, -9.4880])\n",
      "    Grad:   tensor([-0.2350,  1.3301])\n",
      "Epoch 476, Loss 8.279534\n",
      "    Params: tensor([ 3.9892, -9.5013])\n",
      "    Grad:   tensor([-0.2346,  1.3279])\n",
      "Epoch 477, Loss 8.261369\n",
      "    Params: tensor([ 3.9915, -9.5145])\n",
      "    Grad:   tensor([-0.2342,  1.3256])\n",
      "Epoch 478, Loss 8.243259\n",
      "    Params: tensor([ 3.9939, -9.5277])\n",
      "    Grad:   tensor([-0.2338,  1.3234])\n",
      "Epoch 479, Loss 8.225213\n",
      "    Params: tensor([ 3.9962, -9.5410])\n",
      "    Grad:   tensor([-0.2334,  1.3211])\n",
      "Epoch 480, Loss 8.207231\n",
      "    Params: tensor([ 3.9985, -9.5541])\n",
      "    Grad:   tensor([-0.2330,  1.3189])\n",
      "Epoch 481, Loss 8.189310\n",
      "    Params: tensor([ 4.0009, -9.5673])\n",
      "    Grad:   tensor([-0.2326,  1.3166])\n",
      "Epoch 482, Loss 8.171452\n",
      "    Params: tensor([ 4.0032, -9.5805])\n",
      "    Grad:   tensor([-0.2322,  1.3144])\n",
      "Epoch 483, Loss 8.153647\n",
      "    Params: tensor([ 4.0055, -9.5936])\n",
      "    Grad:   tensor([-0.2318,  1.3122])\n",
      "Epoch 484, Loss 8.135906\n",
      "    Params: tensor([ 4.0078, -9.6067])\n",
      "    Grad:   tensor([-0.2314,  1.3100])\n",
      "Epoch 485, Loss 8.118226\n",
      "    Params: tensor([ 4.0101, -9.6198])\n",
      "    Grad:   tensor([-0.2310,  1.3077])\n",
      "Epoch 486, Loss 8.100607\n",
      "    Params: tensor([ 4.0124, -9.6328])\n",
      "    Grad:   tensor([-0.2306,  1.3055])\n",
      "Epoch 487, Loss 8.083045\n",
      "    Params: tensor([ 4.0147, -9.6458])\n",
      "    Grad:   tensor([-0.2302,  1.3033])\n",
      "Epoch 488, Loss 8.065548\n",
      "    Params: tensor([ 4.0170, -9.6589])\n",
      "    Grad:   tensor([-0.2298,  1.3011])\n",
      "Epoch 489, Loss 8.048104\n",
      "    Params: tensor([ 4.0193, -9.6718])\n",
      "    Grad:   tensor([-0.2295,  1.2989])\n",
      "Epoch 490, Loss 8.030724\n",
      "    Params: tensor([ 4.0216, -9.6848])\n",
      "    Grad:   tensor([-0.2291,  1.2967])\n",
      "Epoch 491, Loss 8.013401\n",
      "    Params: tensor([ 4.0239, -9.6978])\n",
      "    Grad:   tensor([-0.2287,  1.2945])\n",
      "Epoch 492, Loss 7.996137\n",
      "    Params: tensor([ 4.0262, -9.7107])\n",
      "    Grad:   tensor([-0.2283,  1.2923])\n",
      "Epoch 493, Loss 7.978930\n",
      "    Params: tensor([ 4.0285, -9.7236])\n",
      "    Grad:   tensor([-0.2279,  1.2901])\n",
      "Epoch 494, Loss 7.961783\n",
      "    Params: tensor([ 4.0308, -9.7365])\n",
      "    Grad:   tensor([-0.2275,  1.2879])\n",
      "Epoch 495, Loss 7.944690\n",
      "    Params: tensor([ 4.0330, -9.7493])\n",
      "    Grad:   tensor([-0.2271,  1.2857])\n",
      "Epoch 496, Loss 7.927663\n",
      "    Params: tensor([ 4.0353, -9.7621])\n",
      "    Grad:   tensor([-0.2267,  1.2835])\n",
      "Epoch 497, Loss 7.910690\n",
      "    Params: tensor([ 4.0376, -9.7750])\n",
      "    Grad:   tensor([-0.2263,  1.2813])\n",
      "Epoch 498, Loss 7.893775\n",
      "    Params: tensor([ 4.0398, -9.7878])\n",
      "    Grad:   tensor([-0.2260,  1.2791])\n",
      "Epoch 499, Loss 7.876915\n",
      "    Params: tensor([ 4.0421, -9.8005])\n",
      "    Grad:   tensor([-0.2256,  1.2770])\n",
      "Epoch 500, Loss 7.860115\n",
      "    Params: tensor([ 4.0443, -9.8133])\n",
      "    Grad:   tensor([-0.2252,  1.2748])\n",
      "Epoch 501, Loss 7.843369\n",
      "    Params: tensor([ 4.0466, -9.8260])\n",
      "    Grad:   tensor([-0.2248,  1.2726])\n",
      "Epoch 502, Loss 7.826683\n",
      "    Params: tensor([ 4.0488, -9.8387])\n",
      "    Grad:   tensor([-0.2244,  1.2705])\n",
      "Epoch 503, Loss 7.810053\n",
      "    Params: tensor([ 4.0511, -9.8514])\n",
      "    Grad:   tensor([-0.2241,  1.2683])\n",
      "Epoch 504, Loss 7.793481\n",
      "    Params: tensor([ 4.0533, -9.8640])\n",
      "    Grad:   tensor([-0.2237,  1.2662])\n",
      "Epoch 505, Loss 7.776962\n",
      "    Params: tensor([ 4.0555, -9.8767])\n",
      "    Grad:   tensor([-0.2233,  1.2640])\n",
      "Epoch 506, Loss 7.760498\n",
      "    Params: tensor([ 4.0578, -9.8893])\n",
      "    Grad:   tensor([-0.2229,  1.2619])\n",
      "Epoch 507, Loss 7.744092\n",
      "    Params: tensor([ 4.0600, -9.9019])\n",
      "    Grad:   tensor([-0.2225,  1.2597])\n",
      "Epoch 508, Loss 7.727745\n",
      "    Params: tensor([ 4.0622, -9.9145])\n",
      "    Grad:   tensor([-0.2222,  1.2576])\n",
      "Epoch 509, Loss 7.711449\n",
      "    Params: tensor([ 4.0644, -9.9270])\n",
      "    Grad:   tensor([-0.2218,  1.2554])\n",
      "Epoch 510, Loss 7.695211\n",
      "    Params: tensor([ 4.0666, -9.9396])\n",
      "    Grad:   tensor([-0.2214,  1.2533])\n",
      "Epoch 511, Loss 7.679024\n",
      "    Params: tensor([ 4.0688, -9.9521])\n",
      "    Grad:   tensor([-0.2210,  1.2512])\n",
      "Epoch 512, Loss 7.662896\n",
      "    Params: tensor([ 4.0710, -9.9646])\n",
      "    Grad:   tensor([-0.2207,  1.2490])\n",
      "Epoch 513, Loss 7.646820\n",
      "    Params: tensor([ 4.0733, -9.9770])\n",
      "    Grad:   tensor([-0.2203,  1.2469])\n",
      "Epoch 514, Loss 7.630803\n",
      "    Params: tensor([ 4.0754, -9.9895])\n",
      "    Grad:   tensor([-0.2199,  1.2448])\n",
      "Epoch 515, Loss 7.614836\n",
      "    Params: tensor([  4.0776, -10.0019])\n",
      "    Grad:   tensor([-0.2195,  1.2427])\n",
      "Epoch 516, Loss 7.598925\n",
      "    Params: tensor([  4.0798, -10.0143])\n",
      "    Grad:   tensor([-0.2192,  1.2406])\n",
      "Epoch 517, Loss 7.583069\n",
      "    Params: tensor([  4.0820, -10.0267])\n",
      "    Grad:   tensor([-0.2188,  1.2385])\n",
      "Epoch 518, Loss 7.567265\n",
      "    Params: tensor([  4.0842, -10.0391])\n",
      "    Grad:   tensor([-0.2184,  1.2364])\n",
      "Epoch 519, Loss 7.551515\n",
      "    Params: tensor([  4.0864, -10.0514])\n",
      "    Grad:   tensor([-0.2180,  1.2343])\n",
      "Epoch 520, Loss 7.535818\n",
      "    Params: tensor([  4.0886, -10.0637])\n",
      "    Grad:   tensor([-0.2177,  1.2322])\n",
      "Epoch 521, Loss 7.520176\n",
      "    Params: tensor([  4.0907, -10.0760])\n",
      "    Grad:   tensor([-0.2173,  1.2301])\n",
      "Epoch 522, Loss 7.504587\n",
      "    Params: tensor([  4.0929, -10.0883])\n",
      "    Grad:   tensor([-0.2169,  1.2280])\n",
      "Epoch 523, Loss 7.489048\n",
      "    Params: tensor([  4.0951, -10.1006])\n",
      "    Grad:   tensor([-0.2165,  1.2259])\n",
      "Epoch 524, Loss 7.473566\n",
      "    Params: tensor([  4.0972, -10.1128])\n",
      "    Grad:   tensor([-0.2162,  1.2238])\n",
      "Epoch 525, Loss 7.458135\n",
      "    Params: tensor([  4.0994, -10.1250])\n",
      "    Grad:   tensor([-0.2158,  1.2217])\n",
      "Epoch 526, Loss 7.442750\n",
      "    Params: tensor([  4.1015, -10.1372])\n",
      "    Grad:   tensor([-0.2155,  1.2197])\n",
      "Epoch 527, Loss 7.427427\n",
      "    Params: tensor([  4.1037, -10.1494])\n",
      "    Grad:   tensor([-0.2151,  1.2176])\n",
      "Epoch 528, Loss 7.412152\n",
      "    Params: tensor([  4.1058, -10.1616])\n",
      "    Grad:   tensor([-0.2147,  1.2155])\n",
      "Epoch 529, Loss 7.396928\n",
      "    Params: tensor([  4.1080, -10.1737])\n",
      "    Grad:   tensor([-0.2144,  1.2135])\n",
      "Epoch 530, Loss 7.381757\n",
      "    Params: tensor([  4.1101, -10.1858])\n",
      "    Grad:   tensor([-0.2140,  1.2114])\n",
      "Epoch 531, Loss 7.366637\n",
      "    Params: tensor([  4.1123, -10.1979])\n",
      "    Grad:   tensor([-0.2136,  1.2093])\n",
      "Epoch 532, Loss 7.351567\n",
      "    Params: tensor([  4.1144, -10.2100])\n",
      "    Grad:   tensor([-0.2133,  1.2073])\n",
      "Epoch 533, Loss 7.336549\n",
      "    Params: tensor([  4.1165, -10.2220])\n",
      "    Grad:   tensor([-0.2129,  1.2052])\n",
      "Epoch 534, Loss 7.321584\n",
      "    Params: tensor([  4.1187, -10.2340])\n",
      "    Grad:   tensor([-0.2125,  1.2032])\n",
      "Epoch 535, Loss 7.306671\n",
      "    Params: tensor([  4.1208, -10.2461])\n",
      "    Grad:   tensor([-0.2122,  1.2012])\n",
      "Epoch 536, Loss 7.291804\n",
      "    Params: tensor([  4.1229, -10.2581])\n",
      "    Grad:   tensor([-0.2118,  1.1991])\n",
      "Epoch 537, Loss 7.276989\n",
      "    Params: tensor([  4.1250, -10.2700])\n",
      "    Grad:   tensor([-0.2115,  1.1971])\n",
      "Epoch 538, Loss 7.262227\n",
      "    Params: tensor([  4.1271, -10.2820])\n",
      "    Grad:   tensor([-0.2111,  1.1950])\n",
      "Epoch 539, Loss 7.247512\n",
      "    Params: tensor([  4.1292, -10.2939])\n",
      "    Grad:   tensor([-0.2108,  1.1930])\n",
      "Epoch 540, Loss 7.232845\n",
      "    Params: tensor([  4.1313, -10.3058])\n",
      "    Grad:   tensor([-0.2104,  1.1910])\n",
      "Epoch 541, Loss 7.218231\n",
      "    Params: tensor([  4.1334, -10.3177])\n",
      "    Grad:   tensor([-0.2100,  1.1890])\n",
      "Epoch 542, Loss 7.203665\n",
      "    Params: tensor([  4.1355, -10.3296])\n",
      "    Grad:   tensor([-0.2097,  1.1869])\n",
      "Epoch 543, Loss 7.189151\n",
      "    Params: tensor([  4.1376, -10.3414])\n",
      "    Grad:   tensor([-0.2093,  1.1849])\n",
      "Epoch 544, Loss 7.174683\n",
      "    Params: tensor([  4.1397, -10.3533])\n",
      "    Grad:   tensor([-0.2090,  1.1829])\n",
      "Epoch 545, Loss 7.160266\n",
      "    Params: tensor([  4.1418, -10.3651])\n",
      "    Grad:   tensor([-0.2086,  1.1809])\n",
      "Epoch 546, Loss 7.145897\n",
      "    Params: tensor([  4.1439, -10.3769])\n",
      "    Grad:   tensor([-0.2083,  1.1789])\n",
      "Epoch 547, Loss 7.131581\n",
      "    Params: tensor([  4.1460, -10.3886])\n",
      "    Grad:   tensor([-0.2079,  1.1769])\n",
      "Epoch 548, Loss 7.117305\n",
      "    Params: tensor([  4.1480, -10.4004])\n",
      "    Grad:   tensor([-0.2075,  1.1749])\n",
      "Epoch 549, Loss 7.103083\n",
      "    Params: tensor([  4.1501, -10.4121])\n",
      "    Grad:   tensor([-0.2072,  1.1729])\n",
      "Epoch 550, Loss 7.088911\n",
      "    Params: tensor([  4.1522, -10.4238])\n",
      "    Grad:   tensor([-0.2068,  1.1709])\n",
      "Epoch 551, Loss 7.074785\n",
      "    Params: tensor([  4.1542, -10.4355])\n",
      "    Grad:   tensor([-0.2065,  1.1689])\n",
      "Epoch 552, Loss 7.060707\n",
      "    Params: tensor([  4.1563, -10.4472])\n",
      "    Grad:   tensor([-0.2062,  1.1669])\n",
      "Epoch 553, Loss 7.046676\n",
      "    Params: tensor([  4.1584, -10.4588])\n",
      "    Grad:   tensor([-0.2058,  1.1649])\n",
      "Epoch 554, Loss 7.032695\n",
      "    Params: tensor([  4.1604, -10.4704])\n",
      "    Grad:   tensor([-0.2054,  1.1630])\n",
      "Epoch 555, Loss 7.018755\n",
      "    Params: tensor([  4.1625, -10.4821])\n",
      "    Grad:   tensor([-0.2051,  1.1610])\n",
      "Epoch 556, Loss 7.004870\n",
      "    Params: tensor([  4.1645, -10.4936])\n",
      "    Grad:   tensor([-0.2047,  1.1590])\n",
      "Epoch 557, Loss 6.991028\n",
      "    Params: tensor([  4.1666, -10.5052])\n",
      "    Grad:   tensor([-0.2044,  1.1571])\n",
      "Epoch 558, Loss 6.977232\n",
      "    Params: tensor([  4.1686, -10.5168])\n",
      "    Grad:   tensor([-0.2041,  1.1551])\n",
      "Epoch 559, Loss 6.963488\n",
      "    Params: tensor([  4.1706, -10.5283])\n",
      "    Grad:   tensor([-0.2037,  1.1531])\n",
      "Epoch 560, Loss 6.949787\n",
      "    Params: tensor([  4.1727, -10.5398])\n",
      "    Grad:   tensor([-0.2034,  1.1512])\n",
      "Epoch 561, Loss 6.936135\n",
      "    Params: tensor([  4.1747, -10.5513])\n",
      "    Grad:   tensor([-0.2030,  1.1492])\n",
      "Epoch 562, Loss 6.922528\n",
      "    Params: tensor([  4.1767, -10.5628])\n",
      "    Grad:   tensor([-0.2027,  1.1473])\n",
      "Epoch 563, Loss 6.908967\n",
      "    Params: tensor([  4.1787, -10.5742])\n",
      "    Grad:   tensor([-0.2023,  1.1453])\n",
      "Epoch 564, Loss 6.895452\n",
      "    Params: tensor([  4.1808, -10.5857])\n",
      "    Grad:   tensor([-0.2020,  1.1434])\n",
      "Epoch 565, Loss 6.881980\n",
      "    Params: tensor([  4.1828, -10.5971])\n",
      "    Grad:   tensor([-0.2016,  1.1414])\n",
      "Epoch 566, Loss 6.868559\n",
      "    Params: tensor([  4.1848, -10.6085])\n",
      "    Grad:   tensor([-0.2013,  1.1395])\n",
      "Epoch 567, Loss 6.855180\n",
      "    Params: tensor([  4.1868, -10.6198])\n",
      "    Grad:   tensor([-0.2010,  1.1375])\n",
      "Epoch 568, Loss 6.841848\n",
      "    Params: tensor([  4.1888, -10.6312])\n",
      "    Grad:   tensor([-0.2006,  1.1356])\n",
      "Epoch 569, Loss 6.828561\n",
      "    Params: tensor([  4.1908, -10.6425])\n",
      "    Grad:   tensor([-0.2003,  1.1337])\n",
      "Epoch 570, Loss 6.815319\n",
      "    Params: tensor([  4.1928, -10.6539])\n",
      "    Grad:   tensor([-0.1999,  1.1318])\n",
      "Epoch 571, Loss 6.802118\n",
      "    Params: tensor([  4.1948, -10.6652])\n",
      "    Grad:   tensor([-0.1996,  1.1298])\n",
      "Epoch 572, Loss 6.788968\n",
      "    Params: tensor([  4.1968, -10.6764])\n",
      "    Grad:   tensor([-0.1993,  1.1279])\n",
      "Epoch 573, Loss 6.775864\n",
      "    Params: tensor([  4.1988, -10.6877])\n",
      "    Grad:   tensor([-0.1989,  1.1260])\n",
      "Epoch 574, Loss 6.762797\n",
      "    Params: tensor([  4.2008, -10.6989])\n",
      "    Grad:   tensor([-0.1986,  1.1241])\n",
      "Epoch 575, Loss 6.749779\n",
      "    Params: tensor([  4.2028, -10.7102])\n",
      "    Grad:   tensor([-0.1982,  1.1222])\n",
      "Epoch 576, Loss 6.736804\n",
      "    Params: tensor([  4.2047, -10.7214])\n",
      "    Grad:   tensor([-0.1979,  1.1203])\n",
      "Epoch 577, Loss 6.723876\n",
      "    Params: tensor([  4.2067, -10.7325])\n",
      "    Grad:   tensor([-0.1976,  1.1184])\n",
      "Epoch 578, Loss 6.710987\n",
      "    Params: tensor([  4.2087, -10.7437])\n",
      "    Grad:   tensor([-0.1972,  1.1165])\n",
      "Epoch 579, Loss 6.698142\n",
      "    Params: tensor([  4.2107, -10.7549])\n",
      "    Grad:   tensor([-0.1969,  1.1146])\n",
      "Epoch 580, Loss 6.685345\n",
      "    Params: tensor([  4.2126, -10.7660])\n",
      "    Grad:   tensor([-0.1966,  1.1127])\n",
      "Epoch 581, Loss 6.672589\n",
      "    Params: tensor([  4.2146, -10.7771])\n",
      "    Grad:   tensor([-0.1962,  1.1108])\n",
      "Epoch 582, Loss 6.659873\n",
      "    Params: tensor([  4.2165, -10.7882])\n",
      "    Grad:   tensor([-0.1959,  1.1089])\n",
      "Epoch 583, Loss 6.647207\n",
      "    Params: tensor([  4.2185, -10.7992])\n",
      "    Grad:   tensor([-0.1956,  1.1070])\n",
      "Epoch 584, Loss 6.634578\n",
      "    Params: tensor([  4.2204, -10.8103])\n",
      "    Grad:   tensor([-0.1952,  1.1051])\n",
      "Epoch 585, Loss 6.621994\n",
      "    Params: tensor([  4.2224, -10.8213])\n",
      "    Grad:   tensor([-0.1949,  1.1033])\n",
      "Epoch 586, Loss 6.609454\n",
      "    Params: tensor([  4.2243, -10.8323])\n",
      "    Grad:   tensor([-0.1946,  1.1014])\n",
      "Epoch 587, Loss 6.596953\n",
      "    Params: tensor([  4.2263, -10.8433])\n",
      "    Grad:   tensor([-0.1942,  1.0995])\n",
      "Epoch 588, Loss 6.584499\n",
      "    Params: tensor([  4.2282, -10.8543])\n",
      "    Grad:   tensor([-0.1939,  1.0976])\n",
      "Epoch 589, Loss 6.572087\n",
      "    Params: tensor([  4.2302, -10.8653])\n",
      "    Grad:   tensor([-0.1936,  1.0958])\n",
      "Epoch 590, Loss 6.559712\n",
      "    Params: tensor([  4.2321, -10.8762])\n",
      "    Grad:   tensor([-0.1932,  1.0939])\n",
      "Epoch 591, Loss 6.547384\n",
      "    Params: tensor([  4.2340, -10.8871])\n",
      "    Grad:   tensor([-0.1929,  1.0921])\n",
      "Epoch 592, Loss 6.535097\n",
      "    Params: tensor([  4.2359, -10.8980])\n",
      "    Grad:   tensor([-0.1926,  1.0902])\n",
      "Epoch 593, Loss 6.522851\n",
      "    Params: tensor([  4.2379, -10.9089])\n",
      "    Grad:   tensor([-0.1923,  1.0884])\n",
      "Epoch 594, Loss 6.510646\n",
      "    Params: tensor([  4.2398, -10.9198])\n",
      "    Grad:   tensor([-0.1919,  1.0865])\n",
      "Epoch 595, Loss 6.498482\n",
      "    Params: tensor([  4.2417, -10.9306])\n",
      "    Grad:   tensor([-0.1916,  1.0847])\n",
      "Epoch 596, Loss 6.486361\n",
      "    Params: tensor([  4.2436, -10.9415])\n",
      "    Grad:   tensor([-0.1913,  1.0828])\n",
      "Epoch 597, Loss 6.474282\n",
      "    Params: tensor([  4.2455, -10.9523])\n",
      "    Grad:   tensor([-0.1910,  1.0810])\n",
      "Epoch 598, Loss 6.462241\n",
      "    Params: tensor([  4.2474, -10.9631])\n",
      "    Grad:   tensor([-0.1906,  1.0791])\n",
      "Epoch 599, Loss 6.450243\n",
      "    Params: tensor([  4.2493, -10.9738])\n",
      "    Grad:   tensor([-0.1903,  1.0773])\n",
      "Epoch 600, Loss 6.438284\n",
      "    Params: tensor([  4.2512, -10.9846])\n",
      "    Grad:   tensor([-0.1900,  1.0755])\n",
      "Epoch 601, Loss 6.426368\n",
      "    Params: tensor([  4.2531, -10.9953])\n",
      "    Grad:   tensor([-0.1897,  1.0737])\n",
      "Epoch 602, Loss 6.414490\n",
      "    Params: tensor([  4.2550, -11.0060])\n",
      "    Grad:   tensor([-0.1893,  1.0718])\n",
      "Epoch 603, Loss 6.402653\n",
      "    Params: tensor([  4.2569, -11.0167])\n",
      "    Grad:   tensor([-0.1890,  1.0700])\n",
      "Epoch 604, Loss 6.390859\n",
      "    Params: tensor([  4.2588, -11.0274])\n",
      "    Grad:   tensor([-0.1887,  1.0682])\n",
      "Epoch 605, Loss 6.379103\n",
      "    Params: tensor([  4.2607, -11.0381])\n",
      "    Grad:   tensor([-0.1884,  1.0664])\n",
      "Epoch 606, Loss 6.367385\n",
      "    Params: tensor([  4.2626, -11.0487])\n",
      "    Grad:   tensor([-0.1880,  1.0646])\n",
      "Epoch 607, Loss 6.355706\n",
      "    Params: tensor([  4.2644, -11.0594])\n",
      "    Grad:   tensor([-0.1877,  1.0628])\n",
      "Epoch 608, Loss 6.344070\n",
      "    Params: tensor([  4.2663, -11.0700])\n",
      "    Grad:   tensor([-0.1874,  1.0609])\n",
      "Epoch 609, Loss 6.332472\n",
      "    Params: tensor([  4.2682, -11.0806])\n",
      "    Grad:   tensor([-0.1871,  1.0591])\n",
      "Epoch 610, Loss 6.320912\n",
      "    Params: tensor([  4.2701, -11.0911])\n",
      "    Grad:   tensor([-0.1868,  1.0573])\n",
      "Epoch 611, Loss 6.309395\n",
      "    Params: tensor([  4.2719, -11.1017])\n",
      "    Grad:   tensor([-0.1865,  1.0555])\n",
      "Epoch 612, Loss 6.297915\n",
      "    Params: tensor([  4.2738, -11.1122])\n",
      "    Grad:   tensor([-0.1861,  1.0538])\n",
      "Epoch 613, Loss 6.286473\n",
      "    Params: tensor([  4.2756, -11.1227])\n",
      "    Grad:   tensor([-0.1858,  1.0520])\n",
      "Epoch 614, Loss 6.275074\n",
      "    Params: tensor([  4.2775, -11.1333])\n",
      "    Grad:   tensor([-0.1855,  1.0502])\n",
      "Epoch 615, Loss 6.263708\n",
      "    Params: tensor([  4.2794, -11.1437])\n",
      "    Grad:   tensor([-0.1852,  1.0484])\n",
      "Epoch 616, Loss 6.252382\n",
      "    Params: tensor([  4.2812, -11.1542])\n",
      "    Grad:   tensor([-0.1849,  1.0466])\n",
      "Epoch 617, Loss 6.241098\n",
      "    Params: tensor([  4.2830, -11.1646])\n",
      "    Grad:   tensor([-0.1846,  1.0448])\n",
      "Epoch 618, Loss 6.229849\n",
      "    Params: tensor([  4.2849, -11.1751])\n",
      "    Grad:   tensor([-0.1843,  1.0431])\n",
      "Epoch 619, Loss 6.218639\n",
      "    Params: tensor([  4.2867, -11.1855])\n",
      "    Grad:   tensor([-0.1840,  1.0413])\n",
      "Epoch 620, Loss 6.207470\n",
      "    Params: tensor([  4.2886, -11.1959])\n",
      "    Grad:   tensor([-0.1836,  1.0395])\n",
      "Epoch 621, Loss 6.196334\n",
      "    Params: tensor([  4.2904, -11.2063])\n",
      "    Grad:   tensor([-0.1833,  1.0378])\n",
      "Epoch 622, Loss 6.185240\n",
      "    Params: tensor([  4.2922, -11.2166])\n",
      "    Grad:   tensor([-0.1830,  1.0360])\n",
      "Epoch 623, Loss 6.174181\n",
      "    Params: tensor([  4.2941, -11.2270])\n",
      "    Grad:   tensor([-0.1827,  1.0342])\n",
      "Epoch 624, Loss 6.163159\n",
      "    Params: tensor([  4.2959, -11.2373])\n",
      "    Grad:   tensor([-0.1824,  1.0325])\n",
      "Epoch 625, Loss 6.152177\n",
      "    Params: tensor([  4.2977, -11.2476])\n",
      "    Grad:   tensor([-0.1821,  1.0307])\n",
      "Epoch 626, Loss 6.141230\n",
      "    Params: tensor([  4.2995, -11.2579])\n",
      "    Grad:   tensor([-0.1818,  1.0290])\n",
      "Epoch 627, Loss 6.130322\n",
      "    Params: tensor([  4.3013, -11.2682])\n",
      "    Grad:   tensor([-0.1815,  1.0272])\n",
      "Epoch 628, Loss 6.119448\n",
      "    Params: tensor([  4.3031, -11.2784])\n",
      "    Grad:   tensor([-0.1811,  1.0255])\n",
      "Epoch 629, Loss 6.108614\n",
      "    Params: tensor([  4.3050, -11.2887])\n",
      "    Grad:   tensor([-0.1808,  1.0237])\n",
      "Epoch 630, Loss 6.097815\n",
      "    Params: tensor([  4.3068, -11.2989])\n",
      "    Grad:   tensor([-0.1805,  1.0220])\n",
      "Epoch 631, Loss 6.087054\n",
      "    Params: tensor([  4.3086, -11.3091])\n",
      "    Grad:   tensor([-0.1802,  1.0203])\n",
      "Epoch 632, Loss 6.076329\n",
      "    Params: tensor([  4.3104, -11.3193])\n",
      "    Grad:   tensor([-0.1799,  1.0185])\n",
      "Epoch 633, Loss 6.065644\n",
      "    Params: tensor([  4.3122, -11.3294])\n",
      "    Grad:   tensor([-0.1796,  1.0168])\n",
      "Epoch 634, Loss 6.054988\n",
      "    Params: tensor([  4.3139, -11.3396])\n",
      "    Grad:   tensor([-0.1793,  1.0151])\n",
      "Epoch 635, Loss 6.044372\n",
      "    Params: tensor([  4.3157, -11.3497])\n",
      "    Grad:   tensor([-0.1790,  1.0133])\n",
      "Epoch 636, Loss 6.033794\n",
      "    Params: tensor([  4.3175, -11.3598])\n",
      "    Grad:   tensor([-0.1787,  1.0116])\n",
      "Epoch 637, Loss 6.023247\n",
      "    Params: tensor([  4.3193, -11.3699])\n",
      "    Grad:   tensor([-0.1784,  1.0099])\n",
      "Epoch 638, Loss 6.012738\n",
      "    Params: tensor([  4.3211, -11.3800])\n",
      "    Grad:   tensor([-0.1781,  1.0082])\n",
      "Epoch 639, Loss 6.002264\n",
      "    Params: tensor([  4.3229, -11.3901])\n",
      "    Grad:   tensor([-0.1778,  1.0065])\n",
      "Epoch 640, Loss 5.991828\n",
      "    Params: tensor([  4.3246, -11.4001])\n",
      "    Grad:   tensor([-0.1775,  1.0048])\n",
      "Epoch 641, Loss 5.981425\n",
      "    Params: tensor([  4.3264, -11.4102])\n",
      "    Grad:   tensor([-0.1772,  1.0031])\n",
      "Epoch 642, Loss 5.971058\n",
      "    Params: tensor([  4.3282, -11.4202])\n",
      "    Grad:   tensor([-0.1769,  1.0014])\n",
      "Epoch 643, Loss 5.960727\n",
      "    Params: tensor([  4.3300, -11.4302])\n",
      "    Grad:   tensor([-0.1766,  0.9997])\n",
      "Epoch 644, Loss 5.950432\n",
      "    Params: tensor([  4.3317, -11.4401])\n",
      "    Grad:   tensor([-0.1763,  0.9980])\n",
      "Epoch 645, Loss 5.940171\n",
      "    Params: tensor([  4.3335, -11.4501])\n",
      "    Grad:   tensor([-0.1760,  0.9963])\n",
      "Epoch 646, Loss 5.929944\n",
      "    Params: tensor([  4.3352, -11.4601])\n",
      "    Grad:   tensor([-0.1757,  0.9946])\n",
      "Epoch 647, Loss 5.919752\n",
      "    Params: tensor([  4.3370, -11.4700])\n",
      "    Grad:   tensor([-0.1754,  0.9929])\n",
      "Epoch 648, Loss 5.909596\n",
      "    Params: tensor([  4.3387, -11.4799])\n",
      "    Grad:   tensor([-0.1751,  0.9912])\n",
      "Epoch 649, Loss 5.899472\n",
      "    Params: tensor([  4.3405, -11.4898])\n",
      "    Grad:   tensor([-0.1748,  0.9895])\n",
      "Epoch 650, Loss 5.889383\n",
      "    Params: tensor([  4.3422, -11.4997])\n",
      "    Grad:   tensor([-0.1745,  0.9878])\n",
      "Epoch 651, Loss 5.879326\n",
      "    Params: tensor([  4.3440, -11.5095])\n",
      "    Grad:   tensor([-0.1742,  0.9862])\n",
      "Epoch 652, Loss 5.869310\n",
      "    Params: tensor([  4.3457, -11.5194])\n",
      "    Grad:   tensor([-0.1739,  0.9845])\n",
      "Epoch 653, Loss 5.859322\n",
      "    Params: tensor([  4.3474, -11.5292])\n",
      "    Grad:   tensor([-0.1736,  0.9828])\n",
      "Epoch 654, Loss 5.849374\n",
      "    Params: tensor([  4.3492, -11.5390])\n",
      "    Grad:   tensor([-0.1733,  0.9811])\n",
      "Epoch 655, Loss 5.839453\n",
      "    Params: tensor([  4.3509, -11.5488])\n",
      "    Grad:   tensor([-0.1730,  0.9795])\n",
      "Epoch 656, Loss 5.829570\n",
      "    Params: tensor([  4.3526, -11.5586])\n",
      "    Grad:   tensor([-0.1727,  0.9778])\n",
      "Epoch 657, Loss 5.819718\n",
      "    Params: tensor([  4.3544, -11.5683])\n",
      "    Grad:   tensor([-0.1724,  0.9761])\n",
      "Epoch 658, Loss 5.809901\n",
      "    Params: tensor([  4.3561, -11.5781])\n",
      "    Grad:   tensor([-0.1722,  0.9745])\n",
      "Epoch 659, Loss 5.800116\n",
      "    Params: tensor([  4.3578, -11.5878])\n",
      "    Grad:   tensor([-0.1719,  0.9728])\n",
      "Epoch 660, Loss 5.790367\n",
      "    Params: tensor([  4.3595, -11.5975])\n",
      "    Grad:   tensor([-0.1716,  0.9712])\n",
      "Epoch 661, Loss 5.780646\n",
      "    Params: tensor([  4.3612, -11.6072])\n",
      "    Grad:   tensor([-0.1713,  0.9695])\n",
      "Epoch 662, Loss 5.770962\n",
      "    Params: tensor([  4.3629, -11.6169])\n",
      "    Grad:   tensor([-0.1710,  0.9679])\n",
      "Epoch 663, Loss 5.761312\n",
      "    Params: tensor([  4.3646, -11.6266])\n",
      "    Grad:   tensor([-0.1707,  0.9662])\n",
      "Epoch 664, Loss 5.751694\n",
      "    Params: tensor([  4.3664, -11.6362])\n",
      "    Grad:   tensor([-0.1704,  0.9646])\n",
      "Epoch 665, Loss 5.742105\n",
      "    Params: tensor([  4.3681, -11.6458])\n",
      "    Grad:   tensor([-0.1701,  0.9630])\n",
      "Epoch 666, Loss 5.732550\n",
      "    Params: tensor([  4.3697, -11.6555])\n",
      "    Grad:   tensor([-0.1698,  0.9613])\n",
      "Epoch 667, Loss 5.723031\n",
      "    Params: tensor([  4.3714, -11.6651])\n",
      "    Grad:   tensor([-0.1695,  0.9597])\n",
      "Epoch 668, Loss 5.713540\n",
      "    Params: tensor([  4.3731, -11.6746])\n",
      "    Grad:   tensor([-0.1692,  0.9581])\n",
      "Epoch 669, Loss 5.704083\n",
      "    Params: tensor([  4.3748, -11.6842])\n",
      "    Grad:   tensor([-0.1690,  0.9564])\n",
      "Epoch 670, Loss 5.694659\n",
      "    Params: tensor([  4.3765, -11.6937])\n",
      "    Grad:   tensor([-0.1687,  0.9548])\n",
      "Epoch 671, Loss 5.685266\n",
      "    Params: tensor([  4.3782, -11.7033])\n",
      "    Grad:   tensor([-0.1684,  0.9532])\n",
      "Epoch 672, Loss 5.675904\n",
      "    Params: tensor([  4.3799, -11.7128])\n",
      "    Grad:   tensor([-0.1681,  0.9516])\n",
      "Epoch 673, Loss 5.666573\n",
      "    Params: tensor([  4.3816, -11.7223])\n",
      "    Grad:   tensor([-0.1678,  0.9499])\n",
      "Epoch 674, Loss 5.657277\n",
      "    Params: tensor([  4.3832, -11.7318])\n",
      "    Grad:   tensor([-0.1675,  0.9483])\n",
      "Epoch 675, Loss 5.648010\n",
      "    Params: tensor([  4.3849, -11.7412])\n",
      "    Grad:   tensor([-0.1673,  0.9467])\n",
      "Epoch 676, Loss 5.638776\n",
      "    Params: tensor([  4.3866, -11.7507])\n",
      "    Grad:   tensor([-0.1670,  0.9451])\n",
      "Epoch 677, Loss 5.629574\n",
      "    Params: tensor([  4.3882, -11.7601])\n",
      "    Grad:   tensor([-0.1667,  0.9435])\n",
      "Epoch 678, Loss 5.620402\n",
      "    Params: tensor([  4.3899, -11.7696])\n",
      "    Grad:   tensor([-0.1664,  0.9419])\n",
      "Epoch 679, Loss 5.611260\n",
      "    Params: tensor([  4.3916, -11.7790])\n",
      "    Grad:   tensor([-0.1661,  0.9403])\n",
      "Epoch 680, Loss 5.602149\n",
      "    Params: tensor([  4.3932, -11.7883])\n",
      "    Grad:   tensor([-0.1658,  0.9387])\n",
      "Epoch 681, Loss 5.593071\n",
      "    Params: tensor([  4.3949, -11.7977])\n",
      "    Grad:   tensor([-0.1656,  0.9371])\n",
      "Epoch 682, Loss 5.584022\n",
      "    Params: tensor([  4.3965, -11.8071])\n",
      "    Grad:   tensor([-0.1653,  0.9355])\n",
      "Epoch 683, Loss 5.575005\n",
      "    Params: tensor([  4.3982, -11.8164])\n",
      "    Grad:   tensor([-0.1650,  0.9339])\n",
      "Epoch 684, Loss 5.566019\n",
      "    Params: tensor([  4.3998, -11.8257])\n",
      "    Grad:   tensor([-0.1647,  0.9323])\n",
      "Epoch 685, Loss 5.557063\n",
      "    Params: tensor([  4.4015, -11.8350])\n",
      "    Grad:   tensor([-0.1644,  0.9308])\n",
      "Epoch 686, Loss 5.548136\n",
      "    Params: tensor([  4.4031, -11.8443])\n",
      "    Grad:   tensor([-0.1641,  0.9292])\n",
      "Epoch 687, Loss 5.539241\n",
      "    Params: tensor([  4.4048, -11.8536])\n",
      "    Grad:   tensor([-0.1639,  0.9276])\n",
      "Epoch 688, Loss 5.530376\n",
      "    Params: tensor([  4.4064, -11.8629])\n",
      "    Grad:   tensor([-0.1636,  0.9260])\n",
      "Epoch 689, Loss 5.521540\n",
      "    Params: tensor([  4.4080, -11.8721])\n",
      "    Grad:   tensor([-0.1633,  0.9245])\n",
      "Epoch 690, Loss 5.512734\n",
      "    Params: tensor([  4.4097, -11.8813])\n",
      "    Grad:   tensor([-0.1630,  0.9229])\n",
      "Epoch 691, Loss 5.503958\n",
      "    Params: tensor([  4.4113, -11.8906])\n",
      "    Grad:   tensor([-0.1628,  0.9213])\n",
      "Epoch 692, Loss 5.495212\n",
      "    Params: tensor([  4.4129, -11.8998])\n",
      "    Grad:   tensor([-0.1625,  0.9197])\n",
      "Epoch 693, Loss 5.486496\n",
      "    Params: tensor([  4.4145, -11.9089])\n",
      "    Grad:   tensor([-0.1622,  0.9182])\n",
      "Epoch 694, Loss 5.477808\n",
      "    Params: tensor([  4.4161, -11.9181])\n",
      "    Grad:   tensor([-0.1619,  0.9166])\n",
      "Epoch 695, Loss 5.469152\n",
      "    Params: tensor([  4.4178, -11.9272])\n",
      "    Grad:   tensor([-0.1617,  0.9151])\n",
      "Epoch 696, Loss 5.460525\n",
      "    Params: tensor([  4.4194, -11.9364])\n",
      "    Grad:   tensor([-0.1614,  0.9135])\n",
      "Epoch 697, Loss 5.451928\n",
      "    Params: tensor([  4.4210, -11.9455])\n",
      "    Grad:   tensor([-0.1611,  0.9120])\n",
      "Epoch 698, Loss 5.443358\n",
      "    Params: tensor([  4.4226, -11.9546])\n",
      "    Grad:   tensor([-0.1608,  0.9104])\n",
      "Epoch 699, Loss 5.434819\n",
      "    Params: tensor([  4.4242, -11.9637])\n",
      "    Grad:   tensor([-0.1605,  0.9089])\n",
      "Epoch 700, Loss 5.426309\n",
      "    Params: tensor([  4.4258, -11.9728])\n",
      "    Grad:   tensor([-0.1603,  0.9073])\n",
      "Epoch 701, Loss 5.417827\n",
      "    Params: tensor([  4.4274, -11.9818])\n",
      "    Grad:   tensor([-0.1600,  0.9058])\n",
      "Epoch 702, Loss 5.409372\n",
      "    Params: tensor([  4.4290, -11.9909])\n",
      "    Grad:   tensor([-0.1597,  0.9042])\n",
      "Epoch 703, Loss 5.400949\n",
      "    Params: tensor([  4.4306, -11.9999])\n",
      "    Grad:   tensor([-0.1595,  0.9027])\n",
      "Epoch 704, Loss 5.392550\n",
      "    Params: tensor([  4.4322, -12.0089])\n",
      "    Grad:   tensor([-0.1592,  0.9012])\n",
      "Epoch 705, Loss 5.384184\n",
      "    Params: tensor([  4.4338, -12.0179])\n",
      "    Grad:   tensor([-0.1589,  0.8996])\n",
      "Epoch 706, Loss 5.375846\n",
      "    Params: tensor([  4.4354, -12.0269])\n",
      "    Grad:   tensor([-0.1586,  0.8981])\n",
      "Epoch 707, Loss 5.367537\n",
      "    Params: tensor([  4.4369, -12.0359])\n",
      "    Grad:   tensor([-0.1584,  0.8966])\n",
      "Epoch 708, Loss 5.359253\n",
      "    Params: tensor([  4.4385, -12.0448])\n",
      "    Grad:   tensor([-0.1581,  0.8951])\n",
      "Epoch 709, Loss 5.350999\n",
      "    Params: tensor([  4.4401, -12.0537])\n",
      "    Grad:   tensor([-0.1578,  0.8935])\n",
      "Epoch 710, Loss 5.342772\n",
      "    Params: tensor([  4.4417, -12.0627])\n",
      "    Grad:   tensor([-0.1576,  0.8920])\n",
      "Epoch 711, Loss 5.334575\n",
      "    Params: tensor([  4.4433, -12.0716])\n",
      "    Grad:   tensor([-0.1573,  0.8905])\n",
      "Epoch 712, Loss 5.326402\n",
      "    Params: tensor([  4.4448, -12.0805])\n",
      "    Grad:   tensor([-0.1570,  0.8890])\n",
      "Epoch 713, Loss 5.318260\n",
      "    Params: tensor([  4.4464, -12.0893])\n",
      "    Grad:   tensor([-0.1568,  0.8875])\n",
      "Epoch 714, Loss 5.310144\n",
      "    Params: tensor([  4.4480, -12.0982])\n",
      "    Grad:   tensor([-0.1565,  0.8860])\n",
      "Epoch 715, Loss 5.302055\n",
      "    Params: tensor([  4.4495, -12.1070])\n",
      "    Grad:   tensor([-0.1562,  0.8845])\n",
      "Epoch 716, Loss 5.293994\n",
      "    Params: tensor([  4.4511, -12.1159])\n",
      "    Grad:   tensor([-0.1560,  0.8830])\n",
      "Epoch 717, Loss 5.285964\n",
      "    Params: tensor([  4.4526, -12.1247])\n",
      "    Grad:   tensor([-0.1557,  0.8815])\n",
      "Epoch 718, Loss 5.277958\n",
      "    Params: tensor([  4.4542, -12.1335])\n",
      "    Grad:   tensor([-0.1555,  0.8800])\n",
      "Epoch 719, Loss 5.269979\n",
      "    Params: tensor([  4.4557, -12.1423])\n",
      "    Grad:   tensor([-0.1552,  0.8785])\n",
      "Epoch 720, Loss 5.262027\n",
      "    Params: tensor([  4.4573, -12.1510])\n",
      "    Grad:   tensor([-0.1549,  0.8770])\n",
      "Epoch 721, Loss 5.254103\n",
      "    Params: tensor([  4.4588, -12.1598])\n",
      "    Grad:   tensor([-0.1547,  0.8755])\n",
      "Epoch 722, Loss 5.246205\n",
      "    Params: tensor([  4.4604, -12.1685])\n",
      "    Grad:   tensor([-0.1544,  0.8740])\n",
      "Epoch 723, Loss 5.238335\n",
      "    Params: tensor([  4.4619, -12.1773])\n",
      "    Grad:   tensor([-0.1541,  0.8725])\n",
      "Epoch 724, Loss 5.230492\n",
      "    Params: tensor([  4.4635, -12.1860])\n",
      "    Grad:   tensor([-0.1539,  0.8710])\n",
      "Epoch 725, Loss 5.222674\n",
      "    Params: tensor([  4.4650, -12.1947])\n",
      "    Grad:   tensor([-0.1536,  0.8696])\n",
      "Epoch 726, Loss 5.214881\n",
      "    Params: tensor([  4.4665, -12.2033])\n",
      "    Grad:   tensor([-0.1533,  0.8681])\n",
      "Epoch 727, Loss 5.207120\n",
      "    Params: tensor([  4.4681, -12.2120])\n",
      "    Grad:   tensor([-0.1531,  0.8666])\n",
      "Epoch 728, Loss 5.199381\n",
      "    Params: tensor([  4.4696, -12.2207])\n",
      "    Grad:   tensor([-0.1528,  0.8651])\n",
      "Epoch 729, Loss 5.191670\n",
      "    Params: tensor([  4.4711, -12.2293])\n",
      "    Grad:   tensor([-0.1526,  0.8637])\n",
      "Epoch 730, Loss 5.183985\n",
      "    Params: tensor([  4.4726, -12.2379])\n",
      "    Grad:   tensor([-0.1523,  0.8622])\n",
      "Epoch 731, Loss 5.176324\n",
      "    Params: tensor([  4.4742, -12.2465])\n",
      "    Grad:   tensor([-0.1520,  0.8607])\n",
      "Epoch 732, Loss 5.168688\n",
      "    Params: tensor([  4.4757, -12.2551])\n",
      "    Grad:   tensor([-0.1518,  0.8593])\n",
      "Epoch 733, Loss 5.161084\n",
      "    Params: tensor([  4.4772, -12.2637])\n",
      "    Grad:   tensor([-0.1515,  0.8578])\n",
      "Epoch 734, Loss 5.153500\n",
      "    Params: tensor([  4.4787, -12.2723])\n",
      "    Grad:   tensor([-0.1513,  0.8564])\n",
      "Epoch 735, Loss 5.145944\n",
      "    Params: tensor([  4.4802, -12.2808])\n",
      "    Grad:   tensor([-0.1510,  0.8549])\n",
      "Epoch 736, Loss 5.138413\n",
      "    Params: tensor([  4.4817, -12.2893])\n",
      "    Grad:   tensor([-0.1508,  0.8535])\n",
      "Epoch 737, Loss 5.130910\n",
      "    Params: tensor([  4.4832, -12.2979])\n",
      "    Grad:   tensor([-0.1505,  0.8520])\n",
      "Epoch 738, Loss 5.123428\n",
      "    Params: tensor([  4.4847, -12.3064])\n",
      "    Grad:   tensor([-0.1502,  0.8506])\n",
      "Epoch 739, Loss 5.115978\n",
      "    Params: tensor([  4.4862, -12.3149])\n",
      "    Grad:   tensor([-0.1500,  0.8491])\n",
      "Epoch 740, Loss 5.108547\n",
      "    Params: tensor([  4.4877, -12.3233])\n",
      "    Grad:   tensor([-0.1497,  0.8477])\n",
      "Epoch 741, Loss 5.101143\n",
      "    Params: tensor([  4.4892, -12.3318])\n",
      "    Grad:   tensor([-0.1495,  0.8462])\n",
      "Epoch 742, Loss 5.093765\n",
      "    Params: tensor([  4.4907, -12.3402])\n",
      "    Grad:   tensor([-0.1492,  0.8448])\n",
      "Epoch 743, Loss 5.086414\n",
      "    Params: tensor([  4.4922, -12.3487])\n",
      "    Grad:   tensor([-0.1490,  0.8434])\n",
      "Epoch 744, Loss 5.079086\n",
      "    Params: tensor([  4.4937, -12.3571])\n",
      "    Grad:   tensor([-0.1487,  0.8419])\n",
      "Epoch 745, Loss 5.071781\n",
      "    Params: tensor([  4.4952, -12.3655])\n",
      "    Grad:   tensor([-0.1485,  0.8405])\n",
      "Epoch 746, Loss 5.064505\n",
      "    Params: tensor([  4.4967, -12.3739])\n",
      "    Grad:   tensor([-0.1482,  0.8391])\n",
      "Epoch 747, Loss 5.057247\n",
      "    Params: tensor([  4.4981, -12.3823])\n",
      "    Grad:   tensor([-0.1480,  0.8376])\n",
      "Epoch 748, Loss 5.050021\n",
      "    Params: tensor([  4.4996, -12.3906])\n",
      "    Grad:   tensor([-0.1477,  0.8362])\n",
      "Epoch 749, Loss 5.042817\n",
      "    Params: tensor([  4.5011, -12.3990])\n",
      "    Grad:   tensor([-0.1475,  0.8348])\n",
      "Epoch 750, Loss 5.035636\n",
      "    Params: tensor([  4.5026, -12.4073])\n",
      "    Grad:   tensor([-0.1472,  0.8334])\n",
      "Epoch 751, Loss 5.028476\n",
      "    Params: tensor([  4.5040, -12.4156])\n",
      "    Grad:   tensor([-0.1470,  0.8320])\n",
      "Epoch 752, Loss 5.021346\n",
      "    Params: tensor([  4.5055, -12.4239])\n",
      "    Grad:   tensor([-0.1467,  0.8305])\n",
      "Epoch 753, Loss 5.014240\n",
      "    Params: tensor([  4.5070, -12.4322])\n",
      "    Grad:   tensor([-0.1465,  0.8291])\n",
      "Epoch 754, Loss 5.007157\n",
      "    Params: tensor([  4.5084, -12.4405])\n",
      "    Grad:   tensor([-0.1462,  0.8277])\n",
      "Epoch 755, Loss 5.000099\n",
      "    Params: tensor([  4.5099, -12.4488])\n",
      "    Grad:   tensor([-0.1460,  0.8263])\n",
      "Epoch 756, Loss 4.993064\n",
      "    Params: tensor([  4.5113, -12.4570])\n",
      "    Grad:   tensor([-0.1457,  0.8249])\n",
      "Epoch 757, Loss 4.986051\n",
      "    Params: tensor([  4.5128, -12.4653])\n",
      "    Grad:   tensor([-0.1455,  0.8235])\n",
      "Epoch 758, Loss 4.979064\n",
      "    Params: tensor([  4.5143, -12.4735])\n",
      "    Grad:   tensor([-0.1452,  0.8221])\n",
      "Epoch 759, Loss 4.972100\n",
      "    Params: tensor([  4.5157, -12.4817])\n",
      "    Grad:   tensor([-0.1450,  0.8207])\n",
      "Epoch 760, Loss 4.965159\n",
      "    Params: tensor([  4.5172, -12.4899])\n",
      "    Grad:   tensor([-0.1447,  0.8193])\n",
      "Epoch 761, Loss 4.958245\n",
      "    Params: tensor([  4.5186, -12.4981])\n",
      "    Grad:   tensor([-0.1445,  0.8179])\n",
      "Epoch 762, Loss 4.951351\n",
      "    Params: tensor([  4.5200, -12.5062])\n",
      "    Grad:   tensor([-0.1443,  0.8165])\n",
      "Epoch 763, Loss 4.944479\n",
      "    Params: tensor([  4.5215, -12.5144])\n",
      "    Grad:   tensor([-0.1440,  0.8152])\n",
      "Epoch 764, Loss 4.937633\n",
      "    Params: tensor([  4.5229, -12.5225])\n",
      "    Grad:   tensor([-0.1438,  0.8138])\n",
      "Epoch 765, Loss 4.930812\n",
      "    Params: tensor([  4.5244, -12.5306])\n",
      "    Grad:   tensor([-0.1435,  0.8124])\n",
      "Epoch 766, Loss 4.924009\n",
      "    Params: tensor([  4.5258, -12.5387])\n",
      "    Grad:   tensor([-0.1433,  0.8110])\n",
      "Epoch 767, Loss 4.917234\n",
      "    Params: tensor([  4.5272, -12.5468])\n",
      "    Grad:   tensor([-0.1430,  0.8096])\n",
      "Epoch 768, Loss 4.910480\n",
      "    Params: tensor([  4.5286, -12.5549])\n",
      "    Grad:   tensor([-0.1428,  0.8083])\n",
      "Epoch 769, Loss 4.903749\n",
      "    Params: tensor([  4.5301, -12.5630])\n",
      "    Grad:   tensor([-0.1426,  0.8069])\n",
      "Epoch 770, Loss 4.897040\n",
      "    Params: tensor([  4.5315, -12.5711])\n",
      "    Grad:   tensor([-0.1423,  0.8055])\n",
      "Epoch 771, Loss 4.890356\n",
      "    Params: tensor([  4.5329, -12.5791])\n",
      "    Grad:   tensor([-0.1420,  0.8042])\n",
      "Epoch 772, Loss 4.883692\n",
      "    Params: tensor([  4.5343, -12.5871])\n",
      "    Grad:   tensor([-0.1418,  0.8028])\n",
      "Epoch 773, Loss 4.877052\n",
      "    Params: tensor([  4.5357, -12.5951])\n",
      "    Grad:   tensor([-0.1416,  0.8014])\n",
      "Epoch 774, Loss 4.870436\n",
      "    Params: tensor([  4.5372, -12.6031])\n",
      "    Grad:   tensor([-0.1413,  0.8001])\n",
      "Epoch 775, Loss 4.863839\n",
      "    Params: tensor([  4.5386, -12.6111])\n",
      "    Grad:   tensor([-0.1411,  0.7987])\n",
      "Epoch 776, Loss 4.857268\n",
      "    Params: tensor([  4.5400, -12.6191])\n",
      "    Grad:   tensor([-0.1408,  0.7973])\n",
      "Epoch 777, Loss 4.850718\n",
      "    Params: tensor([  4.5414, -12.6271])\n",
      "    Grad:   tensor([-0.1406,  0.7960])\n",
      "Epoch 778, Loss 4.844189\n",
      "    Params: tensor([  4.5428, -12.6350])\n",
      "    Grad:   tensor([-0.1404,  0.7946])\n",
      "Epoch 779, Loss 4.837683\n",
      "    Params: tensor([  4.5442, -12.6429])\n",
      "    Grad:   tensor([-0.1401,  0.7933])\n",
      "Epoch 780, Loss 4.831196\n",
      "    Params: tensor([  4.5456, -12.6509])\n",
      "    Grad:   tensor([-0.1399,  0.7919])\n",
      "Epoch 781, Loss 4.824737\n",
      "    Params: tensor([  4.5470, -12.6588])\n",
      "    Grad:   tensor([-0.1397,  0.7906])\n",
      "Epoch 782, Loss 4.818298\n",
      "    Params: tensor([  4.5484, -12.6667])\n",
      "    Grad:   tensor([-0.1394,  0.7893])\n",
      "Epoch 783, Loss 4.811879\n",
      "    Params: tensor([  4.5498, -12.6745])\n",
      "    Grad:   tensor([-0.1392,  0.7879])\n",
      "Epoch 784, Loss 4.805481\n",
      "    Params: tensor([  4.5512, -12.6824])\n",
      "    Grad:   tensor([-0.1389,  0.7866])\n",
      "Epoch 785, Loss 4.799106\n",
      "    Params: tensor([  4.5525, -12.6902])\n",
      "    Grad:   tensor([-0.1387,  0.7852])\n",
      "Epoch 786, Loss 4.792755\n",
      "    Params: tensor([  4.5539, -12.6981])\n",
      "    Grad:   tensor([-0.1385,  0.7839])\n",
      "Epoch 787, Loss 4.786422\n",
      "    Params: tensor([  4.5553, -12.7059])\n",
      "    Grad:   tensor([-0.1383,  0.7826])\n",
      "Epoch 788, Loss 4.780112\n",
      "    Params: tensor([  4.5567, -12.7137])\n",
      "    Grad:   tensor([-0.1380,  0.7812])\n",
      "Epoch 789, Loss 4.773824\n",
      "    Params: tensor([  4.5581, -12.7215])\n",
      "    Grad:   tensor([-0.1378,  0.7799])\n",
      "Epoch 790, Loss 4.767558\n",
      "    Params: tensor([  4.5594, -12.7293])\n",
      "    Grad:   tensor([-0.1375,  0.7786])\n",
      "Epoch 791, Loss 4.761312\n",
      "    Params: tensor([  4.5608, -12.7371])\n",
      "    Grad:   tensor([-0.1373,  0.7773])\n",
      "Epoch 792, Loss 4.755087\n",
      "    Params: tensor([  4.5622, -12.7448])\n",
      "    Grad:   tensor([-0.1371,  0.7759])\n",
      "Epoch 793, Loss 4.748885\n",
      "    Params: tensor([  4.5636, -12.7526])\n",
      "    Grad:   tensor([-0.1368,  0.7746])\n",
      "Epoch 794, Loss 4.742700\n",
      "    Params: tensor([  4.5649, -12.7603])\n",
      "    Grad:   tensor([-0.1366,  0.7733])\n",
      "Epoch 795, Loss 4.736537\n",
      "    Params: tensor([  4.5663, -12.7680])\n",
      "    Grad:   tensor([-0.1364,  0.7720])\n",
      "Epoch 796, Loss 4.730397\n",
      "    Params: tensor([  4.5677, -12.7758])\n",
      "    Grad:   tensor([-0.1361,  0.7707])\n",
      "Epoch 797, Loss 4.724279\n",
      "    Params: tensor([  4.5690, -12.7834])\n",
      "    Grad:   tensor([-0.1359,  0.7694])\n",
      "Epoch 798, Loss 4.718181\n",
      "    Params: tensor([  4.5704, -12.7911])\n",
      "    Grad:   tensor([-0.1357,  0.7681])\n",
      "Epoch 799, Loss 4.712101\n",
      "    Params: tensor([  4.5717, -12.7988])\n",
      "    Grad:   tensor([-0.1354,  0.7668])\n",
      "Epoch 800, Loss 4.706046\n",
      "    Params: tensor([  4.5731, -12.8064])\n",
      "    Grad:   tensor([-0.1352,  0.7655])\n",
      "Epoch 801, Loss 4.700009\n",
      "    Params: tensor([  4.5744, -12.8141])\n",
      "    Grad:   tensor([-0.1350,  0.7642])\n",
      "Epoch 802, Loss 4.693990\n",
      "    Params: tensor([  4.5758, -12.8217])\n",
      "    Grad:   tensor([-0.1347,  0.7629])\n",
      "Epoch 803, Loss 4.687995\n",
      "    Params: tensor([  4.5771, -12.8293])\n",
      "    Grad:   tensor([-0.1345,  0.7616])\n",
      "Epoch 804, Loss 4.682020\n",
      "    Params: tensor([  4.5785, -12.8369])\n",
      "    Grad:   tensor([-0.1343,  0.7603])\n",
      "Epoch 805, Loss 4.676063\n",
      "    Params: tensor([  4.5798, -12.8445])\n",
      "    Grad:   tensor([-0.1341,  0.7590])\n",
      "Epoch 806, Loss 4.670130\n",
      "    Params: tensor([  4.5811, -12.8521])\n",
      "    Grad:   tensor([-0.1338,  0.7577])\n",
      "Epoch 807, Loss 4.664214\n",
      "    Params: tensor([  4.5825, -12.8597])\n",
      "    Grad:   tensor([-0.1336,  0.7564])\n",
      "Epoch 808, Loss 4.658319\n",
      "    Params: tensor([  4.5838, -12.8672])\n",
      "    Grad:   tensor([-0.1334,  0.7551])\n",
      "Epoch 809, Loss 4.652445\n",
      "    Params: tensor([  4.5851, -12.8748])\n",
      "    Grad:   tensor([-0.1332,  0.7538])\n",
      "Epoch 810, Loss 4.646592\n",
      "    Params: tensor([  4.5865, -12.8823])\n",
      "    Grad:   tensor([-0.1330,  0.7526])\n",
      "Epoch 811, Loss 4.640754\n",
      "    Params: tensor([  4.5878, -12.8898])\n",
      "    Grad:   tensor([-0.1327,  0.7513])\n",
      "Epoch 812, Loss 4.634938\n",
      "    Params: tensor([  4.5891, -12.8973])\n",
      "    Grad:   tensor([-0.1325,  0.7500])\n",
      "Epoch 813, Loss 4.629142\n",
      "    Params: tensor([  4.5904, -12.9048])\n",
      "    Grad:   tensor([-0.1323,  0.7487])\n",
      "Epoch 814, Loss 4.623367\n",
      "    Params: tensor([  4.5918, -12.9123])\n",
      "    Grad:   tensor([-0.1320,  0.7475])\n",
      "Epoch 815, Loss 4.617611\n",
      "    Params: tensor([  4.5931, -12.9197])\n",
      "    Grad:   tensor([-0.1318,  0.7462])\n",
      "Epoch 816, Loss 4.611872\n",
      "    Params: tensor([  4.5944, -12.9272])\n",
      "    Grad:   tensor([-0.1316,  0.7449])\n",
      "Epoch 817, Loss 4.606156\n",
      "    Params: tensor([  4.5957, -12.9346])\n",
      "    Grad:   tensor([-0.1314,  0.7437])\n",
      "Epoch 818, Loss 4.600458\n",
      "    Params: tensor([  4.5970, -12.9420])\n",
      "    Grad:   tensor([-0.1311,  0.7424])\n",
      "Epoch 819, Loss 4.594780\n",
      "    Params: tensor([  4.5983, -12.9494])\n",
      "    Grad:   tensor([-0.1309,  0.7411])\n",
      "Epoch 820, Loss 4.589119\n",
      "    Params: tensor([  4.5996, -12.9568])\n",
      "    Grad:   tensor([-0.1307,  0.7399])\n",
      "Epoch 821, Loss 4.583479\n",
      "    Params: tensor([  4.6009, -12.9642])\n",
      "    Grad:   tensor([-0.1305,  0.7386])\n",
      "Epoch 822, Loss 4.577857\n",
      "    Params: tensor([  4.6022, -12.9716])\n",
      "    Grad:   tensor([-0.1303,  0.7374])\n",
      "Epoch 823, Loss 4.572256\n",
      "    Params: tensor([  4.6035, -12.9790])\n",
      "    Grad:   tensor([-0.1300,  0.7361])\n",
      "Epoch 824, Loss 4.566675\n",
      "    Params: tensor([  4.6048, -12.9863])\n",
      "    Grad:   tensor([-0.1298,  0.7349])\n",
      "Epoch 825, Loss 4.561108\n",
      "    Params: tensor([  4.6061, -12.9936])\n",
      "    Grad:   tensor([-0.1296,  0.7336])\n",
      "Epoch 826, Loss 4.555565\n",
      "    Params: tensor([  4.6074, -13.0010])\n",
      "    Grad:   tensor([-0.1294,  0.7324])\n",
      "Epoch 827, Loss 4.550039\n",
      "    Params: tensor([  4.6087, -13.0083])\n",
      "    Grad:   tensor([-0.1292,  0.7311])\n",
      "Epoch 828, Loss 4.544534\n",
      "    Params: tensor([  4.6100, -13.0156])\n",
      "    Grad:   tensor([-0.1289,  0.7299])\n",
      "Epoch 829, Loss 4.539044\n",
      "    Params: tensor([  4.6113, -13.0229])\n",
      "    Grad:   tensor([-0.1287,  0.7286])\n",
      "Epoch 830, Loss 4.533575\n",
      "    Params: tensor([  4.6126, -13.0301])\n",
      "    Grad:   tensor([-0.1285,  0.7274])\n",
      "Epoch 831, Loss 4.528122\n",
      "    Params: tensor([  4.6139, -13.0374])\n",
      "    Grad:   tensor([-0.1283,  0.7262])\n",
      "Epoch 832, Loss 4.522691\n",
      "    Params: tensor([  4.6152, -13.0446])\n",
      "    Grad:   tensor([-0.1280,  0.7249])\n",
      "Epoch 833, Loss 4.517276\n",
      "    Params: tensor([  4.6164, -13.0519])\n",
      "    Grad:   tensor([-0.1278,  0.7237])\n",
      "Epoch 834, Loss 4.511879\n",
      "    Params: tensor([  4.6177, -13.0591])\n",
      "    Grad:   tensor([-0.1276,  0.7225])\n",
      "Epoch 835, Loss 4.506505\n",
      "    Params: tensor([  4.6190, -13.0663])\n",
      "    Grad:   tensor([-0.1274,  0.7212])\n",
      "Epoch 836, Loss 4.501141\n",
      "    Params: tensor([  4.6203, -13.0735])\n",
      "    Grad:   tensor([-0.1272,  0.7200])\n",
      "Epoch 837, Loss 4.495801\n",
      "    Params: tensor([  4.6215, -13.0807])\n",
      "    Grad:   tensor([-0.1270,  0.7188])\n",
      "Epoch 838, Loss 4.490475\n",
      "    Params: tensor([  4.6228, -13.0879])\n",
      "    Grad:   tensor([-0.1268,  0.7176])\n",
      "Epoch 839, Loss 4.485169\n",
      "    Params: tensor([  4.6241, -13.0950])\n",
      "    Grad:   tensor([-0.1266,  0.7163])\n",
      "Epoch 840, Loss 4.479884\n",
      "    Params: tensor([  4.6253, -13.1022])\n",
      "    Grad:   tensor([-0.1263,  0.7151])\n",
      "Epoch 841, Loss 4.474613\n",
      "    Params: tensor([  4.6266, -13.1093])\n",
      "    Grad:   tensor([-0.1261,  0.7139])\n",
      "Epoch 842, Loss 4.469364\n",
      "    Params: tensor([  4.6278, -13.1165])\n",
      "    Grad:   tensor([-0.1259,  0.7127])\n",
      "Epoch 843, Loss 4.464130\n",
      "    Params: tensor([  4.6291, -13.1236])\n",
      "    Grad:   tensor([-0.1257,  0.7115])\n",
      "Epoch 844, Loss 4.458913\n",
      "    Params: tensor([  4.6304, -13.1307])\n",
      "    Grad:   tensor([-0.1255,  0.7103])\n",
      "Epoch 845, Loss 4.453716\n",
      "    Params: tensor([  4.6316, -13.1378])\n",
      "    Grad:   tensor([-0.1253,  0.7091])\n",
      "Epoch 846, Loss 4.448535\n",
      "    Params: tensor([  4.6329, -13.1449])\n",
      "    Grad:   tensor([-0.1250,  0.7079])\n",
      "Epoch 847, Loss 4.443372\n",
      "    Params: tensor([  4.6341, -13.1519])\n",
      "    Grad:   tensor([-0.1249,  0.7067])\n",
      "Epoch 848, Loss 4.438226\n",
      "    Params: tensor([  4.6353, -13.1590])\n",
      "    Grad:   tensor([-0.1246,  0.7055])\n",
      "Epoch 849, Loss 4.433099\n",
      "    Params: tensor([  4.6366, -13.1660])\n",
      "    Grad:   tensor([-0.1244,  0.7043])\n",
      "Epoch 850, Loss 4.427990\n",
      "    Params: tensor([  4.6378, -13.1730])\n",
      "    Grad:   tensor([-0.1242,  0.7031])\n",
      "Epoch 851, Loss 4.422897\n",
      "    Params: tensor([  4.6391, -13.1801])\n",
      "    Grad:   tensor([-0.1240,  0.7019])\n",
      "Epoch 852, Loss 4.417819\n",
      "    Params: tensor([  4.6403, -13.1871])\n",
      "    Grad:   tensor([-0.1238,  0.7007])\n",
      "Epoch 853, Loss 4.412762\n",
      "    Params: tensor([  4.6415, -13.1941])\n",
      "    Grad:   tensor([-0.1236,  0.6995])\n",
      "Epoch 854, Loss 4.407721\n",
      "    Params: tensor([  4.6428, -13.2010])\n",
      "    Grad:   tensor([-0.1234,  0.6983])\n",
      "Epoch 855, Loss 4.402698\n",
      "    Params: tensor([  4.6440, -13.2080])\n",
      "    Grad:   tensor([-0.1232,  0.6971])\n",
      "Epoch 856, Loss 4.397688\n",
      "    Params: tensor([  4.6452, -13.2150])\n",
      "    Grad:   tensor([-0.1229,  0.6959])\n",
      "Epoch 857, Loss 4.392697\n",
      "    Params: tensor([  4.6465, -13.2219])\n",
      "    Grad:   tensor([-0.1227,  0.6948])\n",
      "Epoch 858, Loss 4.387725\n",
      "    Params: tensor([  4.6477, -13.2289])\n",
      "    Grad:   tensor([-0.1225,  0.6936])\n",
      "Epoch 859, Loss 4.382770\n",
      "    Params: tensor([  4.6489, -13.2358])\n",
      "    Grad:   tensor([-0.1223,  0.6924])\n",
      "Epoch 860, Loss 4.377828\n",
      "    Params: tensor([  4.6501, -13.2427])\n",
      "    Grad:   tensor([-0.1221,  0.6912])\n",
      "Epoch 861, Loss 4.372905\n",
      "    Params: tensor([  4.6514, -13.2496])\n",
      "    Grad:   tensor([-0.1219,  0.6901])\n",
      "Epoch 862, Loss 4.368000\n",
      "    Params: tensor([  4.6526, -13.2565])\n",
      "    Grad:   tensor([-0.1217,  0.6889])\n",
      "Epoch 863, Loss 4.363111\n",
      "    Params: tensor([  4.6538, -13.2634])\n",
      "    Grad:   tensor([-0.1215,  0.6877])\n",
      "Epoch 864, Loss 4.358238\n",
      "    Params: tensor([  4.6550, -13.2702])\n",
      "    Grad:   tensor([-0.1213,  0.6865])\n",
      "Epoch 865, Loss 4.353383\n",
      "    Params: tensor([  4.6562, -13.2771])\n",
      "    Grad:   tensor([-0.1211,  0.6854])\n",
      "Epoch 866, Loss 4.348542\n",
      "    Params: tensor([  4.6574, -13.2839])\n",
      "    Grad:   tensor([-0.1209,  0.6842])\n",
      "Epoch 867, Loss 4.343716\n",
      "    Params: tensor([  4.6586, -13.2908])\n",
      "    Grad:   tensor([-0.1207,  0.6830])\n",
      "Epoch 868, Loss 4.338911\n",
      "    Params: tensor([  4.6598, -13.2976])\n",
      "    Grad:   tensor([-0.1205,  0.6819])\n",
      "Epoch 869, Loss 4.334120\n",
      "    Params: tensor([  4.6610, -13.3044])\n",
      "    Grad:   tensor([-0.1203,  0.6807])\n",
      "Epoch 870, Loss 4.329345\n",
      "    Params: tensor([  4.6622, -13.3112])\n",
      "    Grad:   tensor([-0.1201,  0.6796])\n",
      "Epoch 871, Loss 4.324588\n",
      "    Params: tensor([  4.6634, -13.3180])\n",
      "    Grad:   tensor([-0.1198,  0.6784])\n",
      "Epoch 872, Loss 4.319846\n",
      "    Params: tensor([  4.6646, -13.3247])\n",
      "    Grad:   tensor([-0.1196,  0.6773])\n",
      "Epoch 873, Loss 4.315117\n",
      "    Params: tensor([  4.6658, -13.3315])\n",
      "    Grad:   tensor([-0.1195,  0.6761])\n",
      "Epoch 874, Loss 4.310409\n",
      "    Params: tensor([  4.6670, -13.3382])\n",
      "    Grad:   tensor([-0.1192,  0.6750])\n",
      "Epoch 875, Loss 4.305714\n",
      "    Params: tensor([  4.6682, -13.3450])\n",
      "    Grad:   tensor([-0.1190,  0.6738])\n",
      "Epoch 876, Loss 4.301036\n",
      "    Params: tensor([  4.6694, -13.3517])\n",
      "    Grad:   tensor([-0.1188,  0.6727])\n",
      "Epoch 877, Loss 4.296376\n",
      "    Params: tensor([  4.6706, -13.3584])\n",
      "    Grad:   tensor([-0.1186,  0.6715])\n",
      "Epoch 878, Loss 4.291727\n",
      "    Params: tensor([  4.6718, -13.3651])\n",
      "    Grad:   tensor([-0.1184,  0.6704])\n",
      "Epoch 879, Loss 4.287098\n",
      "    Params: tensor([  4.6730, -13.3718])\n",
      "    Grad:   tensor([-0.1182,  0.6693])\n",
      "Epoch 880, Loss 4.282482\n",
      "    Params: tensor([  4.6741, -13.3785])\n",
      "    Grad:   tensor([-0.1180,  0.6681])\n",
      "Epoch 881, Loss 4.277882\n",
      "    Params: tensor([  4.6753, -13.3852])\n",
      "    Grad:   tensor([-0.1178,  0.6670])\n",
      "Epoch 882, Loss 4.273299\n",
      "    Params: tensor([  4.6765, -13.3918])\n",
      "    Grad:   tensor([-0.1176,  0.6658])\n",
      "Epoch 883, Loss 4.268732\n",
      "    Params: tensor([  4.6777, -13.3985])\n",
      "    Grad:   tensor([-0.1174,  0.6647])\n",
      "Epoch 884, Loss 4.264178\n",
      "    Params: tensor([  4.6788, -13.4051])\n",
      "    Grad:   tensor([-0.1172,  0.6636])\n",
      "Epoch 885, Loss 4.259643\n",
      "    Params: tensor([  4.6800, -13.4117])\n",
      "    Grad:   tensor([-0.1170,  0.6625])\n",
      "Epoch 886, Loss 4.255120\n",
      "    Params: tensor([  4.6812, -13.4184])\n",
      "    Grad:   tensor([-0.1168,  0.6613])\n",
      "Epoch 887, Loss 4.250614\n",
      "    Params: tensor([  4.6823, -13.4250])\n",
      "    Grad:   tensor([-0.1166,  0.6602])\n",
      "Epoch 888, Loss 4.246124\n",
      "    Params: tensor([  4.6835, -13.4316])\n",
      "    Grad:   tensor([-0.1164,  0.6591])\n",
      "Epoch 889, Loss 4.241648\n",
      "    Params: tensor([  4.6847, -13.4381])\n",
      "    Grad:   tensor([-0.1162,  0.6580])\n",
      "Epoch 890, Loss 4.237185\n",
      "    Params: tensor([  4.6858, -13.4447])\n",
      "    Grad:   tensor([-0.1160,  0.6569])\n",
      "Epoch 891, Loss 4.232740\n",
      "    Params: tensor([  4.6870, -13.4513])\n",
      "    Grad:   tensor([-0.1158,  0.6557])\n",
      "Epoch 892, Loss 4.228308\n",
      "    Params: tensor([  4.6881, -13.4578])\n",
      "    Grad:   tensor([-0.1157,  0.6546])\n",
      "Epoch 893, Loss 4.223895\n",
      "    Params: tensor([  4.6893, -13.4643])\n",
      "    Grad:   tensor([-0.1154,  0.6535])\n",
      "Epoch 894, Loss 4.219494\n",
      "    Params: tensor([  4.6904, -13.4709])\n",
      "    Grad:   tensor([-0.1153,  0.6524])\n",
      "Epoch 895, Loss 4.215109\n",
      "    Params: tensor([  4.6916, -13.4774])\n",
      "    Grad:   tensor([-0.1151,  0.6513])\n",
      "Epoch 896, Loss 4.210737\n",
      "    Params: tensor([  4.6927, -13.4839])\n",
      "    Grad:   tensor([-0.1148,  0.6502])\n",
      "Epoch 897, Loss 4.206383\n",
      "    Params: tensor([  4.6939, -13.4904])\n",
      "    Grad:   tensor([-0.1147,  0.6491])\n",
      "Epoch 898, Loss 4.202043\n",
      "    Params: tensor([  4.6950, -13.4968])\n",
      "    Grad:   tensor([-0.1145,  0.6480])\n",
      "Epoch 899, Loss 4.197715\n",
      "    Params: tensor([  4.6962, -13.5033])\n",
      "    Grad:   tensor([-0.1143,  0.6469])\n",
      "Epoch 900, Loss 4.193405\n",
      "    Params: tensor([  4.6973, -13.5098])\n",
      "    Grad:   tensor([-0.1141,  0.6458])\n",
      "Epoch 901, Loss 4.189108\n",
      "    Params: tensor([  4.6985, -13.5162])\n",
      "    Grad:   tensor([-0.1139,  0.6447])\n",
      "Epoch 902, Loss 4.184825\n",
      "    Params: tensor([  4.6996, -13.5227])\n",
      "    Grad:   tensor([-0.1137,  0.6436])\n",
      "Epoch 903, Loss 4.180559\n",
      "    Params: tensor([  4.7007, -13.5291])\n",
      "    Grad:   tensor([-0.1135,  0.6425])\n",
      "Epoch 904, Loss 4.176305\n",
      "    Params: tensor([  4.7019, -13.5355])\n",
      "    Grad:   tensor([-0.1133,  0.6414])\n",
      "Epoch 905, Loss 4.172065\n",
      "    Params: tensor([  4.7030, -13.5419])\n",
      "    Grad:   tensor([-0.1131,  0.6403])\n",
      "Epoch 906, Loss 4.167842\n",
      "    Params: tensor([  4.7041, -13.5483])\n",
      "    Grad:   tensor([-0.1129,  0.6392])\n",
      "Epoch 907, Loss 4.163630\n",
      "    Params: tensor([  4.7053, -13.5547])\n",
      "    Grad:   tensor([-0.1127,  0.6381])\n",
      "Epoch 908, Loss 4.159436\n",
      "    Params: tensor([  4.7064, -13.5610])\n",
      "    Grad:   tensor([-0.1125,  0.6371])\n",
      "Epoch 909, Loss 4.155253\n",
      "    Params: tensor([  4.7075, -13.5674])\n",
      "    Grad:   tensor([-0.1124,  0.6360])\n",
      "Epoch 910, Loss 4.151086\n",
      "    Params: tensor([  4.7086, -13.5738])\n",
      "    Grad:   tensor([-0.1122,  0.6349])\n",
      "Epoch 911, Loss 4.146934\n",
      "    Params: tensor([  4.7097, -13.5801])\n",
      "    Grad:   tensor([-0.1120,  0.6338])\n",
      "Epoch 912, Loss 4.142794\n",
      "    Params: tensor([  4.7109, -13.5864])\n",
      "    Grad:   tensor([-0.1118,  0.6327])\n",
      "Epoch 913, Loss 4.138669\n",
      "    Params: tensor([  4.7120, -13.5927])\n",
      "    Grad:   tensor([-0.1116,  0.6317])\n",
      "Epoch 914, Loss 4.134559\n",
      "    Params: tensor([  4.7131, -13.5990])\n",
      "    Grad:   tensor([-0.1114,  0.6306])\n",
      "Epoch 915, Loss 4.130465\n",
      "    Params: tensor([  4.7142, -13.6053])\n",
      "    Grad:   tensor([-0.1112,  0.6295])\n",
      "Epoch 916, Loss 4.126378\n",
      "    Params: tensor([  4.7153, -13.6116])\n",
      "    Grad:   tensor([-0.1110,  0.6284])\n",
      "Epoch 917, Loss 4.122310\n",
      "    Params: tensor([  4.7164, -13.6179])\n",
      "    Grad:   tensor([-0.1108,  0.6274])\n",
      "Epoch 918, Loss 4.118253\n",
      "    Params: tensor([  4.7175, -13.6242])\n",
      "    Grad:   tensor([-0.1107,  0.6263])\n",
      "Epoch 919, Loss 4.114213\n",
      "    Params: tensor([  4.7186, -13.6304])\n",
      "    Grad:   tensor([-0.1104,  0.6253])\n",
      "Epoch 920, Loss 4.110184\n",
      "    Params: tensor([  4.7197, -13.6367])\n",
      "    Grad:   tensor([-0.1103,  0.6242])\n",
      "Epoch 921, Loss 4.106170\n",
      "    Params: tensor([  4.7208, -13.6429])\n",
      "    Grad:   tensor([-0.1101,  0.6231])\n",
      "Epoch 922, Loss 4.102171\n",
      "    Params: tensor([  4.7219, -13.6491])\n",
      "    Grad:   tensor([-0.1099,  0.6221])\n",
      "Epoch 923, Loss 4.098181\n",
      "    Params: tensor([  4.7230, -13.6553])\n",
      "    Grad:   tensor([-0.1097,  0.6210])\n",
      "Epoch 924, Loss 4.094209\n",
      "    Params: tensor([  4.7241, -13.6615])\n",
      "    Grad:   tensor([-0.1095,  0.6200])\n",
      "Epoch 925, Loss 4.090250\n",
      "    Params: tensor([  4.7252, -13.6677])\n",
      "    Grad:   tensor([-0.1093,  0.6189])\n",
      "Epoch 926, Loss 4.086300\n",
      "    Params: tensor([  4.7263, -13.6739])\n",
      "    Grad:   tensor([-0.1091,  0.6179])\n",
      "Epoch 927, Loss 4.082366\n",
      "    Params: tensor([  4.7274, -13.6800])\n",
      "    Grad:   tensor([-0.1090,  0.6168])\n",
      "Epoch 928, Loss 4.078448\n",
      "    Params: tensor([  4.7285, -13.6862])\n",
      "    Grad:   tensor([-0.1088,  0.6158])\n",
      "Epoch 929, Loss 4.074540\n",
      "    Params: tensor([  4.7296, -13.6924])\n",
      "    Grad:   tensor([-0.1086,  0.6147])\n",
      "Epoch 930, Loss 4.070650\n",
      "    Params: tensor([  4.7307, -13.6985])\n",
      "    Grad:   tensor([-0.1084,  0.6137])\n",
      "Epoch 931, Loss 4.066769\n",
      "    Params: tensor([  4.7317, -13.7046])\n",
      "    Grad:   tensor([-0.1082,  0.6126])\n",
      "Epoch 932, Loss 4.062900\n",
      "    Params: tensor([  4.7328, -13.7107])\n",
      "    Grad:   tensor([-0.1080,  0.6116])\n",
      "Epoch 933, Loss 4.059047\n",
      "    Params: tensor([  4.7339, -13.7168])\n",
      "    Grad:   tensor([-0.1079,  0.6105])\n",
      "Epoch 934, Loss 4.055204\n",
      "    Params: tensor([  4.7350, -13.7229])\n",
      "    Grad:   tensor([-0.1077,  0.6095])\n",
      "Epoch 935, Loss 4.051378\n",
      "    Params: tensor([  4.7360, -13.7290])\n",
      "    Grad:   tensor([-0.1075,  0.6085])\n",
      "Epoch 936, Loss 4.047564\n",
      "    Params: tensor([  4.7371, -13.7351])\n",
      "    Grad:   tensor([-0.1073,  0.6074])\n",
      "Epoch 937, Loss 4.043762\n",
      "    Params: tensor([  4.7382, -13.7412])\n",
      "    Grad:   tensor([-0.1071,  0.6064])\n",
      "Epoch 938, Loss 4.039972\n",
      "    Params: tensor([  4.7393, -13.7472])\n",
      "    Grad:   tensor([-0.1069,  0.6054])\n",
      "Epoch 939, Loss 4.036197\n",
      "    Params: tensor([  4.7403, -13.7533])\n",
      "    Grad:   tensor([-0.1068,  0.6043])\n",
      "Epoch 940, Loss 4.032433\n",
      "    Params: tensor([  4.7414, -13.7593])\n",
      "    Grad:   tensor([-0.1066,  0.6033])\n",
      "Epoch 941, Loss 4.028685\n",
      "    Params: tensor([  4.7425, -13.7653])\n",
      "    Grad:   tensor([-0.1064,  0.6023])\n",
      "Epoch 942, Loss 4.024947\n",
      "    Params: tensor([  4.7435, -13.7713])\n",
      "    Grad:   tensor([-0.1062,  0.6013])\n",
      "Epoch 943, Loss 4.021221\n",
      "    Params: tensor([  4.7446, -13.7773])\n",
      "    Grad:   tensor([-0.1060,  0.6003])\n",
      "Epoch 944, Loss 4.017508\n",
      "    Params: tensor([  4.7456, -13.7833])\n",
      "    Grad:   tensor([-0.1058,  0.5992])\n",
      "Epoch 945, Loss 4.013809\n",
      "    Params: tensor([  4.7467, -13.7893])\n",
      "    Grad:   tensor([-0.1057,  0.5982])\n",
      "Epoch 946, Loss 4.010123\n",
      "    Params: tensor([  4.7478, -13.7953])\n",
      "    Grad:   tensor([-0.1055,  0.5972])\n",
      "Epoch 947, Loss 4.006446\n",
      "    Params: tensor([  4.7488, -13.8012])\n",
      "    Grad:   tensor([-0.1053,  0.5962])\n",
      "Epoch 948, Loss 4.002786\n",
      "    Params: tensor([  4.7499, -13.8072])\n",
      "    Grad:   tensor([-0.1051,  0.5952])\n",
      "Epoch 949, Loss 3.999135\n",
      "    Params: tensor([  4.7509, -13.8131])\n",
      "    Grad:   tensor([-0.1050,  0.5942])\n",
      "Epoch 950, Loss 3.995498\n",
      "    Params: tensor([  4.7520, -13.8191])\n",
      "    Grad:   tensor([-0.1048,  0.5931])\n",
      "Epoch 951, Loss 3.991874\n",
      "    Params: tensor([  4.7530, -13.8250])\n",
      "    Grad:   tensor([-0.1046,  0.5921])\n",
      "Epoch 952, Loss 3.988261\n",
      "    Params: tensor([  4.7540, -13.8309])\n",
      "    Grad:   tensor([-0.1044,  0.5911])\n",
      "Epoch 953, Loss 3.984660\n",
      "    Params: tensor([  4.7551, -13.8368])\n",
      "    Grad:   tensor([-0.1042,  0.5901])\n",
      "Epoch 954, Loss 3.981073\n",
      "    Params: tensor([  4.7561, -13.8427])\n",
      "    Grad:   tensor([-0.1041,  0.5891])\n",
      "Epoch 955, Loss 3.977496\n",
      "    Params: tensor([  4.7572, -13.8486])\n",
      "    Grad:   tensor([-0.1039,  0.5881])\n",
      "Epoch 956, Loss 3.973931\n",
      "    Params: tensor([  4.7582, -13.8544])\n",
      "    Grad:   tensor([-0.1037,  0.5871])\n",
      "Epoch 957, Loss 3.970381\n",
      "    Params: tensor([  4.7592, -13.8603])\n",
      "    Grad:   tensor([-0.1035,  0.5861])\n",
      "Epoch 958, Loss 3.966841\n",
      "    Params: tensor([  4.7603, -13.8661])\n",
      "    Grad:   tensor([-0.1034,  0.5851])\n",
      "Epoch 959, Loss 3.963313\n",
      "    Params: tensor([  4.7613, -13.8720])\n",
      "    Grad:   tensor([-0.1032,  0.5841])\n",
      "Epoch 960, Loss 3.959796\n",
      "    Params: tensor([  4.7623, -13.8778])\n",
      "    Grad:   tensor([-0.1030,  0.5831])\n",
      "Epoch 961, Loss 3.956295\n",
      "    Params: tensor([  4.7634, -13.8836])\n",
      "    Grad:   tensor([-0.1028,  0.5822])\n",
      "Epoch 962, Loss 3.952801\n",
      "    Params: tensor([  4.7644, -13.8895])\n",
      "    Grad:   tensor([-0.1026,  0.5812])\n",
      "Epoch 963, Loss 3.949323\n",
      "    Params: tensor([  4.7654, -13.8953])\n",
      "    Grad:   tensor([-0.1025,  0.5802])\n",
      "Epoch 964, Loss 3.945855\n",
      "    Params: tensor([  4.7664, -13.9010])\n",
      "    Grad:   tensor([-0.1023,  0.5792])\n",
      "Epoch 965, Loss 3.942398\n",
      "    Params: tensor([  4.7675, -13.9068])\n",
      "    Grad:   tensor([-0.1021,  0.5782])\n",
      "Epoch 966, Loss 3.938954\n",
      "    Params: tensor([  4.7685, -13.9126])\n",
      "    Grad:   tensor([-0.1020,  0.5772])\n",
      "Epoch 967, Loss 3.935520\n",
      "    Params: tensor([  4.7695, -13.9184])\n",
      "    Grad:   tensor([-0.1018,  0.5762])\n",
      "Epoch 968, Loss 3.932096\n",
      "    Params: tensor([  4.7705, -13.9241])\n",
      "    Grad:   tensor([-0.1016,  0.5753])\n",
      "Epoch 969, Loss 3.928688\n",
      "    Params: tensor([  4.7715, -13.9299])\n",
      "    Grad:   tensor([-0.1015,  0.5743])\n",
      "Epoch 970, Loss 3.925292\n",
      "    Params: tensor([  4.7725, -13.9356])\n",
      "    Grad:   tensor([-0.1013,  0.5733])\n",
      "Epoch 971, Loss 3.921906\n",
      "    Params: tensor([  4.7736, -13.9413])\n",
      "    Grad:   tensor([-0.1011,  0.5723])\n",
      "Epoch 972, Loss 3.918527\n",
      "    Params: tensor([  4.7746, -13.9470])\n",
      "    Grad:   tensor([-0.1009,  0.5714])\n",
      "Epoch 973, Loss 3.915166\n",
      "    Params: tensor([  4.7756, -13.9527])\n",
      "    Grad:   tensor([-0.1008,  0.5704])\n",
      "Epoch 974, Loss 3.911815\n",
      "    Params: tensor([  4.7766, -13.9584])\n",
      "    Grad:   tensor([-0.1006,  0.5694])\n",
      "Epoch 975, Loss 3.908474\n",
      "    Params: tensor([  4.7776, -13.9641])\n",
      "    Grad:   tensor([-0.1004,  0.5685])\n",
      "Epoch 976, Loss 3.905143\n",
      "    Params: tensor([  4.7786, -13.9698])\n",
      "    Grad:   tensor([-0.1003,  0.5675])\n",
      "Epoch 977, Loss 3.901825\n",
      "    Params: tensor([  4.7796, -13.9755])\n",
      "    Grad:   tensor([-0.1001,  0.5665])\n",
      "Epoch 978, Loss 3.898517\n",
      "    Params: tensor([  4.7806, -13.9811])\n",
      "    Grad:   tensor([-0.0999,  0.5656])\n",
      "Epoch 979, Loss 3.895222\n",
      "    Params: tensor([  4.7816, -13.9868])\n",
      "    Grad:   tensor([-0.0997,  0.5646])\n",
      "Epoch 980, Loss 3.891935\n",
      "    Params: tensor([  4.7826, -13.9924])\n",
      "    Grad:   tensor([-0.0996,  0.5637])\n",
      "Epoch 981, Loss 3.888664\n",
      "    Params: tensor([  4.7836, -13.9980])\n",
      "    Grad:   tensor([-0.0994,  0.5627])\n",
      "Epoch 982, Loss 3.885401\n",
      "    Params: tensor([  4.7846, -14.0036])\n",
      "    Grad:   tensor([-0.0992,  0.5617])\n",
      "Epoch 983, Loss 3.882150\n",
      "    Params: tensor([  4.7856, -14.0092])\n",
      "    Grad:   tensor([-0.0991,  0.5608])\n",
      "Epoch 984, Loss 3.878910\n",
      "    Params: tensor([  4.7865, -14.0148])\n",
      "    Grad:   tensor([-0.0989,  0.5598])\n",
      "Epoch 985, Loss 3.875680\n",
      "    Params: tensor([  4.7875, -14.0204])\n",
      "    Grad:   tensor([-0.0987,  0.5589])\n",
      "Epoch 986, Loss 3.872463\n",
      "    Params: tensor([  4.7885, -14.0260])\n",
      "    Grad:   tensor([-0.0986,  0.5579])\n",
      "Epoch 987, Loss 3.869256\n",
      "    Params: tensor([  4.7895, -14.0316])\n",
      "    Grad:   tensor([-0.0984,  0.5570])\n",
      "Epoch 988, Loss 3.866060\n",
      "    Params: tensor([  4.7905, -14.0371])\n",
      "    Grad:   tensor([-0.0982,  0.5560])\n",
      "Epoch 989, Loss 3.862872\n",
      "    Params: tensor([  4.7915, -14.0427])\n",
      "    Grad:   tensor([-0.0981,  0.5551])\n",
      "Epoch 990, Loss 3.859699\n",
      "    Params: tensor([  4.7924, -14.0482])\n",
      "    Grad:   tensor([-0.0979,  0.5541])\n",
      "Epoch 991, Loss 3.856535\n",
      "    Params: tensor([  4.7934, -14.0538])\n",
      "    Grad:   tensor([-0.0978,  0.5532])\n",
      "Epoch 992, Loss 3.853381\n",
      "    Params: tensor([  4.7944, -14.0593])\n",
      "    Grad:   tensor([-0.0976,  0.5523])\n",
      "Epoch 993, Loss 3.850237\n",
      "    Params: tensor([  4.7954, -14.0648])\n",
      "    Grad:   tensor([-0.0974,  0.5513])\n",
      "Epoch 994, Loss 3.847109\n",
      "    Params: tensor([  4.7963, -14.0703])\n",
      "    Grad:   tensor([-0.0973,  0.5504])\n",
      "Epoch 995, Loss 3.843984\n",
      "    Params: tensor([  4.7973, -14.0758])\n",
      "    Grad:   tensor([-0.0971,  0.5495])\n",
      "Epoch 996, Loss 3.840876\n",
      "    Params: tensor([  4.7983, -14.0813])\n",
      "    Grad:   tensor([-0.0969,  0.5485])\n",
      "Epoch 997, Loss 3.837775\n",
      "    Params: tensor([  4.7992, -14.0868])\n",
      "    Grad:   tensor([-0.0967,  0.5476])\n",
      "Epoch 998, Loss 3.834686\n",
      "    Params: tensor([  4.8002, -14.0922])\n",
      "    Grad:   tensor([-0.0966,  0.5467])\n",
      "Epoch 999, Loss 3.831606\n",
      "    Params: tensor([  4.8012, -14.0977])\n",
      "    Grad:   tensor([-0.0964,  0.5457])\n",
      "Epoch 1000, Loss 3.828538\n",
      "    Params: tensor([  4.8021, -14.1031])\n",
      "    Grad:   tensor([-0.0962,  0.5448])\n",
      "Epoch 1001, Loss 3.825483\n",
      "    Params: tensor([  4.8031, -14.1086])\n",
      "    Grad:   tensor([-0.0961,  0.5439])\n",
      "Epoch 1002, Loss 3.822433\n",
      "    Params: tensor([  4.8041, -14.1140])\n",
      "    Grad:   tensor([-0.0959,  0.5430])\n",
      "Epoch 1003, Loss 3.819398\n",
      "    Params: tensor([  4.8050, -14.1194])\n",
      "    Grad:   tensor([-0.0957,  0.5420])\n",
      "Epoch 1004, Loss 3.816369\n",
      "    Params: tensor([  4.8060, -14.1248])\n",
      "    Grad:   tensor([-0.0956,  0.5411])\n",
      "Epoch 1005, Loss 3.813350\n",
      "    Params: tensor([  4.8069, -14.1302])\n",
      "    Grad:   tensor([-0.0954,  0.5402])\n",
      "Epoch 1006, Loss 3.810344\n",
      "    Params: tensor([  4.8079, -14.1356])\n",
      "    Grad:   tensor([-0.0953,  0.5393])\n",
      "Epoch 1007, Loss 3.807348\n",
      "    Params: tensor([  4.8088, -14.1410])\n",
      "    Grad:   tensor([-0.0951,  0.5384])\n",
      "Epoch 1008, Loss 3.804360\n",
      "    Params: tensor([  4.8098, -14.1464])\n",
      "    Grad:   tensor([-0.0949,  0.5375])\n",
      "Epoch 1009, Loss 3.801384\n",
      "    Params: tensor([  4.8107, -14.1518])\n",
      "    Grad:   tensor([-0.0948,  0.5365])\n",
      "Epoch 1010, Loss 3.798421\n",
      "    Params: tensor([  4.8117, -14.1571])\n",
      "    Grad:   tensor([-0.0946,  0.5356])\n",
      "Epoch 1011, Loss 3.795465\n",
      "    Params: tensor([  4.8126, -14.1625])\n",
      "    Grad:   tensor([-0.0945,  0.5347])\n",
      "Epoch 1012, Loss 3.792518\n",
      "    Params: tensor([  4.8136, -14.1678])\n",
      "    Grad:   tensor([-0.0943,  0.5338])\n",
      "Epoch 1013, Loss 3.789584\n",
      "    Params: tensor([  4.8145, -14.1731])\n",
      "    Grad:   tensor([-0.0942,  0.5329])\n",
      "Epoch 1014, Loss 3.786658\n",
      "    Params: tensor([  4.8154, -14.1784])\n",
      "    Grad:   tensor([-0.0940,  0.5320])\n",
      "Epoch 1015, Loss 3.783740\n",
      "    Params: tensor([  4.8164, -14.1838])\n",
      "    Grad:   tensor([-0.0938,  0.5311])\n",
      "Epoch 1016, Loss 3.780832\n",
      "    Params: tensor([  4.8173, -14.1891])\n",
      "    Grad:   tensor([-0.0937,  0.5302])\n",
      "Epoch 1017, Loss 3.777939\n",
      "    Params: tensor([  4.8183, -14.1943])\n",
      "    Grad:   tensor([-0.0935,  0.5293])\n",
      "Epoch 1018, Loss 3.775053\n",
      "    Params: tensor([  4.8192, -14.1996])\n",
      "    Grad:   tensor([-0.0933,  0.5284])\n",
      "Epoch 1019, Loss 3.772173\n",
      "    Params: tensor([  4.8201, -14.2049])\n",
      "    Grad:   tensor([-0.0932,  0.5275])\n",
      "Epoch 1020, Loss 3.769311\n",
      "    Params: tensor([  4.8210, -14.2102])\n",
      "    Grad:   tensor([-0.0930,  0.5266])\n",
      "Epoch 1021, Loss 3.766450\n",
      "    Params: tensor([  4.8220, -14.2154])\n",
      "    Grad:   tensor([-0.0929,  0.5257])\n",
      "Epoch 1022, Loss 3.763602\n",
      "    Params: tensor([  4.8229, -14.2207])\n",
      "    Grad:   tensor([-0.0927,  0.5248])\n",
      "Epoch 1023, Loss 3.760766\n",
      "    Params: tensor([  4.8238, -14.2259])\n",
      "    Grad:   tensor([-0.0926,  0.5239])\n",
      "Epoch 1024, Loss 3.757936\n",
      "    Params: tensor([  4.8248, -14.2311])\n",
      "    Grad:   tensor([-0.0924,  0.5230])\n",
      "Epoch 1025, Loss 3.755118\n",
      "    Params: tensor([  4.8257, -14.2364])\n",
      "    Grad:   tensor([-0.0922,  0.5221])\n",
      "Epoch 1026, Loss 3.752309\n",
      "    Params: tensor([  4.8266, -14.2416])\n",
      "    Grad:   tensor([-0.0921,  0.5213])\n",
      "Epoch 1027, Loss 3.749511\n",
      "    Params: tensor([  4.8275, -14.2468])\n",
      "    Grad:   tensor([-0.0919,  0.5204])\n",
      "Epoch 1028, Loss 3.746722\n",
      "    Params: tensor([  4.8284, -14.2520])\n",
      "    Grad:   tensor([-0.0918,  0.5195])\n",
      "Epoch 1029, Loss 3.743940\n",
      "    Params: tensor([  4.8293, -14.2572])\n",
      "    Grad:   tensor([-0.0916,  0.5186])\n",
      "Epoch 1030, Loss 3.741169\n",
      "    Params: tensor([  4.8303, -14.2623])\n",
      "    Grad:   tensor([-0.0915,  0.5177])\n",
      "Epoch 1031, Loss 3.738407\n",
      "    Params: tensor([  4.8312, -14.2675])\n",
      "    Grad:   tensor([-0.0913,  0.5168])\n",
      "Epoch 1032, Loss 3.735656\n",
      "    Params: tensor([  4.8321, -14.2727])\n",
      "    Grad:   tensor([-0.0912,  0.5160])\n",
      "Epoch 1033, Loss 3.732914\n",
      "    Params: tensor([  4.8330, -14.2778])\n",
      "    Grad:   tensor([-0.0910,  0.5151])\n",
      "Epoch 1034, Loss 3.730181\n",
      "    Params: tensor([  4.8339, -14.2830])\n",
      "    Grad:   tensor([-0.0908,  0.5142])\n",
      "Epoch 1035, Loss 3.727456\n",
      "    Params: tensor([  4.8348, -14.2881])\n",
      "    Grad:   tensor([-0.0907,  0.5133])\n",
      "Epoch 1036, Loss 3.724740\n",
      "    Params: tensor([  4.8357, -14.2932])\n",
      "    Grad:   tensor([-0.0905,  0.5125])\n",
      "Epoch 1037, Loss 3.722034\n",
      "    Params: tensor([  4.8366, -14.2983])\n",
      "    Grad:   tensor([-0.0904,  0.5116])\n",
      "Epoch 1038, Loss 3.719337\n",
      "    Params: tensor([  4.8375, -14.3034])\n",
      "    Grad:   tensor([-0.0902,  0.5107])\n",
      "Epoch 1039, Loss 3.716651\n",
      "    Params: tensor([  4.8384, -14.3085])\n",
      "    Grad:   tensor([-0.0901,  0.5099])\n",
      "Epoch 1040, Loss 3.713972\n",
      "    Params: tensor([  4.8393, -14.3136])\n",
      "    Grad:   tensor([-0.0899,  0.5090])\n",
      "Epoch 1041, Loss 3.711302\n",
      "    Params: tensor([  4.8402, -14.3187])\n",
      "    Grad:   tensor([-0.0898,  0.5081])\n",
      "Epoch 1042, Loss 3.708644\n",
      "    Params: tensor([  4.8411, -14.3238])\n",
      "    Grad:   tensor([-0.0896,  0.5073])\n",
      "Epoch 1043, Loss 3.705991\n",
      "    Params: tensor([  4.8420, -14.3288])\n",
      "    Grad:   tensor([-0.0895,  0.5064])\n",
      "Epoch 1044, Loss 3.703351\n",
      "    Params: tensor([  4.8429, -14.3339])\n",
      "    Grad:   tensor([-0.0893,  0.5055])\n",
      "Epoch 1045, Loss 3.700716\n",
      "    Params: tensor([  4.8438, -14.3390])\n",
      "    Grad:   tensor([-0.0892,  0.5047])\n",
      "Epoch 1046, Loss 3.698091\n",
      "    Params: tensor([  4.8447, -14.3440])\n",
      "    Grad:   tensor([-0.0890,  0.5038])\n",
      "Epoch 1047, Loss 3.695476\n",
      "    Params: tensor([  4.8456, -14.3490])\n",
      "    Grad:   tensor([-0.0888,  0.5030])\n",
      "Epoch 1048, Loss 3.692869\n",
      "    Params: tensor([  4.8465, -14.3540])\n",
      "    Grad:   tensor([-0.0887,  0.5021])\n",
      "Epoch 1049, Loss 3.690273\n",
      "    Params: tensor([  4.8473, -14.3591])\n",
      "    Grad:   tensor([-0.0886,  0.5013])\n",
      "Epoch 1050, Loss 3.687683\n",
      "    Params: tensor([  4.8482, -14.3641])\n",
      "    Grad:   tensor([-0.0884,  0.5004])\n",
      "Epoch 1051, Loss 3.685104\n",
      "    Params: tensor([  4.8491, -14.3691])\n",
      "    Grad:   tensor([-0.0882,  0.4996])\n",
      "Epoch 1052, Loss 3.682532\n",
      "    Params: tensor([  4.8500, -14.3740])\n",
      "    Grad:   tensor([-0.0881,  0.4987])\n",
      "Epoch 1053, Loss 3.679969\n",
      "    Params: tensor([  4.8509, -14.3790])\n",
      "    Grad:   tensor([-0.0879,  0.4979])\n",
      "Epoch 1054, Loss 3.677417\n",
      "    Params: tensor([  4.8518, -14.3840])\n",
      "    Grad:   tensor([-0.0878,  0.4970])\n",
      "Epoch 1055, Loss 3.674871\n",
      "    Params: tensor([  4.8526, -14.3889])\n",
      "    Grad:   tensor([-0.0877,  0.4962])\n",
      "Epoch 1056, Loss 3.672335\n",
      "    Params: tensor([  4.8535, -14.3939])\n",
      "    Grad:   tensor([-0.0875,  0.4953])\n",
      "Epoch 1057, Loss 3.669804\n",
      "    Params: tensor([  4.8544, -14.3988])\n",
      "    Grad:   tensor([-0.0873,  0.4945])\n",
      "Epoch 1058, Loss 3.667287\n",
      "    Params: tensor([  4.8552, -14.4038])\n",
      "    Grad:   tensor([-0.0872,  0.4936])\n",
      "Epoch 1059, Loss 3.664775\n",
      "    Params: tensor([  4.8561, -14.4087])\n",
      "    Grad:   tensor([-0.0870,  0.4928])\n",
      "Epoch 1060, Loss 3.662273\n",
      "    Params: tensor([  4.8570, -14.4136])\n",
      "    Grad:   tensor([-0.0869,  0.4920])\n",
      "Epoch 1061, Loss 3.659778\n",
      "    Params: tensor([  4.8579, -14.4185])\n",
      "    Grad:   tensor([-0.0868,  0.4911])\n",
      "Epoch 1062, Loss 3.657295\n",
      "    Params: tensor([  4.8587, -14.4234])\n",
      "    Grad:   tensor([-0.0866,  0.4903])\n",
      "Epoch 1063, Loss 3.654816\n",
      "    Params: tensor([  4.8596, -14.4283])\n",
      "    Grad:   tensor([-0.0865,  0.4895])\n",
      "Epoch 1064, Loss 3.652349\n",
      "    Params: tensor([  4.8604, -14.4332])\n",
      "    Grad:   tensor([-0.0863,  0.4886])\n",
      "Epoch 1065, Loss 3.649889\n",
      "    Params: tensor([  4.8613, -14.4381])\n",
      "    Grad:   tensor([-0.0862,  0.4878])\n",
      "Epoch 1066, Loss 3.647437\n",
      "    Params: tensor([  4.8622, -14.4430])\n",
      "    Grad:   tensor([-0.0860,  0.4870])\n",
      "Epoch 1067, Loss 3.644991\n",
      "    Params: tensor([  4.8630, -14.4478])\n",
      "    Grad:   tensor([-0.0859,  0.4862])\n",
      "Epoch 1068, Loss 3.642559\n",
      "    Params: tensor([  4.8639, -14.4527])\n",
      "    Grad:   tensor([-0.0857,  0.4853])\n",
      "Epoch 1069, Loss 3.640132\n",
      "    Params: tensor([  4.8647, -14.4575])\n",
      "    Grad:   tensor([-0.0856,  0.4845])\n",
      "Epoch 1070, Loss 3.637711\n",
      "    Params: tensor([  4.8656, -14.4624])\n",
      "    Grad:   tensor([-0.0854,  0.4837])\n",
      "Epoch 1071, Loss 3.635302\n",
      "    Params: tensor([  4.8665, -14.4672])\n",
      "    Grad:   tensor([-0.0853,  0.4829])\n",
      "Epoch 1072, Loss 3.632902\n",
      "    Params: tensor([  4.8673, -14.4720])\n",
      "    Grad:   tensor([-0.0851,  0.4820])\n",
      "Epoch 1073, Loss 3.630508\n",
      "    Params: tensor([  4.8682, -14.4768])\n",
      "    Grad:   tensor([-0.0850,  0.4812])\n",
      "Epoch 1074, Loss 3.628119\n",
      "    Params: tensor([  4.8690, -14.4816])\n",
      "    Grad:   tensor([-0.0849,  0.4804])\n",
      "Epoch 1075, Loss 3.625741\n",
      "    Params: tensor([  4.8698, -14.4864])\n",
      "    Grad:   tensor([-0.0847,  0.4796])\n",
      "Epoch 1076, Loss 3.623374\n",
      "    Params: tensor([  4.8707, -14.4912])\n",
      "    Grad:   tensor([-0.0846,  0.4788])\n",
      "Epoch 1077, Loss 3.621010\n",
      "    Params: tensor([  4.8715, -14.4960])\n",
      "    Grad:   tensor([-0.0844,  0.4780])\n",
      "Epoch 1078, Loss 3.618659\n",
      "    Params: tensor([  4.8724, -14.5008])\n",
      "    Grad:   tensor([-0.0843,  0.4771])\n",
      "Epoch 1079, Loss 3.616311\n",
      "    Params: tensor([  4.8732, -14.5055])\n",
      "    Grad:   tensor([-0.0841,  0.4763])\n",
      "Epoch 1080, Loss 3.613973\n",
      "    Params: tensor([  4.8741, -14.5103])\n",
      "    Grad:   tensor([-0.0840,  0.4755])\n",
      "Epoch 1081, Loss 3.611643\n",
      "    Params: tensor([  4.8749, -14.5150])\n",
      "    Grad:   tensor([-0.0839,  0.4747])\n",
      "Epoch 1082, Loss 3.609321\n",
      "    Params: tensor([  4.8757, -14.5198])\n",
      "    Grad:   tensor([-0.0837,  0.4739])\n",
      "Epoch 1083, Loss 3.607008\n",
      "    Params: tensor([  4.8766, -14.5245])\n",
      "    Grad:   tensor([-0.0836,  0.4731])\n",
      "Epoch 1084, Loss 3.604701\n",
      "    Params: tensor([  4.8774, -14.5292])\n",
      "    Grad:   tensor([-0.0834,  0.4723])\n",
      "Epoch 1085, Loss 3.602403\n",
      "    Params: tensor([  4.8782, -14.5339])\n",
      "    Grad:   tensor([-0.0833,  0.4715])\n",
      "Epoch 1086, Loss 3.600114\n",
      "    Params: tensor([  4.8791, -14.5387])\n",
      "    Grad:   tensor([-0.0832,  0.4707])\n",
      "Epoch 1087, Loss 3.597831\n",
      "    Params: tensor([  4.8799, -14.5434])\n",
      "    Grad:   tensor([-0.0830,  0.4699])\n",
      "Epoch 1088, Loss 3.595553\n",
      "    Params: tensor([  4.8807, -14.5480])\n",
      "    Grad:   tensor([-0.0829,  0.4691])\n",
      "Epoch 1089, Loss 3.593287\n",
      "    Params: tensor([  4.8816, -14.5527])\n",
      "    Grad:   tensor([-0.0827,  0.4683])\n",
      "Epoch 1090, Loss 3.591030\n",
      "    Params: tensor([  4.8824, -14.5574])\n",
      "    Grad:   tensor([-0.0826,  0.4675])\n",
      "Epoch 1091, Loss 3.588776\n",
      "    Params: tensor([  4.8832, -14.5621])\n",
      "    Grad:   tensor([-0.0824,  0.4667])\n",
      "Epoch 1092, Loss 3.586534\n",
      "    Params: tensor([  4.8840, -14.5667])\n",
      "    Grad:   tensor([-0.0823,  0.4659])\n",
      "Epoch 1093, Loss 3.584294\n",
      "    Params: tensor([  4.8849, -14.5714])\n",
      "    Grad:   tensor([-0.0822,  0.4651])\n",
      "Epoch 1094, Loss 3.582067\n",
      "    Params: tensor([  4.8857, -14.5760])\n",
      "    Grad:   tensor([-0.0820,  0.4643])\n",
      "Epoch 1095, Loss 3.579845\n",
      "    Params: tensor([  4.8865, -14.5807])\n",
      "    Grad:   tensor([-0.0819,  0.4636])\n",
      "Epoch 1096, Loss 3.577631\n",
      "    Params: tensor([  4.8873, -14.5853])\n",
      "    Grad:   tensor([-0.0818,  0.4628])\n",
      "Epoch 1097, Loss 3.575424\n",
      "    Params: tensor([  4.8881, -14.5899])\n",
      "    Grad:   tensor([-0.0816,  0.4620])\n",
      "Epoch 1098, Loss 3.573225\n",
      "    Params: tensor([  4.8889, -14.5945])\n",
      "    Grad:   tensor([-0.0815,  0.4612])\n",
      "Epoch 1099, Loss 3.571035\n",
      "    Params: tensor([  4.8898, -14.5991])\n",
      "    Grad:   tensor([-0.0813,  0.4604])\n",
      "Epoch 1100, Loss 3.568848\n",
      "    Params: tensor([  4.8906, -14.6037])\n",
      "    Grad:   tensor([-0.0812,  0.4596])\n",
      "Epoch 1101, Loss 3.566673\n",
      "    Params: tensor([  4.8914, -14.6083])\n",
      "    Grad:   tensor([-0.0810,  0.4588])\n",
      "Epoch 1102, Loss 3.564506\n",
      "    Params: tensor([  4.8922, -14.6129])\n",
      "    Grad:   tensor([-0.0809,  0.4581])\n",
      "Epoch 1103, Loss 3.562341\n",
      "    Params: tensor([  4.8930, -14.6175])\n",
      "    Grad:   tensor([-0.0808,  0.4573])\n",
      "Epoch 1104, Loss 3.560185\n",
      "    Params: tensor([  4.8938, -14.6220])\n",
      "    Grad:   tensor([-0.0806,  0.4565])\n",
      "Epoch 1105, Loss 3.558040\n",
      "    Params: tensor([  4.8946, -14.6266])\n",
      "    Grad:   tensor([-0.0805,  0.4557])\n",
      "Epoch 1106, Loss 3.555901\n",
      "    Params: tensor([  4.8954, -14.6311])\n",
      "    Grad:   tensor([-0.0804,  0.4550])\n",
      "Epoch 1107, Loss 3.553767\n",
      "    Params: tensor([  4.8962, -14.6357])\n",
      "    Grad:   tensor([-0.0802,  0.4542])\n",
      "Epoch 1108, Loss 3.551641\n",
      "    Params: tensor([  4.8970, -14.6402])\n",
      "    Grad:   tensor([-0.0801,  0.4534])\n",
      "Epoch 1109, Loss 3.549524\n",
      "    Params: tensor([  4.8978, -14.6447])\n",
      "    Grad:   tensor([-0.0799,  0.4527])\n",
      "Epoch 1110, Loss 3.547411\n",
      "    Params: tensor([  4.8986, -14.6493])\n",
      "    Grad:   tensor([-0.0798,  0.4519])\n",
      "Epoch 1111, Loss 3.545309\n",
      "    Params: tensor([  4.8994, -14.6538])\n",
      "    Grad:   tensor([-0.0797,  0.4511])\n",
      "Epoch 1112, Loss 3.543211\n",
      "    Params: tensor([  4.9002, -14.6583])\n",
      "    Grad:   tensor([-0.0796,  0.4503])\n",
      "Epoch 1113, Loss 3.541124\n",
      "    Params: tensor([  4.9010, -14.6628])\n",
      "    Grad:   tensor([-0.0794,  0.4496])\n",
      "Epoch 1114, Loss 3.539041\n",
      "    Params: tensor([  4.9018, -14.6673])\n",
      "    Grad:   tensor([-0.0793,  0.4488])\n",
      "Epoch 1115, Loss 3.536967\n",
      "    Params: tensor([  4.9026, -14.6717])\n",
      "    Grad:   tensor([-0.0791,  0.4481])\n",
      "Epoch 1116, Loss 3.534896\n",
      "    Params: tensor([  4.9034, -14.6762])\n",
      "    Grad:   tensor([-0.0790,  0.4473])\n",
      "Epoch 1117, Loss 3.532835\n",
      "    Params: tensor([  4.9042, -14.6807])\n",
      "    Grad:   tensor([-0.0789,  0.4465])\n",
      "Epoch 1118, Loss 3.530781\n",
      "    Params: tensor([  4.9049, -14.6851])\n",
      "    Grad:   tensor([-0.0787,  0.4458])\n",
      "Epoch 1119, Loss 3.528734\n",
      "    Params: tensor([  4.9057, -14.6896])\n",
      "    Grad:   tensor([-0.0786,  0.4450])\n",
      "Epoch 1120, Loss 3.526694\n",
      "    Params: tensor([  4.9065, -14.6940])\n",
      "    Grad:   tensor([-0.0785,  0.4443])\n",
      "Epoch 1121, Loss 3.524662\n",
      "    Params: tensor([  4.9073, -14.6985])\n",
      "    Grad:   tensor([-0.0784,  0.4435])\n",
      "Epoch 1122, Loss 3.522633\n",
      "    Params: tensor([  4.9081, -14.7029])\n",
      "    Grad:   tensor([-0.0782,  0.4428])\n",
      "Epoch 1123, Loss 3.520614\n",
      "    Params: tensor([  4.9089, -14.7073])\n",
      "    Grad:   tensor([-0.0781,  0.4420])\n",
      "Epoch 1124, Loss 3.518601\n",
      "    Params: tensor([  4.9096, -14.7117])\n",
      "    Grad:   tensor([-0.0779,  0.4413])\n",
      "Epoch 1125, Loss 3.516594\n",
      "    Params: tensor([  4.9104, -14.7161])\n",
      "    Grad:   tensor([-0.0778,  0.4405])\n",
      "Epoch 1126, Loss 3.514594\n",
      "    Params: tensor([  4.9112, -14.7205])\n",
      "    Grad:   tensor([-0.0777,  0.4398])\n",
      "Epoch 1127, Loss 3.512602\n",
      "    Params: tensor([  4.9120, -14.7249])\n",
      "    Grad:   tensor([-0.0775,  0.4390])\n",
      "Epoch 1128, Loss 3.510619\n",
      "    Params: tensor([  4.9128, -14.7293])\n",
      "    Grad:   tensor([-0.0774,  0.4383])\n",
      "Epoch 1129, Loss 3.508637\n",
      "    Params: tensor([  4.9135, -14.7337])\n",
      "    Grad:   tensor([-0.0773,  0.4375])\n",
      "Epoch 1130, Loss 3.506665\n",
      "    Params: tensor([  4.9143, -14.7380])\n",
      "    Grad:   tensor([-0.0772,  0.4368])\n",
      "Epoch 1131, Loss 3.504699\n",
      "    Params: tensor([  4.9151, -14.7424])\n",
      "    Grad:   tensor([-0.0770,  0.4360])\n",
      "Epoch 1132, Loss 3.502741\n",
      "    Params: tensor([  4.9158, -14.7467])\n",
      "    Grad:   tensor([-0.0769,  0.4353])\n",
      "Epoch 1133, Loss 3.500789\n",
      "    Params: tensor([  4.9166, -14.7511])\n",
      "    Grad:   tensor([-0.0767,  0.4346])\n",
      "Epoch 1134, Loss 3.498843\n",
      "    Params: tensor([  4.9174, -14.7554])\n",
      "    Grad:   tensor([-0.0766,  0.4338])\n",
      "Epoch 1135, Loss 3.496905\n",
      "    Params: tensor([  4.9181, -14.7598])\n",
      "    Grad:   tensor([-0.0765,  0.4331])\n",
      "Epoch 1136, Loss 3.494972\n",
      "    Params: tensor([  4.9189, -14.7641])\n",
      "    Grad:   tensor([-0.0764,  0.4323])\n",
      "Epoch 1137, Loss 3.493046\n",
      "    Params: tensor([  4.9197, -14.7684])\n",
      "    Grad:   tensor([-0.0763,  0.4316])\n",
      "Epoch 1138, Loss 3.491127\n",
      "    Params: tensor([  4.9204, -14.7727])\n",
      "    Grad:   tensor([-0.0761,  0.4309])\n",
      "Epoch 1139, Loss 3.489214\n",
      "    Params: tensor([  4.9212, -14.7770])\n",
      "    Grad:   tensor([-0.0760,  0.4301])\n",
      "Epoch 1140, Loss 3.487308\n",
      "    Params: tensor([  4.9219, -14.7813])\n",
      "    Grad:   tensor([-0.0759,  0.4294])\n",
      "Epoch 1141, Loss 3.485410\n",
      "    Params: tensor([  4.9227, -14.7856])\n",
      "    Grad:   tensor([-0.0757,  0.4287])\n",
      "Epoch 1142, Loss 3.483515\n",
      "    Params: tensor([  4.9235, -14.7899])\n",
      "    Grad:   tensor([-0.0756,  0.4280])\n",
      "Epoch 1143, Loss 3.481627\n",
      "    Params: tensor([  4.9242, -14.7941])\n",
      "    Grad:   tensor([-0.0755,  0.4272])\n",
      "Epoch 1144, Loss 3.479746\n",
      "    Params: tensor([  4.9250, -14.7984])\n",
      "    Grad:   tensor([-0.0753,  0.4265])\n",
      "Epoch 1145, Loss 3.477872\n",
      "    Params: tensor([  4.9257, -14.8027])\n",
      "    Grad:   tensor([-0.0752,  0.4258])\n",
      "Epoch 1146, Loss 3.476005\n",
      "    Params: tensor([  4.9265, -14.8069])\n",
      "    Grad:   tensor([-0.0751,  0.4250])\n",
      "Epoch 1147, Loss 3.474143\n",
      "    Params: tensor([  4.9272, -14.8112])\n",
      "    Grad:   tensor([-0.0750,  0.4243])\n",
      "Epoch 1148, Loss 3.472288\n",
      "    Params: tensor([  4.9280, -14.8154])\n",
      "    Grad:   tensor([-0.0748,  0.4236])\n",
      "Epoch 1149, Loss 3.470441\n",
      "    Params: tensor([  4.9287, -14.8196])\n",
      "    Grad:   tensor([-0.0747,  0.4229])\n",
      "Epoch 1150, Loss 3.468597\n",
      "    Params: tensor([  4.9295, -14.8238])\n",
      "    Grad:   tensor([-0.0746,  0.4222])\n",
      "Epoch 1151, Loss 3.466762\n",
      "    Params: tensor([  4.9302, -14.8281])\n",
      "    Grad:   tensor([-0.0745,  0.4215])\n",
      "Epoch 1152, Loss 3.464930\n",
      "    Params: tensor([  4.9309, -14.8323])\n",
      "    Grad:   tensor([-0.0743,  0.4207])\n",
      "Epoch 1153, Loss 3.463105\n",
      "    Params: tensor([  4.9317, -14.8365])\n",
      "    Grad:   tensor([-0.0742,  0.4200])\n",
      "Epoch 1154, Loss 3.461290\n",
      "    Params: tensor([  4.9324, -14.8407])\n",
      "    Grad:   tensor([-0.0741,  0.4193])\n",
      "Epoch 1155, Loss 3.459477\n",
      "    Params: tensor([  4.9332, -14.8448])\n",
      "    Grad:   tensor([-0.0739,  0.4186])\n",
      "Epoch 1156, Loss 3.457671\n",
      "    Params: tensor([  4.9339, -14.8490])\n",
      "    Grad:   tensor([-0.0738,  0.4179])\n",
      "Epoch 1157, Loss 3.455873\n",
      "    Params: tensor([  4.9346, -14.8532])\n",
      "    Grad:   tensor([-0.0737,  0.4172])\n",
      "Epoch 1158, Loss 3.454080\n",
      "    Params: tensor([  4.9354, -14.8574])\n",
      "    Grad:   tensor([-0.0736,  0.4165])\n",
      "Epoch 1159, Loss 3.452293\n",
      "    Params: tensor([  4.9361, -14.8615])\n",
      "    Grad:   tensor([-0.0734,  0.4158])\n",
      "Epoch 1160, Loss 3.450513\n",
      "    Params: tensor([  4.9368, -14.8657])\n",
      "    Grad:   tensor([-0.0733,  0.4151])\n",
      "Epoch 1161, Loss 3.448736\n",
      "    Params: tensor([  4.9376, -14.8698])\n",
      "    Grad:   tensor([-0.0732,  0.4143])\n",
      "Epoch 1162, Loss 3.446968\n",
      "    Params: tensor([  4.9383, -14.8739])\n",
      "    Grad:   tensor([-0.0731,  0.4136])\n",
      "Epoch 1163, Loss 3.445203\n",
      "    Params: tensor([  4.9390, -14.8781])\n",
      "    Grad:   tensor([-0.0730,  0.4129])\n",
      "Epoch 1164, Loss 3.443449\n",
      "    Params: tensor([  4.9398, -14.8822])\n",
      "    Grad:   tensor([-0.0728,  0.4122])\n",
      "Epoch 1165, Loss 3.441697\n",
      "    Params: tensor([  4.9405, -14.8863])\n",
      "    Grad:   tensor([-0.0727,  0.4115])\n",
      "Epoch 1166, Loss 3.439952\n",
      "    Params: tensor([  4.9412, -14.8904])\n",
      "    Grad:   tensor([-0.0726,  0.4108])\n",
      "Epoch 1167, Loss 3.438210\n",
      "    Params: tensor([  4.9419, -14.8945])\n",
      "    Grad:   tensor([-0.0725,  0.4101])\n",
      "Epoch 1168, Loss 3.436479\n",
      "    Params: tensor([  4.9427, -14.8986])\n",
      "    Grad:   tensor([-0.0723,  0.4094])\n",
      "Epoch 1169, Loss 3.434753\n",
      "    Params: tensor([  4.9434, -14.9027])\n",
      "    Grad:   tensor([-0.0722,  0.4087])\n",
      "Epoch 1170, Loss 3.433030\n",
      "    Params: tensor([  4.9441, -14.9068])\n",
      "    Grad:   tensor([-0.0721,  0.4081])\n",
      "Epoch 1171, Loss 3.431314\n",
      "    Params: tensor([  4.9448, -14.9109])\n",
      "    Grad:   tensor([-0.0720,  0.4074])\n",
      "Epoch 1172, Loss 3.429607\n",
      "    Params: tensor([  4.9455, -14.9149])\n",
      "    Grad:   tensor([-0.0719,  0.4067])\n",
      "Epoch 1173, Loss 3.427903\n",
      "    Params: tensor([  4.9463, -14.9190])\n",
      "    Grad:   tensor([-0.0717,  0.4060])\n",
      "Epoch 1174, Loss 3.426204\n",
      "    Params: tensor([  4.9470, -14.9230])\n",
      "    Grad:   tensor([-0.0716,  0.4053])\n",
      "Epoch 1175, Loss 3.424510\n",
      "    Params: tensor([  4.9477, -14.9271])\n",
      "    Grad:   tensor([-0.0715,  0.4046])\n",
      "Epoch 1176, Loss 3.422823\n",
      "    Params: tensor([  4.9484, -14.9311])\n",
      "    Grad:   tensor([-0.0714,  0.4039])\n",
      "Epoch 1177, Loss 3.421144\n",
      "    Params: tensor([  4.9491, -14.9352])\n",
      "    Grad:   tensor([-0.0712,  0.4032])\n",
      "Epoch 1178, Loss 3.419468\n",
      "    Params: tensor([  4.9498, -14.9392])\n",
      "    Grad:   tensor([-0.0711,  0.4025])\n",
      "Epoch 1179, Loss 3.417798\n",
      "    Params: tensor([  4.9505, -14.9432])\n",
      "    Grad:   tensor([-0.0710,  0.4019])\n",
      "Epoch 1180, Loss 3.416134\n",
      "    Params: tensor([  4.9512, -14.9472])\n",
      "    Grad:   tensor([-0.0709,  0.4012])\n",
      "Epoch 1181, Loss 3.414476\n",
      "    Params: tensor([  4.9520, -14.9512])\n",
      "    Grad:   tensor([-0.0708,  0.4005])\n",
      "Epoch 1182, Loss 3.412824\n",
      "    Params: tensor([  4.9527, -14.9552])\n",
      "    Grad:   tensor([-0.0706,  0.3998])\n",
      "Epoch 1183, Loss 3.411176\n",
      "    Params: tensor([  4.9534, -14.9592])\n",
      "    Grad:   tensor([-0.0705,  0.3991])\n",
      "Epoch 1184, Loss 3.409534\n",
      "    Params: tensor([  4.9541, -14.9632])\n",
      "    Grad:   tensor([-0.0704,  0.3985])\n",
      "Epoch 1185, Loss 3.407900\n",
      "    Params: tensor([  4.9548, -14.9672])\n",
      "    Grad:   tensor([-0.0703,  0.3978])\n",
      "Epoch 1186, Loss 3.406271\n",
      "    Params: tensor([  4.9555, -14.9711])\n",
      "    Grad:   tensor([-0.0701,  0.3971])\n",
      "Epoch 1187, Loss 3.404645\n",
      "    Params: tensor([  4.9562, -14.9751])\n",
      "    Grad:   tensor([-0.0700,  0.3964])\n",
      "Epoch 1188, Loss 3.403024\n",
      "    Params: tensor([  4.9569, -14.9791])\n",
      "    Grad:   tensor([-0.0699,  0.3958])\n",
      "Epoch 1189, Loss 3.401413\n",
      "    Params: tensor([  4.9576, -14.9830])\n",
      "    Grad:   tensor([-0.0698,  0.3951])\n",
      "Epoch 1190, Loss 3.399802\n",
      "    Params: tensor([  4.9583, -14.9870])\n",
      "    Grad:   tensor([-0.0697,  0.3944])\n",
      "Epoch 1191, Loss 3.398200\n",
      "    Params: tensor([  4.9590, -14.9909])\n",
      "    Grad:   tensor([-0.0696,  0.3937])\n",
      "Epoch 1192, Loss 3.396602\n",
      "    Params: tensor([  4.9597, -14.9948])\n",
      "    Grad:   tensor([-0.0694,  0.3931])\n",
      "Epoch 1193, Loss 3.395011\n",
      "    Params: tensor([  4.9604, -14.9988])\n",
      "    Grad:   tensor([-0.0693,  0.3924])\n",
      "Epoch 1194, Loss 3.393425\n",
      "    Params: tensor([  4.9610, -15.0027])\n",
      "    Grad:   tensor([-0.0692,  0.3917])\n",
      "Epoch 1195, Loss 3.391844\n",
      "    Params: tensor([  4.9617, -15.0066])\n",
      "    Grad:   tensor([-0.0691,  0.3911])\n",
      "Epoch 1196, Loss 3.390266\n",
      "    Params: tensor([  4.9624, -15.0105])\n",
      "    Grad:   tensor([-0.0690,  0.3904])\n",
      "Epoch 1197, Loss 3.388697\n",
      "    Params: tensor([  4.9631, -15.0144])\n",
      "    Grad:   tensor([-0.0689,  0.3897])\n",
      "Epoch 1198, Loss 3.387131\n",
      "    Params: tensor([  4.9638, -15.0183])\n",
      "    Grad:   tensor([-0.0687,  0.3891])\n",
      "Epoch 1199, Loss 3.385571\n",
      "    Params: tensor([  4.9645, -15.0222])\n",
      "    Grad:   tensor([-0.0686,  0.3884])\n",
      "Epoch 1200, Loss 3.384018\n",
      "    Params: tensor([  4.9652, -15.0260])\n",
      "    Grad:   tensor([-0.0685,  0.3878])\n",
      "Epoch 1201, Loss 3.382467\n",
      "    Params: tensor([  4.9659, -15.0299])\n",
      "    Grad:   tensor([-0.0684,  0.3871])\n",
      "Epoch 1202, Loss 3.380925\n",
      "    Params: tensor([  4.9665, -15.0338])\n",
      "    Grad:   tensor([-0.0683,  0.3864])\n",
      "Epoch 1203, Loss 3.379385\n",
      "    Params: tensor([  4.9672, -15.0376])\n",
      "    Grad:   tensor([-0.0681,  0.3858])\n",
      "Epoch 1204, Loss 3.377851\n",
      "    Params: tensor([  4.9679, -15.0415])\n",
      "    Grad:   tensor([-0.0680,  0.3851])\n",
      "Epoch 1205, Loss 3.376323\n",
      "    Params: tensor([  4.9686, -15.0453])\n",
      "    Grad:   tensor([-0.0679,  0.3845])\n",
      "Epoch 1206, Loss 3.374800\n",
      "    Params: tensor([  4.9693, -15.0492])\n",
      "    Grad:   tensor([-0.0678,  0.3838])\n",
      "Epoch 1207, Loss 3.373284\n",
      "    Params: tensor([  4.9699, -15.0530])\n",
      "    Grad:   tensor([-0.0677,  0.3832])\n",
      "Epoch 1208, Loss 3.371769\n",
      "    Params: tensor([  4.9706, -15.0568])\n",
      "    Grad:   tensor([-0.0676,  0.3825])\n",
      "Epoch 1209, Loss 3.370261\n",
      "    Params: tensor([  4.9713, -15.0606])\n",
      "    Grad:   tensor([-0.0675,  0.3819])\n",
      "Epoch 1210, Loss 3.368760\n",
      "    Params: tensor([  4.9720, -15.0645])\n",
      "    Grad:   tensor([-0.0673,  0.3812])\n",
      "Epoch 1211, Loss 3.367262\n",
      "    Params: tensor([  4.9726, -15.0683])\n",
      "    Grad:   tensor([-0.0672,  0.3806])\n",
      "Epoch 1212, Loss 3.365771\n",
      "    Params: tensor([  4.9733, -15.0721])\n",
      "    Grad:   tensor([-0.0671,  0.3799])\n",
      "Epoch 1213, Loss 3.364282\n",
      "    Params: tensor([  4.9740, -15.0758])\n",
      "    Grad:   tensor([-0.0670,  0.3793])\n",
      "Epoch 1214, Loss 3.362800\n",
      "    Params: tensor([  4.9746, -15.0796])\n",
      "    Grad:   tensor([-0.0669,  0.3786])\n",
      "Epoch 1215, Loss 3.361324\n",
      "    Params: tensor([  4.9753, -15.0834])\n",
      "    Grad:   tensor([-0.0668,  0.3780])\n",
      "Epoch 1216, Loss 3.359850\n",
      "    Params: tensor([  4.9760, -15.0872])\n",
      "    Grad:   tensor([-0.0667,  0.3774])\n",
      "Epoch 1217, Loss 3.358384\n",
      "    Params: tensor([  4.9766, -15.0910])\n",
      "    Grad:   tensor([-0.0665,  0.3767])\n",
      "Epoch 1218, Loss 3.356921\n",
      "    Params: tensor([  4.9773, -15.0947])\n",
      "    Grad:   tensor([-0.0664,  0.3761])\n",
      "Epoch 1219, Loss 3.355464\n",
      "    Params: tensor([  4.9780, -15.0985])\n",
      "    Grad:   tensor([-0.0663,  0.3754])\n",
      "Epoch 1220, Loss 3.354012\n",
      "    Params: tensor([  4.9786, -15.1022])\n",
      "    Grad:   tensor([-0.0662,  0.3748])\n",
      "Epoch 1221, Loss 3.352564\n",
      "    Params: tensor([  4.9793, -15.1060])\n",
      "    Grad:   tensor([-0.0661,  0.3742])\n",
      "Epoch 1222, Loss 3.351122\n",
      "    Params: tensor([  4.9799, -15.1097])\n",
      "    Grad:   tensor([-0.0660,  0.3735])\n",
      "Epoch 1223, Loss 3.349685\n",
      "    Params: tensor([  4.9806, -15.1134])\n",
      "    Grad:   tensor([-0.0659,  0.3729])\n",
      "Epoch 1224, Loss 3.348251\n",
      "    Params: tensor([  4.9813, -15.1171])\n",
      "    Grad:   tensor([-0.0657,  0.3723])\n",
      "Epoch 1225, Loss 3.346824\n",
      "    Params: tensor([  4.9819, -15.1209])\n",
      "    Grad:   tensor([-0.0656,  0.3716])\n",
      "Epoch 1226, Loss 3.345403\n",
      "    Params: tensor([  4.9826, -15.1246])\n",
      "    Grad:   tensor([-0.0655,  0.3710])\n",
      "Epoch 1227, Loss 3.343982\n",
      "    Params: tensor([  4.9832, -15.1283])\n",
      "    Grad:   tensor([-0.0654,  0.3704])\n",
      "Epoch 1228, Loss 3.342571\n",
      "    Params: tensor([  4.9839, -15.1320])\n",
      "    Grad:   tensor([-0.0653,  0.3697])\n",
      "Epoch 1229, Loss 3.341160\n",
      "    Params: tensor([  4.9845, -15.1357])\n",
      "    Grad:   tensor([-0.0652,  0.3691])\n",
      "Epoch 1230, Loss 3.339758\n",
      "    Params: tensor([  4.9852, -15.1393])\n",
      "    Grad:   tensor([-0.0651,  0.3685])\n",
      "Epoch 1231, Loss 3.338359\n",
      "    Params: tensor([  4.9858, -15.1430])\n",
      "    Grad:   tensor([-0.0650,  0.3679])\n",
      "Epoch 1232, Loss 3.336965\n",
      "    Params: tensor([  4.9865, -15.1467])\n",
      "    Grad:   tensor([-0.0649,  0.3672])\n",
      "Epoch 1233, Loss 3.335577\n",
      "    Params: tensor([  4.9871, -15.1504])\n",
      "    Grad:   tensor([-0.0648,  0.3666])\n",
      "Epoch 1234, Loss 3.334192\n",
      "    Params: tensor([  4.9878, -15.1540])\n",
      "    Grad:   tensor([-0.0646,  0.3660])\n",
      "Epoch 1235, Loss 3.332811\n",
      "    Params: tensor([  4.9884, -15.1577])\n",
      "    Grad:   tensor([-0.0645,  0.3654])\n",
      "Epoch 1236, Loss 3.331436\n",
      "    Params: tensor([  4.9891, -15.1613])\n",
      "    Grad:   tensor([-0.0644,  0.3647])\n",
      "Epoch 1237, Loss 3.330065\n",
      "    Params: tensor([  4.9897, -15.1650])\n",
      "    Grad:   tensor([-0.0643,  0.3641])\n",
      "Epoch 1238, Loss 3.328699\n",
      "    Params: tensor([  4.9904, -15.1686])\n",
      "    Grad:   tensor([-0.0642,  0.3635])\n",
      "Epoch 1239, Loss 3.327339\n",
      "    Params: tensor([  4.9910, -15.1722])\n",
      "    Grad:   tensor([-0.0641,  0.3629])\n",
      "Epoch 1240, Loss 3.325980\n",
      "    Params: tensor([  4.9916, -15.1759])\n",
      "    Grad:   tensor([-0.0640,  0.3623])\n",
      "Epoch 1241, Loss 3.324628\n",
      "    Params: tensor([  4.9923, -15.1795])\n",
      "    Grad:   tensor([-0.0639,  0.3617])\n",
      "Epoch 1242, Loss 3.323279\n",
      "    Params: tensor([  4.9929, -15.1831])\n",
      "    Grad:   tensor([-0.0638,  0.3610])\n",
      "Epoch 1243, Loss 3.321935\n",
      "    Params: tensor([  4.9936, -15.1867])\n",
      "    Grad:   tensor([-0.0637,  0.3604])\n",
      "Epoch 1244, Loss 3.320600\n",
      "    Params: tensor([  4.9942, -15.1903])\n",
      "    Grad:   tensor([-0.0636,  0.3598])\n",
      "Epoch 1245, Loss 3.319264\n",
      "    Params: tensor([  4.9948, -15.1939])\n",
      "    Grad:   tensor([-0.0635,  0.3592])\n",
      "Epoch 1246, Loss 3.317935\n",
      "    Params: tensor([  4.9955, -15.1975])\n",
      "    Grad:   tensor([-0.0633,  0.3586])\n",
      "Epoch 1247, Loss 3.316611\n",
      "    Params: tensor([  4.9961, -15.2010])\n",
      "    Grad:   tensor([-0.0633,  0.3580])\n",
      "Epoch 1248, Loss 3.315289\n",
      "    Params: tensor([  4.9967, -15.2046])\n",
      "    Grad:   tensor([-0.0631,  0.3574])\n",
      "Epoch 1249, Loss 3.313973\n",
      "    Params: tensor([  4.9973, -15.2082])\n",
      "    Grad:   tensor([-0.0630,  0.3568])\n",
      "Epoch 1250, Loss 3.312663\n",
      "    Params: tensor([  4.9980, -15.2117])\n",
      "    Grad:   tensor([-0.0629,  0.3562])\n",
      "Epoch 1251, Loss 3.311353\n",
      "    Params: tensor([  4.9986, -15.2153])\n",
      "    Grad:   tensor([-0.0628,  0.3556])\n",
      "Epoch 1252, Loss 3.310053\n",
      "    Params: tensor([  4.9992, -15.2189])\n",
      "    Grad:   tensor([-0.0627,  0.3550])\n",
      "Epoch 1253, Loss 3.308756\n",
      "    Params: tensor([  4.9999, -15.2224])\n",
      "    Grad:   tensor([-0.0626,  0.3543])\n",
      "Epoch 1254, Loss 3.307463\n",
      "    Params: tensor([  5.0005, -15.2259])\n",
      "    Grad:   tensor([-0.0625,  0.3537])\n",
      "Epoch 1255, Loss 3.306170\n",
      "    Params: tensor([  5.0011, -15.2295])\n",
      "    Grad:   tensor([-0.0624,  0.3531])\n",
      "Epoch 1256, Loss 3.304887\n",
      "    Params: tensor([  5.0017, -15.2330])\n",
      "    Grad:   tensor([-0.0623,  0.3525])\n",
      "Epoch 1257, Loss 3.303605\n",
      "    Params: tensor([  5.0024, -15.2365])\n",
      "    Grad:   tensor([-0.0622,  0.3519])\n",
      "Epoch 1258, Loss 3.302329\n",
      "    Params: tensor([  5.0030, -15.2400])\n",
      "    Grad:   tensor([-0.0620,  0.3514])\n",
      "Epoch 1259, Loss 3.301057\n",
      "    Params: tensor([  5.0036, -15.2435])\n",
      "    Grad:   tensor([-0.0620,  0.3508])\n",
      "Epoch 1260, Loss 3.299791\n",
      "    Params: tensor([  5.0042, -15.2470])\n",
      "    Grad:   tensor([-0.0619,  0.3502])\n",
      "Epoch 1261, Loss 3.298528\n",
      "    Params: tensor([  5.0048, -15.2505])\n",
      "    Grad:   tensor([-0.0618,  0.3496])\n",
      "Epoch 1262, Loss 3.297267\n",
      "    Params: tensor([  5.0054, -15.2540])\n",
      "    Grad:   tensor([-0.0616,  0.3490])\n",
      "Epoch 1263, Loss 3.296014\n",
      "    Params: tensor([  5.0061, -15.2575])\n",
      "    Grad:   tensor([-0.0615,  0.3484])\n",
      "Epoch 1264, Loss 3.294762\n",
      "    Params: tensor([  5.0067, -15.2610])\n",
      "    Grad:   tensor([-0.0614,  0.3478])\n",
      "Epoch 1265, Loss 3.293517\n",
      "    Params: tensor([  5.0073, -15.2645])\n",
      "    Grad:   tensor([-0.0613,  0.3472])\n",
      "Epoch 1266, Loss 3.292276\n",
      "    Params: tensor([  5.0079, -15.2679])\n",
      "    Grad:   tensor([-0.0612,  0.3466])\n",
      "Epoch 1267, Loss 3.291036\n",
      "    Params: tensor([  5.0085, -15.2714])\n",
      "    Grad:   tensor([-0.0611,  0.3460])\n",
      "Epoch 1268, Loss 3.289804\n",
      "    Params: tensor([  5.0091, -15.2748])\n",
      "    Grad:   tensor([-0.0610,  0.3454])\n",
      "Epoch 1269, Loss 3.288573\n",
      "    Params: tensor([  5.0097, -15.2783])\n",
      "    Grad:   tensor([-0.0609,  0.3448])\n",
      "Epoch 1270, Loss 3.287347\n",
      "    Params: tensor([  5.0103, -15.2817])\n",
      "    Grad:   tensor([-0.0608,  0.3443])\n",
      "Epoch 1271, Loss 3.286129\n",
      "    Params: tensor([  5.0109, -15.2852])\n",
      "    Grad:   tensor([-0.0607,  0.3437])\n",
      "Epoch 1272, Loss 3.284911\n",
      "    Params: tensor([  5.0116, -15.2886])\n",
      "    Grad:   tensor([-0.0606,  0.3431])\n",
      "Epoch 1273, Loss 3.283698\n",
      "    Params: tensor([  5.0122, -15.2920])\n",
      "    Grad:   tensor([-0.0605,  0.3425])\n",
      "Epoch 1274, Loss 3.282488\n",
      "    Params: tensor([  5.0128, -15.2954])\n",
      "    Grad:   tensor([-0.0604,  0.3419])\n",
      "Epoch 1275, Loss 3.281284\n",
      "    Params: tensor([  5.0134, -15.2988])\n",
      "    Grad:   tensor([-0.0603,  0.3413])\n",
      "Epoch 1276, Loss 3.280085\n",
      "    Params: tensor([  5.0140, -15.3023])\n",
      "    Grad:   tensor([-0.0602,  0.3408])\n",
      "Epoch 1277, Loss 3.278888\n",
      "    Params: tensor([  5.0146, -15.3057])\n",
      "    Grad:   tensor([-0.0601,  0.3402])\n",
      "Epoch 1278, Loss 3.277696\n",
      "    Params: tensor([  5.0152, -15.3091])\n",
      "    Grad:   tensor([-0.0600,  0.3396])\n",
      "Epoch 1279, Loss 3.276506\n",
      "    Params: tensor([  5.0158, -15.3124])\n",
      "    Grad:   tensor([-0.0599,  0.3390])\n",
      "Epoch 1280, Loss 3.275322\n",
      "    Params: tensor([  5.0164, -15.3158])\n",
      "    Grad:   tensor([-0.0598,  0.3384])\n",
      "Epoch 1281, Loss 3.274142\n",
      "    Params: tensor([  5.0170, -15.3192])\n",
      "    Grad:   tensor([-0.0597,  0.3379])\n",
      "Epoch 1282, Loss 3.272968\n",
      "    Params: tensor([  5.0176, -15.3226])\n",
      "    Grad:   tensor([-0.0596,  0.3373])\n",
      "Epoch 1283, Loss 3.271793\n",
      "    Params: tensor([  5.0182, -15.3259])\n",
      "    Grad:   tensor([-0.0595,  0.3367])\n",
      "Epoch 1284, Loss 3.270625\n",
      "    Params: tensor([  5.0187, -15.3293])\n",
      "    Grad:   tensor([-0.0594,  0.3362])\n",
      "Epoch 1285, Loss 3.269460\n",
      "    Params: tensor([  5.0193, -15.3327])\n",
      "    Grad:   tensor([-0.0593,  0.3356])\n",
      "Epoch 1286, Loss 3.268301\n",
      "    Params: tensor([  5.0199, -15.3360])\n",
      "    Grad:   tensor([-0.0592,  0.3350])\n",
      "Epoch 1287, Loss 3.267143\n",
      "    Params: tensor([  5.0205, -15.3394])\n",
      "    Grad:   tensor([-0.0591,  0.3344])\n",
      "Epoch 1288, Loss 3.265991\n",
      "    Params: tensor([  5.0211, -15.3427])\n",
      "    Grad:   tensor([-0.0590,  0.3339])\n",
      "Epoch 1289, Loss 3.264842\n",
      "    Params: tensor([  5.0217, -15.3460])\n",
      "    Grad:   tensor([-0.0589,  0.3333])\n",
      "Epoch 1290, Loss 3.263700\n",
      "    Params: tensor([  5.0223, -15.3494])\n",
      "    Grad:   tensor([-0.0588,  0.3327])\n",
      "Epoch 1291, Loss 3.262556\n",
      "    Params: tensor([  5.0229, -15.3527])\n",
      "    Grad:   tensor([-0.0587,  0.3322])\n",
      "Epoch 1292, Loss 3.261421\n",
      "    Params: tensor([  5.0235, -15.3560])\n",
      "    Grad:   tensor([-0.0586,  0.3316])\n",
      "Epoch 1293, Loss 3.260287\n",
      "    Params: tensor([  5.0240, -15.3593])\n",
      "    Grad:   tensor([-0.0585,  0.3311])\n",
      "Epoch 1294, Loss 3.259160\n",
      "    Params: tensor([  5.0246, -15.3626])\n",
      "    Grad:   tensor([-0.0584,  0.3305])\n",
      "Epoch 1295, Loss 3.258033\n",
      "    Params: tensor([  5.0252, -15.3659])\n",
      "    Grad:   tensor([-0.0583,  0.3299])\n",
      "Epoch 1296, Loss 3.256912\n",
      "    Params: tensor([  5.0258, -15.3692])\n",
      "    Grad:   tensor([-0.0582,  0.3294])\n",
      "Epoch 1297, Loss 3.255795\n",
      "    Params: tensor([  5.0264, -15.3725])\n",
      "    Grad:   tensor([-0.0581,  0.3288])\n",
      "Epoch 1298, Loss 3.254681\n",
      "    Params: tensor([  5.0270, -15.3758])\n",
      "    Grad:   tensor([-0.0580,  0.3282])\n",
      "Epoch 1299, Loss 3.253569\n",
      "    Params: tensor([  5.0275, -15.3791])\n",
      "    Grad:   tensor([-0.0579,  0.3277])\n",
      "Epoch 1300, Loss 3.252462\n",
      "    Params: tensor([  5.0281, -15.3823])\n",
      "    Grad:   tensor([-0.0578,  0.3271])\n",
      "Epoch 1301, Loss 3.251362\n",
      "    Params: tensor([  5.0287, -15.3856])\n",
      "    Grad:   tensor([-0.0577,  0.3266])\n",
      "Epoch 1302, Loss 3.250263\n",
      "    Params: tensor([  5.0293, -15.3888])\n",
      "    Grad:   tensor([-0.0576,  0.3260])\n",
      "Epoch 1303, Loss 3.249168\n",
      "    Params: tensor([  5.0298, -15.3921])\n",
      "    Grad:   tensor([-0.0575,  0.3255])\n",
      "Epoch 1304, Loss 3.248077\n",
      "    Params: tensor([  5.0304, -15.3954])\n",
      "    Grad:   tensor([-0.0574,  0.3249])\n",
      "Epoch 1305, Loss 3.246988\n",
      "    Params: tensor([  5.0310, -15.3986])\n",
      "    Grad:   tensor([-0.0573,  0.3244])\n",
      "Epoch 1306, Loss 3.245904\n",
      "    Params: tensor([  5.0316, -15.4018])\n",
      "    Grad:   tensor([-0.0572,  0.3238])\n",
      "Epoch 1307, Loss 3.244824\n",
      "    Params: tensor([  5.0321, -15.4051])\n",
      "    Grad:   tensor([-0.0571,  0.3233])\n",
      "Epoch 1308, Loss 3.243747\n",
      "    Params: tensor([  5.0327, -15.4083])\n",
      "    Grad:   tensor([-0.0570,  0.3227])\n",
      "Epoch 1309, Loss 3.242674\n",
      "    Params: tensor([  5.0333, -15.4115])\n",
      "    Grad:   tensor([-0.0569,  0.3222])\n",
      "Epoch 1310, Loss 3.241606\n",
      "    Params: tensor([  5.0338, -15.4147])\n",
      "    Grad:   tensor([-0.0568,  0.3216])\n",
      "Epoch 1311, Loss 3.240538\n",
      "    Params: tensor([  5.0344, -15.4179])\n",
      "    Grad:   tensor([-0.0567,  0.3211])\n",
      "Epoch 1312, Loss 3.239475\n",
      "    Params: tensor([  5.0350, -15.4211])\n",
      "    Grad:   tensor([-0.0566,  0.3205])\n",
      "Epoch 1313, Loss 3.238419\n",
      "    Params: tensor([  5.0355, -15.4243])\n",
      "    Grad:   tensor([-0.0565,  0.3200])\n",
      "Epoch 1314, Loss 3.237363\n",
      "    Params: tensor([  5.0361, -15.4275])\n",
      "    Grad:   tensor([-0.0564,  0.3194])\n",
      "Epoch 1315, Loss 3.236314\n",
      "    Params: tensor([  5.0367, -15.4307])\n",
      "    Grad:   tensor([-0.0563,  0.3189])\n",
      "Epoch 1316, Loss 3.235265\n",
      "    Params: tensor([  5.0372, -15.4339])\n",
      "    Grad:   tensor([-0.0562,  0.3184])\n",
      "Epoch 1317, Loss 3.234218\n",
      "    Params: tensor([  5.0378, -15.4371])\n",
      "    Grad:   tensor([-0.0561,  0.3178])\n",
      "Epoch 1318, Loss 3.233179\n",
      "    Params: tensor([  5.0383, -15.4403])\n",
      "    Grad:   tensor([-0.0561,  0.3173])\n",
      "Epoch 1319, Loss 3.232143\n",
      "    Params: tensor([  5.0389, -15.4434])\n",
      "    Grad:   tensor([-0.0560,  0.3167])\n",
      "Epoch 1320, Loss 3.231109\n",
      "    Params: tensor([  5.0395, -15.4466])\n",
      "    Grad:   tensor([-0.0558,  0.3162])\n",
      "Epoch 1321, Loss 3.230078\n",
      "    Params: tensor([  5.0400, -15.4498])\n",
      "    Grad:   tensor([-0.0558,  0.3157])\n",
      "Epoch 1322, Loss 3.229051\n",
      "    Params: tensor([  5.0406, -15.4529])\n",
      "    Grad:   tensor([-0.0557,  0.3151])\n",
      "Epoch 1323, Loss 3.228027\n",
      "    Params: tensor([  5.0411, -15.4560])\n",
      "    Grad:   tensor([-0.0556,  0.3146])\n",
      "Epoch 1324, Loss 3.227010\n",
      "    Params: tensor([  5.0417, -15.4592])\n",
      "    Grad:   tensor([-0.0555,  0.3141])\n",
      "Epoch 1325, Loss 3.225992\n",
      "    Params: tensor([  5.0422, -15.4623])\n",
      "    Grad:   tensor([-0.0554,  0.3135])\n",
      "Epoch 1326, Loss 3.224979\n",
      "    Params: tensor([  5.0428, -15.4655])\n",
      "    Grad:   tensor([-0.0553,  0.3130])\n",
      "Epoch 1327, Loss 3.223971\n",
      "    Params: tensor([  5.0433, -15.4686])\n",
      "    Grad:   tensor([-0.0552,  0.3125])\n",
      "Epoch 1328, Loss 3.222965\n",
      "    Params: tensor([  5.0439, -15.4717])\n",
      "    Grad:   tensor([-0.0551,  0.3119])\n",
      "Epoch 1329, Loss 3.221960\n",
      "    Params: tensor([  5.0444, -15.4748])\n",
      "    Grad:   tensor([-0.0550,  0.3114])\n",
      "Epoch 1330, Loss 3.220962\n",
      "    Params: tensor([  5.0450, -15.4779])\n",
      "    Grad:   tensor([-0.0549,  0.3109])\n",
      "Epoch 1331, Loss 3.219967\n",
      "    Params: tensor([  5.0455, -15.4810])\n",
      "    Grad:   tensor([-0.0548,  0.3103])\n",
      "Epoch 1332, Loss 3.218975\n",
      "    Params: tensor([  5.0461, -15.4841])\n",
      "    Grad:   tensor([-0.0547,  0.3098])\n",
      "Epoch 1333, Loss 3.217986\n",
      "    Params: tensor([  5.0466, -15.4872])\n",
      "    Grad:   tensor([-0.0546,  0.3093])\n",
      "Epoch 1334, Loss 3.217000\n",
      "    Params: tensor([  5.0472, -15.4903])\n",
      "    Grad:   tensor([-0.0545,  0.3088])\n",
      "Epoch 1335, Loss 3.216017\n",
      "    Params: tensor([  5.0477, -15.4934])\n",
      "    Grad:   tensor([-0.0544,  0.3082])\n",
      "Epoch 1336, Loss 3.215038\n",
      "    Params: tensor([  5.0483, -15.4965])\n",
      "    Grad:   tensor([-0.0543,  0.3077])\n",
      "Epoch 1337, Loss 3.214062\n",
      "    Params: tensor([  5.0488, -15.4995])\n",
      "    Grad:   tensor([-0.0543,  0.3072])\n",
      "Epoch 1338, Loss 3.213092\n",
      "    Params: tensor([  5.0494, -15.5026])\n",
      "    Grad:   tensor([-0.0542,  0.3067])\n",
      "Epoch 1339, Loss 3.212122\n",
      "    Params: tensor([  5.0499, -15.5057])\n",
      "    Grad:   tensor([-0.0541,  0.3061])\n",
      "Epoch 1340, Loss 3.211157\n",
      "    Params: tensor([  5.0504, -15.5087])\n",
      "    Grad:   tensor([-0.0540,  0.3056])\n",
      "Epoch 1341, Loss 3.210192\n",
      "    Params: tensor([  5.0510, -15.5118])\n",
      "    Grad:   tensor([-0.0539,  0.3051])\n",
      "Epoch 1342, Loss 3.209235\n",
      "    Params: tensor([  5.0515, -15.5148])\n",
      "    Grad:   tensor([-0.0538,  0.3046])\n",
      "Epoch 1343, Loss 3.208279\n",
      "    Params: tensor([  5.0521, -15.5179])\n",
      "    Grad:   tensor([-0.0537,  0.3041])\n",
      "Epoch 1344, Loss 3.207326\n",
      "    Params: tensor([  5.0526, -15.5209])\n",
      "    Grad:   tensor([-0.0536,  0.3036])\n",
      "Epoch 1345, Loss 3.206376\n",
      "    Params: tensor([  5.0531, -15.5239])\n",
      "    Grad:   tensor([-0.0535,  0.3030])\n",
      "Epoch 1346, Loss 3.205430\n",
      "    Params: tensor([  5.0537, -15.5269])\n",
      "    Grad:   tensor([-0.0534,  0.3025])\n",
      "Epoch 1347, Loss 3.204488\n",
      "    Params: tensor([  5.0542, -15.5300])\n",
      "    Grad:   tensor([-0.0533,  0.3020])\n",
      "Epoch 1348, Loss 3.203547\n",
      "    Params: tensor([  5.0547, -15.5330])\n",
      "    Grad:   tensor([-0.0532,  0.3015])\n",
      "Epoch 1349, Loss 3.202610\n",
      "    Params: tensor([  5.0553, -15.5360])\n",
      "    Grad:   tensor([-0.0532,  0.3010])\n",
      "Epoch 1350, Loss 3.201678\n",
      "    Params: tensor([  5.0558, -15.5390])\n",
      "    Grad:   tensor([-0.0531,  0.3005])\n",
      "Epoch 1351, Loss 3.200747\n",
      "    Params: tensor([  5.0563, -15.5420])\n",
      "    Grad:   tensor([-0.0530,  0.3000])\n",
      "Epoch 1352, Loss 3.199820\n",
      "    Params: tensor([  5.0568, -15.5450])\n",
      "    Grad:   tensor([-0.0529,  0.2995])\n",
      "Epoch 1353, Loss 3.198897\n",
      "    Params: tensor([  5.0574, -15.5480])\n",
      "    Grad:   tensor([-0.0528,  0.2989])\n",
      "Epoch 1354, Loss 3.197976\n",
      "    Params: tensor([  5.0579, -15.5510])\n",
      "    Grad:   tensor([-0.0527,  0.2984])\n",
      "Epoch 1355, Loss 3.197060\n",
      "    Params: tensor([  5.0584, -15.5539])\n",
      "    Grad:   tensor([-0.0526,  0.2979])\n",
      "Epoch 1356, Loss 3.196143\n",
      "    Params: tensor([  5.0590, -15.5569])\n",
      "    Grad:   tensor([-0.0525,  0.2974])\n",
      "Epoch 1357, Loss 3.195231\n",
      "    Params: tensor([  5.0595, -15.5599])\n",
      "    Grad:   tensor([-0.0524,  0.2969])\n",
      "Epoch 1358, Loss 3.194324\n",
      "    Params: tensor([  5.0600, -15.5629])\n",
      "    Grad:   tensor([-0.0524,  0.2964])\n",
      "Epoch 1359, Loss 3.193420\n",
      "    Params: tensor([  5.0605, -15.5658])\n",
      "    Grad:   tensor([-0.0523,  0.2959])\n",
      "Epoch 1360, Loss 3.192517\n",
      "    Params: tensor([  5.0610, -15.5688])\n",
      "    Grad:   tensor([-0.0522,  0.2954])\n",
      "Epoch 1361, Loss 3.191616\n",
      "    Params: tensor([  5.0616, -15.5717])\n",
      "    Grad:   tensor([-0.0521,  0.2949])\n",
      "Epoch 1362, Loss 3.190720\n",
      "    Params: tensor([  5.0621, -15.5747])\n",
      "    Grad:   tensor([-0.0520,  0.2944])\n",
      "Epoch 1363, Loss 3.189829\n",
      "    Params: tensor([  5.0626, -15.5776])\n",
      "    Grad:   tensor([-0.0519,  0.2939])\n",
      "Epoch 1364, Loss 3.188938\n",
      "    Params: tensor([  5.0631, -15.5805])\n",
      "    Grad:   tensor([-0.0518,  0.2934])\n",
      "Epoch 1365, Loss 3.188051\n",
      "    Params: tensor([  5.0636, -15.5835])\n",
      "    Grad:   tensor([-0.0517,  0.2929])\n",
      "Epoch 1366, Loss 3.187166\n",
      "    Params: tensor([  5.0642, -15.5864])\n",
      "    Grad:   tensor([-0.0516,  0.2924])\n",
      "Epoch 1367, Loss 3.186287\n",
      "    Params: tensor([  5.0647, -15.5893])\n",
      "    Grad:   tensor([-0.0516,  0.2919])\n",
      "Epoch 1368, Loss 3.185409\n",
      "    Params: tensor([  5.0652, -15.5922])\n",
      "    Grad:   tensor([-0.0515,  0.2914])\n",
      "Epoch 1369, Loss 3.184534\n",
      "    Params: tensor([  5.0657, -15.5951])\n",
      "    Grad:   tensor([-0.0514,  0.2909])\n",
      "Epoch 1370, Loss 3.183662\n",
      "    Params: tensor([  5.0662, -15.5980])\n",
      "    Grad:   tensor([-0.0513,  0.2904])\n",
      "Epoch 1371, Loss 3.182792\n",
      "    Params: tensor([  5.0667, -15.6009])\n",
      "    Grad:   tensor([-0.0512,  0.2899])\n",
      "Epoch 1372, Loss 3.181925\n",
      "    Params: tensor([  5.0672, -15.6038])\n",
      "    Grad:   tensor([-0.0511,  0.2894])\n",
      "Epoch 1373, Loss 3.181063\n",
      "    Params: tensor([  5.0678, -15.6067])\n",
      "    Grad:   tensor([-0.0510,  0.2890])\n",
      "Epoch 1374, Loss 3.180201\n",
      "    Params: tensor([  5.0683, -15.6096])\n",
      "    Grad:   tensor([-0.0509,  0.2885])\n",
      "Epoch 1375, Loss 3.179347\n",
      "    Params: tensor([  5.0688, -15.6125])\n",
      "    Grad:   tensor([-0.0509,  0.2880])\n",
      "Epoch 1376, Loss 3.178490\n",
      "    Params: tensor([  5.0693, -15.6154])\n",
      "    Grad:   tensor([-0.0508,  0.2875])\n",
      "Epoch 1377, Loss 3.177638\n",
      "    Params: tensor([  5.0698, -15.6182])\n",
      "    Grad:   tensor([-0.0507,  0.2870])\n",
      "Epoch 1378, Loss 3.176789\n",
      "    Params: tensor([  5.0703, -15.6211])\n",
      "    Grad:   tensor([-0.0506,  0.2865])\n",
      "Epoch 1379, Loss 3.175945\n",
      "    Params: tensor([  5.0708, -15.6240])\n",
      "    Grad:   tensor([-0.0505,  0.2860])\n",
      "Epoch 1380, Loss 3.175101\n",
      "    Params: tensor([  5.0713, -15.6268])\n",
      "    Grad:   tensor([-0.0504,  0.2855])\n",
      "Epoch 1381, Loss 3.174262\n",
      "    Params: tensor([  5.0718, -15.6297])\n",
      "    Grad:   tensor([-0.0504,  0.2850])\n",
      "Epoch 1382, Loss 3.173425\n",
      "    Params: tensor([  5.0723, -15.6325])\n",
      "    Grad:   tensor([-0.0503,  0.2846])\n",
      "Epoch 1383, Loss 3.172590\n",
      "    Params: tensor([  5.0728, -15.6353])\n",
      "    Grad:   tensor([-0.0502,  0.2841])\n",
      "Epoch 1384, Loss 3.171759\n",
      "    Params: tensor([  5.0733, -15.6382])\n",
      "    Grad:   tensor([-0.0501,  0.2836])\n",
      "Epoch 1385, Loss 3.170929\n",
      "    Params: tensor([  5.0738, -15.6410])\n",
      "    Grad:   tensor([-0.0500,  0.2831])\n",
      "Epoch 1386, Loss 3.170103\n",
      "    Params: tensor([  5.0743, -15.6438])\n",
      "    Grad:   tensor([-0.0499,  0.2826])\n",
      "Epoch 1387, Loss 3.169280\n",
      "    Params: tensor([  5.0748, -15.6467])\n",
      "    Grad:   tensor([-0.0498,  0.2822])\n",
      "Epoch 1388, Loss 3.168462\n",
      "    Params: tensor([  5.0753, -15.6495])\n",
      "    Grad:   tensor([-0.0498,  0.2817])\n",
      "Epoch 1389, Loss 3.167644\n",
      "    Params: tensor([  5.0758, -15.6523])\n",
      "    Grad:   tensor([-0.0497,  0.2812])\n",
      "Epoch 1390, Loss 3.166827\n",
      "    Params: tensor([  5.0763, -15.6551])\n",
      "    Grad:   tensor([-0.0496,  0.2807])\n",
      "Epoch 1391, Loss 3.166017\n",
      "    Params: tensor([  5.0768, -15.6579])\n",
      "    Grad:   tensor([-0.0495,  0.2802])\n",
      "Epoch 1392, Loss 3.165207\n",
      "    Params: tensor([  5.0773, -15.6607])\n",
      "    Grad:   tensor([-0.0494,  0.2798])\n",
      "Epoch 1393, Loss 3.164401\n",
      "    Params: tensor([  5.0778, -15.6635])\n",
      "    Grad:   tensor([-0.0493,  0.2793])\n",
      "Epoch 1394, Loss 3.163594\n",
      "    Params: tensor([  5.0783, -15.6663])\n",
      "    Grad:   tensor([-0.0492,  0.2788])\n",
      "Epoch 1395, Loss 3.162795\n",
      "    Params: tensor([  5.0788, -15.6691])\n",
      "    Grad:   tensor([-0.0492,  0.2783])\n",
      "Epoch 1396, Loss 3.161996\n",
      "    Params: tensor([  5.0793, -15.6718])\n",
      "    Grad:   tensor([-0.0491,  0.2779])\n",
      "Epoch 1397, Loss 3.161201\n",
      "    Params: tensor([  5.0797, -15.6746])\n",
      "    Grad:   tensor([-0.0490,  0.2774])\n",
      "Epoch 1398, Loss 3.160410\n",
      "    Params: tensor([  5.0802, -15.6774])\n",
      "    Grad:   tensor([-0.0489,  0.2769])\n",
      "Epoch 1399, Loss 3.159618\n",
      "    Params: tensor([  5.0807, -15.6802])\n",
      "    Grad:   tensor([-0.0488,  0.2765])\n",
      "Epoch 1400, Loss 3.158830\n",
      "    Params: tensor([  5.0812, -15.6829])\n",
      "    Grad:   tensor([-0.0488,  0.2760])\n",
      "Epoch 1401, Loss 3.158046\n",
      "    Params: tensor([  5.0817, -15.6857])\n",
      "    Grad:   tensor([-0.0487,  0.2755])\n",
      "Epoch 1402, Loss 3.157263\n",
      "    Params: tensor([  5.0822, -15.6884])\n",
      "    Grad:   tensor([-0.0486,  0.2751])\n",
      "Epoch 1403, Loss 3.156484\n",
      "    Params: tensor([  5.0827, -15.6912])\n",
      "    Grad:   tensor([-0.0485,  0.2746])\n",
      "Epoch 1404, Loss 3.155708\n",
      "    Params: tensor([  5.0832, -15.6939])\n",
      "    Grad:   tensor([-0.0484,  0.2741])\n",
      "Epoch 1405, Loss 3.154933\n",
      "    Params: tensor([  5.0836, -15.6966])\n",
      "    Grad:   tensor([-0.0483,  0.2736])\n",
      "Epoch 1406, Loss 3.154162\n",
      "    Params: tensor([  5.0841, -15.6994])\n",
      "    Grad:   tensor([-0.0483,  0.2732])\n",
      "Epoch 1407, Loss 3.153393\n",
      "    Params: tensor([  5.0846, -15.7021])\n",
      "    Grad:   tensor([-0.0482,  0.2727])\n",
      "Epoch 1408, Loss 3.152628\n",
      "    Params: tensor([  5.0851, -15.7048])\n",
      "    Grad:   tensor([-0.0481,  0.2723])\n",
      "Epoch 1409, Loss 3.151865\n",
      "    Params: tensor([  5.0856, -15.7075])\n",
      "    Grad:   tensor([-0.0480,  0.2718])\n",
      "Epoch 1410, Loss 3.151101\n",
      "    Params: tensor([  5.0860, -15.7103])\n",
      "    Grad:   tensor([-0.0479,  0.2713])\n",
      "Epoch 1411, Loss 3.150343\n",
      "    Params: tensor([  5.0865, -15.7130])\n",
      "    Grad:   tensor([-0.0479,  0.2709])\n",
      "Epoch 1412, Loss 3.149587\n",
      "    Params: tensor([  5.0870, -15.7157])\n",
      "    Grad:   tensor([-0.0478,  0.2704])\n",
      "Epoch 1413, Loss 3.148833\n",
      "    Params: tensor([  5.0875, -15.7184])\n",
      "    Grad:   tensor([-0.0477,  0.2700])\n",
      "Epoch 1414, Loss 3.148083\n",
      "    Params: tensor([  5.0879, -15.7211])\n",
      "    Grad:   tensor([-0.0476,  0.2695])\n",
      "Epoch 1415, Loss 3.147335\n",
      "    Params: tensor([  5.0884, -15.7238])\n",
      "    Grad:   tensor([-0.0475,  0.2690])\n",
      "Epoch 1416, Loss 3.146588\n",
      "    Params: tensor([  5.0889, -15.7264])\n",
      "    Grad:   tensor([-0.0474,  0.2686])\n",
      "Epoch 1417, Loss 3.145845\n",
      "    Params: tensor([  5.0894, -15.7291])\n",
      "    Grad:   tensor([-0.0474,  0.2681])\n",
      "Epoch 1418, Loss 3.145105\n",
      "    Params: tensor([  5.0898, -15.7318])\n",
      "    Grad:   tensor([-0.0473,  0.2677])\n",
      "Epoch 1419, Loss 3.144367\n",
      "    Params: tensor([  5.0903, -15.7345])\n",
      "    Grad:   tensor([-0.0472,  0.2672])\n",
      "Epoch 1420, Loss 3.143630\n",
      "    Params: tensor([  5.0908, -15.7371])\n",
      "    Grad:   tensor([-0.0471,  0.2668])\n",
      "Epoch 1421, Loss 3.142899\n",
      "    Params: tensor([  5.0913, -15.7398])\n",
      "    Grad:   tensor([-0.0470,  0.2663])\n",
      "Epoch 1422, Loss 3.142166\n",
      "    Params: tensor([  5.0917, -15.7425])\n",
      "    Grad:   tensor([-0.0469,  0.2659])\n",
      "Epoch 1423, Loss 3.141439\n",
      "    Params: tensor([  5.0922, -15.7451])\n",
      "    Grad:   tensor([-0.0469,  0.2654])\n",
      "Epoch 1424, Loss 3.140712\n",
      "    Params: tensor([  5.0927, -15.7478])\n",
      "    Grad:   tensor([-0.0468,  0.2649])\n",
      "Epoch 1425, Loss 3.139989\n",
      "    Params: tensor([  5.0931, -15.7504])\n",
      "    Grad:   tensor([-0.0467,  0.2645])\n",
      "Epoch 1426, Loss 3.139271\n",
      "    Params: tensor([  5.0936, -15.7530])\n",
      "    Grad:   tensor([-0.0466,  0.2641])\n",
      "Epoch 1427, Loss 3.138551\n",
      "    Params: tensor([  5.0941, -15.7557])\n",
      "    Grad:   tensor([-0.0466,  0.2636])\n",
      "Epoch 1428, Loss 3.137835\n",
      "    Params: tensor([  5.0945, -15.7583])\n",
      "    Grad:   tensor([-0.0465,  0.2632])\n",
      "Epoch 1429, Loss 3.137121\n",
      "    Params: tensor([  5.0950, -15.7609])\n",
      "    Grad:   tensor([-0.0464,  0.2627])\n",
      "Epoch 1430, Loss 3.136409\n",
      "    Params: tensor([  5.0955, -15.7636])\n",
      "    Grad:   tensor([-0.0463,  0.2623])\n",
      "Epoch 1431, Loss 3.135702\n",
      "    Params: tensor([  5.0959, -15.7662])\n",
      "    Grad:   tensor([-0.0462,  0.2618])\n",
      "Epoch 1432, Loss 3.134994\n",
      "    Params: tensor([  5.0964, -15.7688])\n",
      "    Grad:   tensor([-0.0461,  0.2614])\n",
      "Epoch 1433, Loss 3.134292\n",
      "    Params: tensor([  5.0968, -15.7714])\n",
      "    Grad:   tensor([-0.0461,  0.2609])\n",
      "Epoch 1434, Loss 3.133590\n",
      "    Params: tensor([  5.0973, -15.7740])\n",
      "    Grad:   tensor([-0.0460,  0.2605])\n",
      "Epoch 1435, Loss 3.132889\n",
      "    Params: tensor([  5.0978, -15.7766])\n",
      "    Grad:   tensor([-0.0459,  0.2600])\n",
      "Epoch 1436, Loss 3.132194\n",
      "    Params: tensor([  5.0982, -15.7792])\n",
      "    Grad:   tensor([-0.0459,  0.2596])\n",
      "Epoch 1437, Loss 3.131500\n",
      "    Params: tensor([  5.0987, -15.7818])\n",
      "    Grad:   tensor([-0.0458,  0.2592])\n",
      "Epoch 1438, Loss 3.130810\n",
      "    Params: tensor([  5.0991, -15.7844])\n",
      "    Grad:   tensor([-0.0457,  0.2587])\n",
      "Epoch 1439, Loss 3.130119\n",
      "    Params: tensor([  5.0996, -15.7870])\n",
      "    Grad:   tensor([-0.0456,  0.2583])\n",
      "Epoch 1440, Loss 3.129432\n",
      "    Params: tensor([  5.1000, -15.7895])\n",
      "    Grad:   tensor([-0.0455,  0.2578])\n",
      "Epoch 1441, Loss 3.128746\n",
      "    Params: tensor([  5.1005, -15.7921])\n",
      "    Grad:   tensor([-0.0455,  0.2574])\n",
      "Epoch 1442, Loss 3.128064\n",
      "    Params: tensor([  5.1010, -15.7947])\n",
      "    Grad:   tensor([-0.0454,  0.2570])\n",
      "Epoch 1443, Loss 3.127382\n",
      "    Params: tensor([  5.1014, -15.7973])\n",
      "    Grad:   tensor([-0.0453,  0.2565])\n",
      "Epoch 1444, Loss 3.126705\n",
      "    Params: tensor([  5.1019, -15.7998])\n",
      "    Grad:   tensor([-0.0453,  0.2561])\n",
      "Epoch 1445, Loss 3.126030\n",
      "    Params: tensor([  5.1023, -15.8024])\n",
      "    Grad:   tensor([-0.0452,  0.2557])\n",
      "Epoch 1446, Loss 3.125356\n",
      "    Params: tensor([  5.1028, -15.8049])\n",
      "    Grad:   tensor([-0.0451,  0.2552])\n",
      "Epoch 1447, Loss 3.124683\n",
      "    Params: tensor([  5.1032, -15.8075])\n",
      "    Grad:   tensor([-0.0450,  0.2548])\n",
      "Epoch 1448, Loss 3.124016\n",
      "    Params: tensor([  5.1037, -15.8100])\n",
      "    Grad:   tensor([-0.0449,  0.2544])\n",
      "Epoch 1449, Loss 3.123349\n",
      "    Params: tensor([  5.1041, -15.8126])\n",
      "    Grad:   tensor([-0.0449,  0.2539])\n",
      "Epoch 1450, Loss 3.122686\n",
      "    Params: tensor([  5.1046, -15.8151])\n",
      "    Grad:   tensor([-0.0448,  0.2535])\n",
      "Epoch 1451, Loss 3.122022\n",
      "    Params: tensor([  5.1050, -15.8176])\n",
      "    Grad:   tensor([-0.0447,  0.2531])\n",
      "Epoch 1452, Loss 3.121362\n",
      "    Params: tensor([  5.1055, -15.8201])\n",
      "    Grad:   tensor([-0.0446,  0.2526])\n",
      "Epoch 1453, Loss 3.120707\n",
      "    Params: tensor([  5.1059, -15.8227])\n",
      "    Grad:   tensor([-0.0445,  0.2522])\n",
      "Epoch 1454, Loss 3.120049\n",
      "    Params: tensor([  5.1063, -15.8252])\n",
      "    Grad:   tensor([-0.0445,  0.2518])\n",
      "Epoch 1455, Loss 3.119397\n",
      "    Params: tensor([  5.1068, -15.8277])\n",
      "    Grad:   tensor([-0.0444,  0.2513])\n",
      "Epoch 1456, Loss 3.118746\n",
      "    Params: tensor([  5.1072, -15.8302])\n",
      "    Grad:   tensor([-0.0443,  0.2509])\n",
      "Epoch 1457, Loss 3.118098\n",
      "    Params: tensor([  5.1077, -15.8327])\n",
      "    Grad:   tensor([-0.0442,  0.2505])\n",
      "Epoch 1458, Loss 3.117451\n",
      "    Params: tensor([  5.1081, -15.8352])\n",
      "    Grad:   tensor([-0.0442,  0.2501])\n",
      "Epoch 1459, Loss 3.116805\n",
      "    Params: tensor([  5.1086, -15.8377])\n",
      "    Grad:   tensor([-0.0441,  0.2496])\n",
      "Epoch 1460, Loss 3.116164\n",
      "    Params: tensor([  5.1090, -15.8402])\n",
      "    Grad:   tensor([-0.0440,  0.2492])\n",
      "Epoch 1461, Loss 3.115525\n",
      "    Params: tensor([  5.1094, -15.8427])\n",
      "    Grad:   tensor([-0.0439,  0.2488])\n",
      "Epoch 1462, Loss 3.114886\n",
      "    Params: tensor([  5.1099, -15.8452])\n",
      "    Grad:   tensor([-0.0439,  0.2484])\n",
      "Epoch 1463, Loss 3.114251\n",
      "    Params: tensor([  5.1103, -15.8477])\n",
      "    Grad:   tensor([-0.0438,  0.2480])\n",
      "Epoch 1464, Loss 3.113617\n",
      "    Params: tensor([  5.1107, -15.8501])\n",
      "    Grad:   tensor([-0.0437,  0.2475])\n",
      "Epoch 1465, Loss 3.112985\n",
      "    Params: tensor([  5.1112, -15.8526])\n",
      "    Grad:   tensor([-0.0437,  0.2471])\n",
      "Epoch 1466, Loss 3.112358\n",
      "    Params: tensor([  5.1116, -15.8551])\n",
      "    Grad:   tensor([-0.0436,  0.2467])\n",
      "Epoch 1467, Loss 3.111731\n",
      "    Params: tensor([  5.1121, -15.8575])\n",
      "    Grad:   tensor([-0.0435,  0.2463])\n",
      "Epoch 1468, Loss 3.111103\n",
      "    Params: tensor([  5.1125, -15.8600])\n",
      "    Grad:   tensor([-0.0434,  0.2459])\n",
      "Epoch 1469, Loss 3.110484\n",
      "    Params: tensor([  5.1129, -15.8624])\n",
      "    Grad:   tensor([-0.0433,  0.2454])\n",
      "Epoch 1470, Loss 3.109859\n",
      "    Params: tensor([  5.1134, -15.8649])\n",
      "    Grad:   tensor([-0.0433,  0.2450])\n",
      "Epoch 1471, Loss 3.109243\n",
      "    Params: tensor([  5.1138, -15.8673])\n",
      "    Grad:   tensor([-0.0432,  0.2446])\n",
      "Epoch 1472, Loss 3.108627\n",
      "    Params: tensor([  5.1142, -15.8698])\n",
      "    Grad:   tensor([-0.0431,  0.2442])\n",
      "Epoch 1473, Loss 3.108011\n",
      "    Params: tensor([  5.1147, -15.8722])\n",
      "    Grad:   tensor([-0.0430,  0.2438])\n",
      "Epoch 1474, Loss 3.107401\n",
      "    Params: tensor([  5.1151, -15.8747])\n",
      "    Grad:   tensor([-0.0430,  0.2434])\n",
      "Epoch 1475, Loss 3.106791\n",
      "    Params: tensor([  5.1155, -15.8771])\n",
      "    Grad:   tensor([-0.0429,  0.2429])\n",
      "Epoch 1476, Loss 3.106180\n",
      "    Params: tensor([  5.1159, -15.8795])\n",
      "    Grad:   tensor([-0.0428,  0.2425])\n",
      "Epoch 1477, Loss 3.105575\n",
      "    Params: tensor([  5.1164, -15.8819])\n",
      "    Grad:   tensor([-0.0428,  0.2421])\n",
      "Epoch 1478, Loss 3.104972\n",
      "    Params: tensor([  5.1168, -15.8843])\n",
      "    Grad:   tensor([-0.0427,  0.2417])\n",
      "Epoch 1479, Loss 3.104370\n",
      "    Params: tensor([  5.1172, -15.8868])\n",
      "    Grad:   tensor([-0.0426,  0.2413])\n",
      "Epoch 1480, Loss 3.103770\n",
      "    Params: tensor([  5.1176, -15.8892])\n",
      "    Grad:   tensor([-0.0425,  0.2409])\n",
      "Epoch 1481, Loss 3.103172\n",
      "    Params: tensor([  5.1181, -15.8916])\n",
      "    Grad:   tensor([-0.0425,  0.2405])\n",
      "Epoch 1482, Loss 3.102576\n",
      "    Params: tensor([  5.1185, -15.8940])\n",
      "    Grad:   tensor([-0.0424,  0.2401])\n",
      "Epoch 1483, Loss 3.101982\n",
      "    Params: tensor([  5.1189, -15.8964])\n",
      "    Grad:   tensor([-0.0423,  0.2397])\n",
      "Epoch 1484, Loss 3.101390\n",
      "    Params: tensor([  5.1193, -15.8988])\n",
      "    Grad:   tensor([-0.0423,  0.2393])\n",
      "Epoch 1485, Loss 3.100802\n",
      "    Params: tensor([  5.1198, -15.9011])\n",
      "    Grad:   tensor([-0.0422,  0.2388])\n",
      "Epoch 1486, Loss 3.100213\n",
      "    Params: tensor([  5.1202, -15.9035])\n",
      "    Grad:   tensor([-0.0421,  0.2384])\n",
      "Epoch 1487, Loss 3.099627\n",
      "    Params: tensor([  5.1206, -15.9059])\n",
      "    Grad:   tensor([-0.0421,  0.2380])\n",
      "Epoch 1488, Loss 3.099044\n",
      "    Params: tensor([  5.1210, -15.9083])\n",
      "    Grad:   tensor([-0.0420,  0.2376])\n",
      "Epoch 1489, Loss 3.098463\n",
      "    Params: tensor([  5.1214, -15.9107])\n",
      "    Grad:   tensor([-0.0419,  0.2372])\n",
      "Epoch 1490, Loss 3.097883\n",
      "    Params: tensor([  5.1219, -15.9130])\n",
      "    Grad:   tensor([-0.0418,  0.2368])\n",
      "Epoch 1491, Loss 3.097302\n",
      "    Params: tensor([  5.1223, -15.9154])\n",
      "    Grad:   tensor([-0.0418,  0.2364])\n",
      "Epoch 1492, Loss 3.096727\n",
      "    Params: tensor([  5.1227, -15.9178])\n",
      "    Grad:   tensor([-0.0417,  0.2360])\n",
      "Epoch 1493, Loss 3.096153\n",
      "    Params: tensor([  5.1231, -15.9201])\n",
      "    Grad:   tensor([-0.0416,  0.2356])\n",
      "Epoch 1494, Loss 3.095583\n",
      "    Params: tensor([  5.1235, -15.9225])\n",
      "    Grad:   tensor([-0.0416,  0.2352])\n",
      "Epoch 1495, Loss 3.095011\n",
      "    Params: tensor([  5.1239, -15.9248])\n",
      "    Grad:   tensor([-0.0415,  0.2348])\n",
      "Epoch 1496, Loss 3.094444\n",
      "    Params: tensor([  5.1244, -15.9272])\n",
      "    Grad:   tensor([-0.0414,  0.2344])\n",
      "Epoch 1497, Loss 3.093877\n",
      "    Params: tensor([  5.1248, -15.9295])\n",
      "    Grad:   tensor([-0.0413,  0.2340])\n",
      "Epoch 1498, Loss 3.093314\n",
      "    Params: tensor([  5.1252, -15.9318])\n",
      "    Grad:   tensor([-0.0413,  0.2336])\n",
      "Epoch 1499, Loss 3.092751\n",
      "    Params: tensor([  5.1256, -15.9342])\n",
      "    Grad:   tensor([-0.0412,  0.2332])\n",
      "Epoch 1500, Loss 3.092191\n",
      "    Params: tensor([  5.1260, -15.9365])\n",
      "    Grad:   tensor([-0.0411,  0.2328])\n",
      "Epoch 1501, Loss 3.091630\n",
      "    Params: tensor([  5.1264, -15.9388])\n",
      "    Grad:   tensor([-0.0411,  0.2324])\n",
      "Epoch 1502, Loss 3.091074\n",
      "    Params: tensor([  5.1268, -15.9411])\n",
      "    Grad:   tensor([-0.0410,  0.2320])\n",
      "Epoch 1503, Loss 3.090520\n",
      "    Params: tensor([  5.1272, -15.9435])\n",
      "    Grad:   tensor([-0.0409,  0.2317])\n",
      "Epoch 1504, Loss 3.089969\n",
      "    Params: tensor([  5.1276, -15.9458])\n",
      "    Grad:   tensor([-0.0408,  0.2313])\n",
      "Epoch 1505, Loss 3.089417\n",
      "    Params: tensor([  5.1281, -15.9481])\n",
      "    Grad:   tensor([-0.0408,  0.2309])\n",
      "Epoch 1506, Loss 3.088867\n",
      "    Params: tensor([  5.1285, -15.9504])\n",
      "    Grad:   tensor([-0.0407,  0.2305])\n",
      "Epoch 1507, Loss 3.088320\n",
      "    Params: tensor([  5.1289, -15.9527])\n",
      "    Grad:   tensor([-0.0406,  0.2301])\n",
      "Epoch 1508, Loss 3.087775\n",
      "    Params: tensor([  5.1293, -15.9550])\n",
      "    Grad:   tensor([-0.0406,  0.2297])\n",
      "Epoch 1509, Loss 3.087232\n",
      "    Params: tensor([  5.1297, -15.9573])\n",
      "    Grad:   tensor([-0.0405,  0.2293])\n",
      "Epoch 1510, Loss 3.086690\n",
      "    Params: tensor([  5.1301, -15.9596])\n",
      "    Grad:   tensor([-0.0404,  0.2289])\n",
      "Epoch 1511, Loss 3.086150\n",
      "    Params: tensor([  5.1305, -15.9618])\n",
      "    Grad:   tensor([-0.0404,  0.2285])\n",
      "Epoch 1512, Loss 3.085612\n",
      "    Params: tensor([  5.1309, -15.9641])\n",
      "    Grad:   tensor([-0.0403,  0.2281])\n",
      "Epoch 1513, Loss 3.085075\n",
      "    Params: tensor([  5.1313, -15.9664])\n",
      "    Grad:   tensor([-0.0402,  0.2277])\n",
      "Epoch 1514, Loss 3.084542\n",
      "    Params: tensor([  5.1317, -15.9687])\n",
      "    Grad:   tensor([-0.0402,  0.2274])\n",
      "Epoch 1515, Loss 3.084009\n",
      "    Params: tensor([  5.1321, -15.9709])\n",
      "    Grad:   tensor([-0.0401,  0.2270])\n",
      "Epoch 1516, Loss 3.083478\n",
      "    Params: tensor([  5.1325, -15.9732])\n",
      "    Grad:   tensor([-0.0400,  0.2266])\n",
      "Epoch 1517, Loss 3.082948\n",
      "    Params: tensor([  5.1329, -15.9755])\n",
      "    Grad:   tensor([-0.0400,  0.2262])\n",
      "Epoch 1518, Loss 3.082422\n",
      "    Params: tensor([  5.1333, -15.9777])\n",
      "    Grad:   tensor([-0.0399,  0.2258])\n",
      "Epoch 1519, Loss 3.081897\n",
      "    Params: tensor([  5.1337, -15.9800])\n",
      "    Grad:   tensor([-0.0398,  0.2254])\n",
      "Epoch 1520, Loss 3.081373\n",
      "    Params: tensor([  5.1341, -15.9822])\n",
      "    Grad:   tensor([-0.0398,  0.2250])\n",
      "Epoch 1521, Loss 3.080850\n",
      "    Params: tensor([  5.1345, -15.9845])\n",
      "    Grad:   tensor([-0.0397,  0.2247])\n",
      "Epoch 1522, Loss 3.080331\n",
      "    Params: tensor([  5.1349, -15.9867])\n",
      "    Grad:   tensor([-0.0396,  0.2243])\n",
      "Epoch 1523, Loss 3.079811\n",
      "    Params: tensor([  5.1353, -15.9890])\n",
      "    Grad:   tensor([-0.0396,  0.2239])\n",
      "Epoch 1524, Loss 3.079296\n",
      "    Params: tensor([  5.1357, -15.9912])\n",
      "    Grad:   tensor([-0.0395,  0.2235])\n",
      "Epoch 1525, Loss 3.078781\n",
      "    Params: tensor([  5.1361, -15.9934])\n",
      "    Grad:   tensor([-0.0394,  0.2231])\n",
      "Epoch 1526, Loss 3.078268\n",
      "    Params: tensor([  5.1365, -15.9957])\n",
      "    Grad:   tensor([-0.0394,  0.2228])\n",
      "Epoch 1527, Loss 3.077758\n",
      "    Params: tensor([  5.1369, -15.9979])\n",
      "    Grad:   tensor([-0.0393,  0.2224])\n",
      "Epoch 1528, Loss 3.077248\n",
      "    Params: tensor([  5.1372, -16.0001])\n",
      "    Grad:   tensor([-0.0392,  0.2220])\n",
      "Epoch 1529, Loss 3.076739\n",
      "    Params: tensor([  5.1376, -16.0023])\n",
      "    Grad:   tensor([-0.0391,  0.2216])\n",
      "Epoch 1530, Loss 3.076232\n",
      "    Params: tensor([  5.1380, -16.0045])\n",
      "    Grad:   tensor([-0.0391,  0.2213])\n",
      "Epoch 1531, Loss 3.075729\n",
      "    Params: tensor([  5.1384, -16.0067])\n",
      "    Grad:   tensor([-0.0390,  0.2209])\n",
      "Epoch 1532, Loss 3.075225\n",
      "    Params: tensor([  5.1388, -16.0089])\n",
      "    Grad:   tensor([-0.0390,  0.2205])\n",
      "Epoch 1533, Loss 3.074724\n",
      "    Params: tensor([  5.1392, -16.0111])\n",
      "    Grad:   tensor([-0.0389,  0.2201])\n",
      "Epoch 1534, Loss 3.074227\n",
      "    Params: tensor([  5.1396, -16.0133])\n",
      "    Grad:   tensor([-0.0388,  0.2198])\n",
      "Epoch 1535, Loss 3.073726\n",
      "    Params: tensor([  5.1400, -16.0155])\n",
      "    Grad:   tensor([-0.0387,  0.2194])\n",
      "Epoch 1536, Loss 3.073232\n",
      "    Params: tensor([  5.1404, -16.0177])\n",
      "    Grad:   tensor([-0.0387,  0.2190])\n",
      "Epoch 1537, Loss 3.072739\n",
      "    Params: tensor([  5.1407, -16.0199])\n",
      "    Grad:   tensor([-0.0386,  0.2186])\n",
      "Epoch 1538, Loss 3.072245\n",
      "    Params: tensor([  5.1411, -16.0221])\n",
      "    Grad:   tensor([-0.0385,  0.2183])\n",
      "Epoch 1539, Loss 3.071753\n",
      "    Params: tensor([  5.1415, -16.0243])\n",
      "    Grad:   tensor([-0.0385,  0.2179])\n",
      "Epoch 1540, Loss 3.071265\n",
      "    Params: tensor([  5.1419, -16.0264])\n",
      "    Grad:   tensor([-0.0384,  0.2175])\n",
      "Epoch 1541, Loss 3.070778\n",
      "    Params: tensor([  5.1423, -16.0286])\n",
      "    Grad:   tensor([-0.0383,  0.2172])\n",
      "Epoch 1542, Loss 3.070293\n",
      "    Params: tensor([  5.1427, -16.0308])\n",
      "    Grad:   tensor([-0.0383,  0.2168])\n",
      "Epoch 1543, Loss 3.069808\n",
      "    Params: tensor([  5.1430, -16.0330])\n",
      "    Grad:   tensor([-0.0382,  0.2164])\n",
      "Epoch 1544, Loss 3.069326\n",
      "    Params: tensor([  5.1434, -16.0351])\n",
      "    Grad:   tensor([-0.0382,  0.2161])\n",
      "Epoch 1545, Loss 3.068845\n",
      "    Params: tensor([  5.1438, -16.0373])\n",
      "    Grad:   tensor([-0.0381,  0.2157])\n",
      "Epoch 1546, Loss 3.068366\n",
      "    Params: tensor([  5.1442, -16.0394])\n",
      "    Grad:   tensor([-0.0380,  0.2153])\n",
      "Epoch 1547, Loss 3.067887\n",
      "    Params: tensor([  5.1446, -16.0416])\n",
      "    Grad:   tensor([-0.0380,  0.2150])\n",
      "Epoch 1548, Loss 3.067412\n",
      "    Params: tensor([  5.1449, -16.0437])\n",
      "    Grad:   tensor([-0.0379,  0.2146])\n",
      "Epoch 1549, Loss 3.066937\n",
      "    Params: tensor([  5.1453, -16.0459])\n",
      "    Grad:   tensor([-0.0378,  0.2142])\n",
      "Epoch 1550, Loss 3.066463\n",
      "    Params: tensor([  5.1457, -16.0480])\n",
      "    Grad:   tensor([-0.0378,  0.2139])\n",
      "Epoch 1551, Loss 3.065993\n",
      "    Params: tensor([  5.1461, -16.0501])\n",
      "    Grad:   tensor([-0.0377,  0.2135])\n",
      "Epoch 1552, Loss 3.065524\n",
      "    Params: tensor([  5.1465, -16.0523])\n",
      "    Grad:   tensor([-0.0376,  0.2131])\n",
      "Epoch 1553, Loss 3.065055\n",
      "    Params: tensor([  5.1468, -16.0544])\n",
      "    Grad:   tensor([-0.0376,  0.2128])\n",
      "Epoch 1554, Loss 3.064588\n",
      "    Params: tensor([  5.1472, -16.0565])\n",
      "    Grad:   tensor([-0.0375,  0.2124])\n",
      "Epoch 1555, Loss 3.064123\n",
      "    Params: tensor([  5.1476, -16.0586])\n",
      "    Grad:   tensor([-0.0375,  0.2120])\n",
      "Epoch 1556, Loss 3.063660\n",
      "    Params: tensor([  5.1480, -16.0608])\n",
      "    Grad:   tensor([-0.0374,  0.2117])\n",
      "Epoch 1557, Loss 3.063199\n",
      "    Params: tensor([  5.1483, -16.0629])\n",
      "    Grad:   tensor([-0.0373,  0.2113])\n",
      "Epoch 1558, Loss 3.062738\n",
      "    Params: tensor([  5.1487, -16.0650])\n",
      "    Grad:   tensor([-0.0373,  0.2110])\n",
      "Epoch 1559, Loss 3.062280\n",
      "    Params: tensor([  5.1491, -16.0671])\n",
      "    Grad:   tensor([-0.0372,  0.2106])\n",
      "Epoch 1560, Loss 3.061822\n",
      "    Params: tensor([  5.1494, -16.0692])\n",
      "    Grad:   tensor([-0.0371,  0.2103])\n",
      "Epoch 1561, Loss 3.061367\n",
      "    Params: tensor([  5.1498, -16.0713])\n",
      "    Grad:   tensor([-0.0371,  0.2099])\n",
      "Epoch 1562, Loss 3.060913\n",
      "    Params: tensor([  5.1502, -16.0734])\n",
      "    Grad:   tensor([-0.0370,  0.2095])\n",
      "Epoch 1563, Loss 3.060462\n",
      "    Params: tensor([  5.1506, -16.0755])\n",
      "    Grad:   tensor([-0.0370,  0.2092])\n",
      "Epoch 1564, Loss 3.060011\n",
      "    Params: tensor([  5.1509, -16.0776])\n",
      "    Grad:   tensor([-0.0369,  0.2088])\n",
      "Epoch 1565, Loss 3.059561\n",
      "    Params: tensor([  5.1513, -16.0796])\n",
      "    Grad:   tensor([-0.0368,  0.2085])\n",
      "Epoch 1566, Loss 3.059114\n",
      "    Params: tensor([  5.1517, -16.0817])\n",
      "    Grad:   tensor([-0.0368,  0.2081])\n",
      "Epoch 1567, Loss 3.058668\n",
      "    Params: tensor([  5.1520, -16.0838])\n",
      "    Grad:   tensor([-0.0367,  0.2078])\n",
      "Epoch 1568, Loss 3.058221\n",
      "    Params: tensor([  5.1524, -16.0859])\n",
      "    Grad:   tensor([-0.0366,  0.2074])\n",
      "Epoch 1569, Loss 3.057781\n",
      "    Params: tensor([  5.1528, -16.0880])\n",
      "    Grad:   tensor([-0.0366,  0.2071])\n",
      "Epoch 1570, Loss 3.057338\n",
      "    Params: tensor([  5.1531, -16.0900])\n",
      "    Grad:   tensor([-0.0365,  0.2067])\n",
      "Epoch 1571, Loss 3.056898\n",
      "    Params: tensor([  5.1535, -16.0921])\n",
      "    Grad:   tensor([-0.0364,  0.2064])\n",
      "Epoch 1572, Loss 3.056458\n",
      "    Params: tensor([  5.1539, -16.0941])\n",
      "    Grad:   tensor([-0.0364,  0.2060])\n",
      "Epoch 1573, Loss 3.056019\n",
      "    Params: tensor([  5.1542, -16.0962])\n",
      "    Grad:   tensor([-0.0363,  0.2057])\n",
      "Epoch 1574, Loss 3.055585\n",
      "    Params: tensor([  5.1546, -16.0983])\n",
      "    Grad:   tensor([-0.0363,  0.2053])\n",
      "Epoch 1575, Loss 3.055151\n",
      "    Params: tensor([  5.1549, -16.1003])\n",
      "    Grad:   tensor([-0.0362,  0.2050])\n",
      "Epoch 1576, Loss 3.054717\n",
      "    Params: tensor([  5.1553, -16.1023])\n",
      "    Grad:   tensor([-0.0361,  0.2046])\n",
      "Epoch 1577, Loss 3.054286\n",
      "    Params: tensor([  5.1557, -16.1044])\n",
      "    Grad:   tensor([-0.0361,  0.2043])\n",
      "Epoch 1578, Loss 3.053857\n",
      "    Params: tensor([  5.1560, -16.1064])\n",
      "    Grad:   tensor([-0.0360,  0.2039])\n",
      "Epoch 1579, Loss 3.053427\n",
      "    Params: tensor([  5.1564, -16.1085])\n",
      "    Grad:   tensor([-0.0360,  0.2036])\n",
      "Epoch 1580, Loss 3.053000\n",
      "    Params: tensor([  5.1567, -16.1105])\n",
      "    Grad:   tensor([-0.0359,  0.2032])\n",
      "Epoch 1581, Loss 3.052576\n",
      "    Params: tensor([  5.1571, -16.1125])\n",
      "    Grad:   tensor([-0.0358,  0.2029])\n",
      "Epoch 1582, Loss 3.052152\n",
      "    Params: tensor([  5.1575, -16.1146])\n",
      "    Grad:   tensor([-0.0358,  0.2025])\n",
      "Epoch 1583, Loss 3.051730\n",
      "    Params: tensor([  5.1578, -16.1166])\n",
      "    Grad:   tensor([-0.0357,  0.2022])\n",
      "Epoch 1584, Loss 3.051306\n",
      "    Params: tensor([  5.1582, -16.1186])\n",
      "    Grad:   tensor([-0.0357,  0.2018])\n",
      "Epoch 1585, Loss 3.050888\n",
      "    Params: tensor([  5.1585, -16.1206])\n",
      "    Grad:   tensor([-0.0356,  0.2015])\n",
      "Epoch 1586, Loss 3.050471\n",
      "    Params: tensor([  5.1589, -16.1226])\n",
      "    Grad:   tensor([-0.0355,  0.2012])\n",
      "Epoch 1587, Loss 3.050052\n",
      "    Params: tensor([  5.1592, -16.1246])\n",
      "    Grad:   tensor([-0.0355,  0.2008])\n",
      "Epoch 1588, Loss 3.049639\n",
      "    Params: tensor([  5.1596, -16.1266])\n",
      "    Grad:   tensor([-0.0354,  0.2005])\n",
      "Epoch 1589, Loss 3.049223\n",
      "    Params: tensor([  5.1599, -16.1286])\n",
      "    Grad:   tensor([-0.0354,  0.2001])\n",
      "Epoch 1590, Loss 3.048811\n",
      "    Params: tensor([  5.1603, -16.1306])\n",
      "    Grad:   tensor([-0.0353,  0.1998])\n",
      "Epoch 1591, Loss 3.048398\n",
      "    Params: tensor([  5.1607, -16.1326])\n",
      "    Grad:   tensor([-0.0353,  0.1995])\n",
      "Epoch 1592, Loss 3.047991\n",
      "    Params: tensor([  5.1610, -16.1346])\n",
      "    Grad:   tensor([-0.0352,  0.1991])\n",
      "Epoch 1593, Loss 3.047581\n",
      "    Params: tensor([  5.1614, -16.1366])\n",
      "    Grad:   tensor([-0.0351,  0.1988])\n",
      "Epoch 1594, Loss 3.047173\n",
      "    Params: tensor([  5.1617, -16.1386])\n",
      "    Grad:   tensor([-0.0351,  0.1984])\n",
      "Epoch 1595, Loss 3.046768\n",
      "    Params: tensor([  5.1621, -16.1406])\n",
      "    Grad:   tensor([-0.0350,  0.1981])\n",
      "Epoch 1596, Loss 3.046362\n",
      "    Params: tensor([  5.1624, -16.1425])\n",
      "    Grad:   tensor([-0.0349,  0.1978])\n",
      "Epoch 1597, Loss 3.045960\n",
      "    Params: tensor([  5.1628, -16.1445])\n",
      "    Grad:   tensor([-0.0349,  0.1974])\n",
      "Epoch 1598, Loss 3.045559\n",
      "    Params: tensor([  5.1631, -16.1465])\n",
      "    Grad:   tensor([-0.0348,  0.1971])\n",
      "Epoch 1599, Loss 3.045160\n",
      "    Params: tensor([  5.1635, -16.1485])\n",
      "    Grad:   tensor([-0.0348,  0.1968])\n",
      "Epoch 1600, Loss 3.044759\n",
      "    Params: tensor([  5.1638, -16.1504])\n",
      "    Grad:   tensor([-0.0347,  0.1964])\n",
      "Epoch 1601, Loss 3.044361\n",
      "    Params: tensor([  5.1641, -16.1524])\n",
      "    Grad:   tensor([-0.0346,  0.1961])\n",
      "Epoch 1602, Loss 3.043966\n",
      "    Params: tensor([  5.1645, -16.1543])\n",
      "    Grad:   tensor([-0.0346,  0.1958])\n",
      "Epoch 1603, Loss 3.043571\n",
      "    Params: tensor([  5.1648, -16.1563])\n",
      "    Grad:   tensor([-0.0345,  0.1954])\n",
      "Epoch 1604, Loss 3.043176\n",
      "    Params: tensor([  5.1652, -16.1582])\n",
      "    Grad:   tensor([-0.0345,  0.1951])\n",
      "Epoch 1605, Loss 3.042785\n",
      "    Params: tensor([  5.1655, -16.1602])\n",
      "    Grad:   tensor([-0.0344,  0.1948])\n",
      "Epoch 1606, Loss 3.042395\n",
      "    Params: tensor([  5.1659, -16.1621])\n",
      "    Grad:   tensor([-0.0343,  0.1944])\n",
      "Epoch 1607, Loss 3.042005\n",
      "    Params: tensor([  5.1662, -16.1641])\n",
      "    Grad:   tensor([-0.0343,  0.1941])\n",
      "Epoch 1608, Loss 3.041615\n",
      "    Params: tensor([  5.1666, -16.1660])\n",
      "    Grad:   tensor([-0.0342,  0.1938])\n",
      "Epoch 1609, Loss 3.041230\n",
      "    Params: tensor([  5.1669, -16.1680])\n",
      "    Grad:   tensor([-0.0342,  0.1934])\n",
      "Epoch 1610, Loss 3.040844\n",
      "    Params: tensor([  5.1672, -16.1699])\n",
      "    Grad:   tensor([-0.0341,  0.1931])\n",
      "Epoch 1611, Loss 3.040461\n",
      "    Params: tensor([  5.1676, -16.1718])\n",
      "    Grad:   tensor([-0.0341,  0.1928])\n",
      "Epoch 1612, Loss 3.040077\n",
      "    Params: tensor([  5.1679, -16.1737])\n",
      "    Grad:   tensor([-0.0340,  0.1925])\n",
      "Epoch 1613, Loss 3.039695\n",
      "    Params: tensor([  5.1683, -16.1757])\n",
      "    Grad:   tensor([-0.0339,  0.1921])\n",
      "Epoch 1614, Loss 3.039314\n",
      "    Params: tensor([  5.1686, -16.1776])\n",
      "    Grad:   tensor([-0.0339,  0.1918])\n",
      "Epoch 1615, Loss 3.038934\n",
      "    Params: tensor([  5.1689, -16.1795])\n",
      "    Grad:   tensor([-0.0338,  0.1915])\n",
      "Epoch 1616, Loss 3.038557\n",
      "    Params: tensor([  5.1693, -16.1814])\n",
      "    Grad:   tensor([-0.0338,  0.1912])\n",
      "Epoch 1617, Loss 3.038181\n",
      "    Params: tensor([  5.1696, -16.1833])\n",
      "    Grad:   tensor([-0.0337,  0.1908])\n",
      "Epoch 1618, Loss 3.037805\n",
      "    Params: tensor([  5.1699, -16.1852])\n",
      "    Grad:   tensor([-0.0337,  0.1905])\n",
      "Epoch 1619, Loss 3.037432\n",
      "    Params: tensor([  5.1703, -16.1871])\n",
      "    Grad:   tensor([-0.0336,  0.1902])\n",
      "Epoch 1620, Loss 3.037059\n",
      "    Params: tensor([  5.1706, -16.1890])\n",
      "    Grad:   tensor([-0.0335,  0.1899])\n",
      "Epoch 1621, Loss 3.036689\n",
      "    Params: tensor([  5.1710, -16.1909])\n",
      "    Grad:   tensor([-0.0335,  0.1895])\n",
      "Epoch 1622, Loss 3.036319\n",
      "    Params: tensor([  5.1713, -16.1928])\n",
      "    Grad:   tensor([-0.0334,  0.1892])\n",
      "Epoch 1623, Loss 3.035949\n",
      "    Params: tensor([  5.1716, -16.1947])\n",
      "    Grad:   tensor([-0.0334,  0.1889])\n",
      "Epoch 1624, Loss 3.035583\n",
      "    Params: tensor([  5.1720, -16.1966])\n",
      "    Grad:   tensor([-0.0333,  0.1886])\n",
      "Epoch 1625, Loss 3.035216\n",
      "    Params: tensor([  5.1723, -16.1985])\n",
      "    Grad:   tensor([-0.0333,  0.1883])\n",
      "Epoch 1626, Loss 3.034849\n",
      "    Params: tensor([  5.1726, -16.2003])\n",
      "    Grad:   tensor([-0.0332,  0.1879])\n",
      "Epoch 1627, Loss 3.034485\n",
      "    Params: tensor([  5.1729, -16.2022])\n",
      "    Grad:   tensor([-0.0331,  0.1876])\n",
      "Epoch 1628, Loss 3.034123\n",
      "    Params: tensor([  5.1733, -16.2041])\n",
      "    Grad:   tensor([-0.0331,  0.1873])\n",
      "Epoch 1629, Loss 3.033762\n",
      "    Params: tensor([  5.1736, -16.2060])\n",
      "    Grad:   tensor([-0.0330,  0.1870])\n",
      "Epoch 1630, Loss 3.033402\n",
      "    Params: tensor([  5.1739, -16.2078])\n",
      "    Grad:   tensor([-0.0330,  0.1867])\n",
      "Epoch 1631, Loss 3.033041\n",
      "    Params: tensor([  5.1743, -16.2097])\n",
      "    Grad:   tensor([-0.0329,  0.1863])\n",
      "Epoch 1632, Loss 3.032685\n",
      "    Params: tensor([  5.1746, -16.2116])\n",
      "    Grad:   tensor([-0.0329,  0.1860])\n",
      "Epoch 1633, Loss 3.032329\n",
      "    Params: tensor([  5.1749, -16.2134])\n",
      "    Grad:   tensor([-0.0328,  0.1857])\n",
      "Epoch 1634, Loss 3.031973\n",
      "    Params: tensor([  5.1753, -16.2153])\n",
      "    Grad:   tensor([-0.0327,  0.1854])\n",
      "Epoch 1635, Loss 3.031619\n",
      "    Params: tensor([  5.1756, -16.2171])\n",
      "    Grad:   tensor([-0.0327,  0.1851])\n",
      "Epoch 1636, Loss 3.031265\n",
      "    Params: tensor([  5.1759, -16.2190])\n",
      "    Grad:   tensor([-0.0326,  0.1848])\n",
      "Epoch 1637, Loss 3.030913\n",
      "    Params: tensor([  5.1762, -16.2208])\n",
      "    Grad:   tensor([-0.0326,  0.1845])\n",
      "Epoch 1638, Loss 3.030564\n",
      "    Params: tensor([  5.1766, -16.2226])\n",
      "    Grad:   tensor([-0.0325,  0.1841])\n",
      "Epoch 1639, Loss 3.030215\n",
      "    Params: tensor([  5.1769, -16.2245])\n",
      "    Grad:   tensor([-0.0325,  0.1838])\n",
      "Epoch 1640, Loss 3.029867\n",
      "    Params: tensor([  5.1772, -16.2263])\n",
      "    Grad:   tensor([-0.0324,  0.1835])\n",
      "Epoch 1641, Loss 3.029518\n",
      "    Params: tensor([  5.1775, -16.2282])\n",
      "    Grad:   tensor([-0.0324,  0.1832])\n",
      "Epoch 1642, Loss 3.029173\n",
      "    Params: tensor([  5.1779, -16.2300])\n",
      "    Grad:   tensor([-0.0323,  0.1829])\n",
      "Epoch 1643, Loss 3.028828\n",
      "    Params: tensor([  5.1782, -16.2318])\n",
      "    Grad:   tensor([-0.0323,  0.1826])\n",
      "Epoch 1644, Loss 3.028486\n",
      "    Params: tensor([  5.1785, -16.2336])\n",
      "    Grad:   tensor([-0.0322,  0.1823])\n",
      "Epoch 1645, Loss 3.028142\n",
      "    Params: tensor([  5.1788, -16.2355])\n",
      "    Grad:   tensor([-0.0321,  0.1820])\n",
      "Epoch 1646, Loss 3.027802\n",
      "    Params: tensor([  5.1791, -16.2373])\n",
      "    Grad:   tensor([-0.0321,  0.1817])\n",
      "Epoch 1647, Loss 3.027463\n",
      "    Params: tensor([  5.1795, -16.2391])\n",
      "    Grad:   tensor([-0.0320,  0.1813])\n",
      "Epoch 1648, Loss 3.027122\n",
      "    Params: tensor([  5.1798, -16.2409])\n",
      "    Grad:   tensor([-0.0320,  0.1810])\n",
      "Epoch 1649, Loss 3.026784\n",
      "    Params: tensor([  5.1801, -16.2427])\n",
      "    Grad:   tensor([-0.0319,  0.1807])\n",
      "Epoch 1650, Loss 3.026447\n",
      "    Params: tensor([  5.1804, -16.2445])\n",
      "    Grad:   tensor([-0.0319,  0.1804])\n",
      "Epoch 1651, Loss 3.026111\n",
      "    Params: tensor([  5.1807, -16.2463])\n",
      "    Grad:   tensor([-0.0318,  0.1801])\n",
      "Epoch 1652, Loss 3.025780\n",
      "    Params: tensor([  5.1811, -16.2481])\n",
      "    Grad:   tensor([-0.0318,  0.1798])\n",
      "Epoch 1653, Loss 3.025447\n",
      "    Params: tensor([  5.1814, -16.2499])\n",
      "    Grad:   tensor([-0.0317,  0.1795])\n",
      "Epoch 1654, Loss 3.025114\n",
      "    Params: tensor([  5.1817, -16.2517])\n",
      "    Grad:   tensor([-0.0317,  0.1792])\n",
      "Epoch 1655, Loss 3.024782\n",
      "    Params: tensor([  5.1820, -16.2535])\n",
      "    Grad:   tensor([-0.0316,  0.1789])\n",
      "Epoch 1656, Loss 3.024452\n",
      "    Params: tensor([  5.1823, -16.2553])\n",
      "    Grad:   tensor([-0.0316,  0.1786])\n",
      "Epoch 1657, Loss 3.024125\n",
      "    Params: tensor([  5.1826, -16.2570])\n",
      "    Grad:   tensor([-0.0315,  0.1783])\n",
      "Epoch 1658, Loss 3.023796\n",
      "    Params: tensor([  5.1829, -16.2588])\n",
      "    Grad:   tensor([-0.0315,  0.1780])\n",
      "Epoch 1659, Loss 3.023471\n",
      "    Params: tensor([  5.1833, -16.2606])\n",
      "    Grad:   tensor([-0.0314,  0.1777])\n",
      "Epoch 1660, Loss 3.023145\n",
      "    Params: tensor([  5.1836, -16.2624])\n",
      "    Grad:   tensor([-0.0313,  0.1774])\n",
      "Epoch 1661, Loss 3.022820\n",
      "    Params: tensor([  5.1839, -16.2641])\n",
      "    Grad:   tensor([-0.0313,  0.1771])\n",
      "Epoch 1662, Loss 3.022498\n",
      "    Params: tensor([  5.1842, -16.2659])\n",
      "    Grad:   tensor([-0.0312,  0.1768])\n",
      "Epoch 1663, Loss 3.022177\n",
      "    Params: tensor([  5.1845, -16.2677])\n",
      "    Grad:   tensor([-0.0312,  0.1765])\n",
      "Epoch 1664, Loss 3.021855\n",
      "    Params: tensor([  5.1848, -16.2694])\n",
      "    Grad:   tensor([-0.0311,  0.1762])\n",
      "Epoch 1665, Loss 3.021534\n",
      "    Params: tensor([  5.1851, -16.2712])\n",
      "    Grad:   tensor([-0.0311,  0.1759])\n",
      "Epoch 1666, Loss 3.021217\n",
      "    Params: tensor([  5.1854, -16.2730])\n",
      "    Grad:   tensor([-0.0310,  0.1756])\n",
      "Epoch 1667, Loss 3.020898\n",
      "    Params: tensor([  5.1858, -16.2747])\n",
      "    Grad:   tensor([-0.0310,  0.1753])\n",
      "Epoch 1668, Loss 3.020582\n",
      "    Params: tensor([  5.1861, -16.2765])\n",
      "    Grad:   tensor([-0.0309,  0.1750])\n",
      "Epoch 1669, Loss 3.020265\n",
      "    Params: tensor([  5.1864, -16.2782])\n",
      "    Grad:   tensor([-0.0309,  0.1747])\n",
      "Epoch 1670, Loss 3.019952\n",
      "    Params: tensor([  5.1867, -16.2800])\n",
      "    Grad:   tensor([-0.0308,  0.1744])\n",
      "Epoch 1671, Loss 3.019639\n",
      "    Params: tensor([  5.1870, -16.2817])\n",
      "    Grad:   tensor([-0.0308,  0.1741])\n",
      "Epoch 1672, Loss 3.019325\n",
      "    Params: tensor([  5.1873, -16.2834])\n",
      "    Grad:   tensor([-0.0307,  0.1738])\n",
      "Epoch 1673, Loss 3.019016\n",
      "    Params: tensor([  5.1876, -16.2852])\n",
      "    Grad:   tensor([-0.0307,  0.1735])\n",
      "Epoch 1674, Loss 3.018706\n",
      "    Params: tensor([  5.1879, -16.2869])\n",
      "    Grad:   tensor([-0.0306,  0.1732])\n",
      "Epoch 1675, Loss 3.018395\n",
      "    Params: tensor([  5.1882, -16.2886])\n",
      "    Grad:   tensor([-0.0305,  0.1729])\n",
      "Epoch 1676, Loss 3.018089\n",
      "    Params: tensor([  5.1885, -16.2904])\n",
      "    Grad:   tensor([-0.0305,  0.1726])\n",
      "Epoch 1677, Loss 3.017780\n",
      "    Params: tensor([  5.1888, -16.2921])\n",
      "    Grad:   tensor([-0.0304,  0.1723])\n",
      "Epoch 1678, Loss 3.017475\n",
      "    Params: tensor([  5.1891, -16.2938])\n",
      "    Grad:   tensor([-0.0304,  0.1720])\n",
      "Epoch 1679, Loss 3.017170\n",
      "    Params: tensor([  5.1894, -16.2955])\n",
      "    Grad:   tensor([-0.0303,  0.1717])\n",
      "Epoch 1680, Loss 3.016867\n",
      "    Params: tensor([  5.1897, -16.2972])\n",
      "    Grad:   tensor([-0.0303,  0.1715])\n",
      "Epoch 1681, Loss 3.016564\n",
      "    Params: tensor([  5.1900, -16.2989])\n",
      "    Grad:   tensor([-0.0302,  0.1712])\n",
      "Epoch 1682, Loss 3.016262\n",
      "    Params: tensor([  5.1903, -16.3006])\n",
      "    Grad:   tensor([-0.0302,  0.1709])\n",
      "Epoch 1683, Loss 3.015959\n",
      "    Params: tensor([  5.1906, -16.3024])\n",
      "    Grad:   tensor([-0.0301,  0.1706])\n",
      "Epoch 1684, Loss 3.015662\n",
      "    Params: tensor([  5.1909, -16.3041])\n",
      "    Grad:   tensor([-0.0301,  0.1703])\n",
      "Epoch 1685, Loss 3.015361\n",
      "    Params: tensor([  5.1912, -16.3058])\n",
      "    Grad:   tensor([-0.0300,  0.1700])\n",
      "Epoch 1686, Loss 3.015064\n",
      "    Params: tensor([  5.1915, -16.3075])\n",
      "    Grad:   tensor([-0.0300,  0.1697])\n",
      "Epoch 1687, Loss 3.014768\n",
      "    Params: tensor([  5.1918, -16.3091])\n",
      "    Grad:   tensor([-0.0299,  0.1694])\n",
      "Epoch 1688, Loss 3.014472\n",
      "    Params: tensor([  5.1921, -16.3108])\n",
      "    Grad:   tensor([-0.0299,  0.1691])\n",
      "Epoch 1689, Loss 3.014179\n",
      "    Params: tensor([  5.1924, -16.3125])\n",
      "    Grad:   tensor([-0.0298,  0.1688])\n",
      "Epoch 1690, Loss 3.013884\n",
      "    Params: tensor([  5.1927, -16.3142])\n",
      "    Grad:   tensor([-0.0298,  0.1686])\n",
      "Epoch 1691, Loss 3.013591\n",
      "    Params: tensor([  5.1930, -16.3159])\n",
      "    Grad:   tensor([-0.0297,  0.1683])\n",
      "Epoch 1692, Loss 3.013299\n",
      "    Params: tensor([  5.1933, -16.3176])\n",
      "    Grad:   tensor([-0.0297,  0.1680])\n",
      "Epoch 1693, Loss 3.013008\n",
      "    Params: tensor([  5.1936, -16.3193])\n",
      "    Grad:   tensor([-0.0296,  0.1677])\n",
      "Epoch 1694, Loss 3.012719\n",
      "    Params: tensor([  5.1939, -16.3209])\n",
      "    Grad:   tensor([-0.0296,  0.1674])\n",
      "Epoch 1695, Loss 3.012431\n",
      "    Params: tensor([  5.1942, -16.3226])\n",
      "    Grad:   tensor([-0.0295,  0.1671])\n",
      "Epoch 1696, Loss 3.012141\n",
      "    Params: tensor([  5.1945, -16.3243])\n",
      "    Grad:   tensor([-0.0295,  0.1668])\n",
      "Epoch 1697, Loss 3.011855\n",
      "    Params: tensor([  5.1948, -16.3259])\n",
      "    Grad:   tensor([-0.0294,  0.1666])\n",
      "Epoch 1698, Loss 3.011570\n",
      "    Params: tensor([  5.1951, -16.3276])\n",
      "    Grad:   tensor([-0.0294,  0.1663])\n",
      "Epoch 1699, Loss 3.011284\n",
      "    Params: tensor([  5.1954, -16.3293])\n",
      "    Grad:   tensor([-0.0293,  0.1660])\n",
      "Epoch 1700, Loss 3.011001\n",
      "    Params: tensor([  5.1957, -16.3309])\n",
      "    Grad:   tensor([-0.0293,  0.1657])\n",
      "Epoch 1701, Loss 3.010718\n",
      "    Params: tensor([  5.1960, -16.3326])\n",
      "    Grad:   tensor([-0.0292,  0.1654])\n",
      "Epoch 1702, Loss 3.010436\n",
      "    Params: tensor([  5.1963, -16.3342])\n",
      "    Grad:   tensor([-0.0292,  0.1652])\n",
      "Epoch 1703, Loss 3.010156\n",
      "    Params: tensor([  5.1966, -16.3359])\n",
      "    Grad:   tensor([-0.0291,  0.1649])\n",
      "Epoch 1704, Loss 3.009876\n",
      "    Params: tensor([  5.1968, -16.3375])\n",
      "    Grad:   tensor([-0.0291,  0.1646])\n",
      "Epoch 1705, Loss 3.009595\n",
      "    Params: tensor([  5.1971, -16.3392])\n",
      "    Grad:   tensor([-0.0290,  0.1643])\n",
      "Epoch 1706, Loss 3.009319\n",
      "    Params: tensor([  5.1974, -16.3408])\n",
      "    Grad:   tensor([-0.0290,  0.1640])\n",
      "Epoch 1707, Loss 3.009040\n",
      "    Params: tensor([  5.1977, -16.3424])\n",
      "    Grad:   tensor([-0.0289,  0.1638])\n",
      "Epoch 1708, Loss 3.008763\n",
      "    Params: tensor([  5.1980, -16.3441])\n",
      "    Grad:   tensor([-0.0289,  0.1635])\n",
      "Epoch 1709, Loss 3.008487\n",
      "    Params: tensor([  5.1983, -16.3457])\n",
      "    Grad:   tensor([-0.0288,  0.1632])\n",
      "Epoch 1710, Loss 3.008215\n",
      "    Params: tensor([  5.1986, -16.3473])\n",
      "    Grad:   tensor([-0.0288,  0.1629])\n",
      "Epoch 1711, Loss 3.007941\n",
      "    Params: tensor([  5.1989, -16.3490])\n",
      "    Grad:   tensor([-0.0287,  0.1626])\n",
      "Epoch 1712, Loss 3.007668\n",
      "    Params: tensor([  5.1992, -16.3506])\n",
      "    Grad:   tensor([-0.0287,  0.1624])\n",
      "Epoch 1713, Loss 3.007397\n",
      "    Params: tensor([  5.1994, -16.3522])\n",
      "    Grad:   tensor([-0.0286,  0.1621])\n",
      "Epoch 1714, Loss 3.007126\n",
      "    Params: tensor([  5.1997, -16.3538])\n",
      "    Grad:   tensor([-0.0286,  0.1618])\n",
      "Epoch 1715, Loss 3.006857\n",
      "    Params: tensor([  5.2000, -16.3554])\n",
      "    Grad:   tensor([-0.0285,  0.1615])\n",
      "Epoch 1716, Loss 3.006586\n",
      "    Params: tensor([  5.2003, -16.3570])\n",
      "    Grad:   tensor([-0.0285,  0.1613])\n",
      "Epoch 1717, Loss 3.006318\n",
      "    Params: tensor([  5.2006, -16.3587])\n",
      "    Grad:   tensor([-0.0284,  0.1610])\n",
      "Epoch 1718, Loss 3.006052\n",
      "    Params: tensor([  5.2009, -16.3603])\n",
      "    Grad:   tensor([-0.0284,  0.1607])\n",
      "Epoch 1719, Loss 3.005785\n",
      "    Params: tensor([  5.2012, -16.3619])\n",
      "    Grad:   tensor([-0.0284,  0.1604])\n",
      "Epoch 1720, Loss 3.005521\n",
      "    Params: tensor([  5.2014, -16.3635])\n",
      "    Grad:   tensor([-0.0283,  0.1602])\n",
      "Epoch 1721, Loss 3.005256\n",
      "    Params: tensor([  5.2017, -16.3651])\n",
      "    Grad:   tensor([-0.0283,  0.1599])\n",
      "Epoch 1722, Loss 3.004993\n",
      "    Params: tensor([  5.2020, -16.3667])\n",
      "    Grad:   tensor([-0.0282,  0.1596])\n",
      "Epoch 1723, Loss 3.004729\n",
      "    Params: tensor([  5.2023, -16.3683])\n",
      "    Grad:   tensor([-0.0281,  0.1594])\n",
      "Epoch 1724, Loss 3.004467\n",
      "    Params: tensor([  5.2026, -16.3699])\n",
      "    Grad:   tensor([-0.0281,  0.1591])\n",
      "Epoch 1725, Loss 3.004207\n",
      "    Params: tensor([  5.2028, -16.3714])\n",
      "    Grad:   tensor([-0.0280,  0.1588])\n",
      "Epoch 1726, Loss 3.003947\n",
      "    Params: tensor([  5.2031, -16.3730])\n",
      "    Grad:   tensor([-0.0280,  0.1586])\n",
      "Epoch 1727, Loss 3.003690\n",
      "    Params: tensor([  5.2034, -16.3746])\n",
      "    Grad:   tensor([-0.0280,  0.1583])\n",
      "Epoch 1728, Loss 3.003431\n",
      "    Params: tensor([  5.2037, -16.3762])\n",
      "    Grad:   tensor([-0.0279,  0.1580])\n",
      "Epoch 1729, Loss 3.003174\n",
      "    Params: tensor([  5.2040, -16.3778])\n",
      "    Grad:   tensor([-0.0279,  0.1577])\n",
      "Epoch 1730, Loss 3.002918\n",
      "    Params: tensor([  5.2042, -16.3793])\n",
      "    Grad:   tensor([-0.0278,  0.1575])\n",
      "Epoch 1731, Loss 3.002661\n",
      "    Params: tensor([  5.2045, -16.3809])\n",
      "    Grad:   tensor([-0.0278,  0.1572])\n",
      "Epoch 1732, Loss 3.002406\n",
      "    Params: tensor([  5.2048, -16.3825])\n",
      "    Grad:   tensor([-0.0277,  0.1569])\n",
      "Epoch 1733, Loss 3.002152\n",
      "    Params: tensor([  5.2051, -16.3840])\n",
      "    Grad:   tensor([-0.0277,  0.1567])\n",
      "Epoch 1734, Loss 3.001901\n",
      "    Params: tensor([  5.2053, -16.3856])\n",
      "    Grad:   tensor([-0.0276,  0.1564])\n",
      "Epoch 1735, Loss 3.001649\n",
      "    Params: tensor([  5.2056, -16.3872])\n",
      "    Grad:   tensor([-0.0276,  0.1561])\n",
      "Epoch 1736, Loss 3.001395\n",
      "    Params: tensor([  5.2059, -16.3887])\n",
      "    Grad:   tensor([-0.0275,  0.1559])\n",
      "Epoch 1737, Loss 3.001145\n",
      "    Params: tensor([  5.2062, -16.3903])\n",
      "    Grad:   tensor([-0.0275,  0.1556])\n",
      "Epoch 1738, Loss 3.000898\n",
      "    Params: tensor([  5.2064, -16.3918])\n",
      "    Grad:   tensor([-0.0274,  0.1553])\n",
      "Epoch 1739, Loss 3.000648\n",
      "    Params: tensor([  5.2067, -16.3934])\n",
      "    Grad:   tensor([-0.0274,  0.1551])\n",
      "Epoch 1740, Loss 3.000400\n",
      "    Params: tensor([  5.2070, -16.3949])\n",
      "    Grad:   tensor([-0.0273,  0.1548])\n",
      "Epoch 1741, Loss 3.000154\n",
      "    Params: tensor([  5.2073, -16.3965])\n",
      "    Grad:   tensor([-0.0273,  0.1546])\n",
      "Epoch 1742, Loss 2.999907\n",
      "    Params: tensor([  5.2075, -16.3980])\n",
      "    Grad:   tensor([-0.0273,  0.1543])\n",
      "Epoch 1743, Loss 2.999662\n",
      "    Params: tensor([  5.2078, -16.3996])\n",
      "    Grad:   tensor([-0.0272,  0.1540])\n",
      "Epoch 1744, Loss 2.999417\n",
      "    Params: tensor([  5.2081, -16.4011])\n",
      "    Grad:   tensor([-0.0272,  0.1538])\n",
      "Epoch 1745, Loss 2.999174\n",
      "    Params: tensor([  5.2084, -16.4026])\n",
      "    Grad:   tensor([-0.0271,  0.1535])\n",
      "Epoch 1746, Loss 2.998930\n",
      "    Params: tensor([  5.2086, -16.4042])\n",
      "    Grad:   tensor([-0.0271,  0.1533])\n",
      "Epoch 1747, Loss 2.998688\n",
      "    Params: tensor([  5.2089, -16.4057])\n",
      "    Grad:   tensor([-0.0270,  0.1530])\n",
      "Epoch 1748, Loss 2.998448\n",
      "    Params: tensor([  5.2092, -16.4072])\n",
      "    Grad:   tensor([-0.0270,  0.1527])\n",
      "Epoch 1749, Loss 2.998208\n",
      "    Params: tensor([  5.2094, -16.4088])\n",
      "    Grad:   tensor([-0.0269,  0.1525])\n",
      "Epoch 1750, Loss 2.997968\n",
      "    Params: tensor([  5.2097, -16.4103])\n",
      "    Grad:   tensor([-0.0269,  0.1522])\n",
      "Epoch 1751, Loss 2.997730\n",
      "    Params: tensor([  5.2100, -16.4118])\n",
      "    Grad:   tensor([-0.0268,  0.1520])\n",
      "Epoch 1752, Loss 2.997490\n",
      "    Params: tensor([  5.2102, -16.4133])\n",
      "    Grad:   tensor([-0.0268,  0.1517])\n",
      "Epoch 1753, Loss 2.997254\n",
      "    Params: tensor([  5.2105, -16.4148])\n",
      "    Grad:   tensor([-0.0267,  0.1514])\n",
      "Epoch 1754, Loss 2.997018\n",
      "    Params: tensor([  5.2108, -16.4163])\n",
      "    Grad:   tensor([-0.0267,  0.1512])\n",
      "Epoch 1755, Loss 2.996782\n",
      "    Params: tensor([  5.2110, -16.4179])\n",
      "    Grad:   tensor([-0.0266,  0.1509])\n",
      "Epoch 1756, Loss 2.996548\n",
      "    Params: tensor([  5.2113, -16.4194])\n",
      "    Grad:   tensor([-0.0266,  0.1507])\n",
      "Epoch 1757, Loss 2.996313\n",
      "    Params: tensor([  5.2116, -16.4209])\n",
      "    Grad:   tensor([-0.0266,  0.1504])\n",
      "Epoch 1758, Loss 2.996081\n",
      "    Params: tensor([  5.2118, -16.4224])\n",
      "    Grad:   tensor([-0.0265,  0.1502])\n",
      "Epoch 1759, Loss 2.995847\n",
      "    Params: tensor([  5.2121, -16.4239])\n",
      "    Grad:   tensor([-0.0265,  0.1499])\n",
      "Epoch 1760, Loss 2.995615\n",
      "    Params: tensor([  5.2124, -16.4254])\n",
      "    Grad:   tensor([-0.0264,  0.1496])\n",
      "Epoch 1761, Loss 2.995387\n",
      "    Params: tensor([  5.2126, -16.4269])\n",
      "    Grad:   tensor([-0.0264,  0.1494])\n",
      "Epoch 1762, Loss 2.995156\n",
      "    Params: tensor([  5.2129, -16.4283])\n",
      "    Grad:   tensor([-0.0263,  0.1491])\n",
      "Epoch 1763, Loss 2.994928\n",
      "    Params: tensor([  5.2132, -16.4298])\n",
      "    Grad:   tensor([-0.0263,  0.1489])\n",
      "Epoch 1764, Loss 2.994699\n",
      "    Params: tensor([  5.2134, -16.4313])\n",
      "    Grad:   tensor([-0.0263,  0.1486])\n",
      "Epoch 1765, Loss 2.994471\n",
      "    Params: tensor([  5.2137, -16.4328])\n",
      "    Grad:   tensor([-0.0262,  0.1484])\n",
      "Epoch 1766, Loss 2.994245\n",
      "    Params: tensor([  5.2139, -16.4343])\n",
      "    Grad:   tensor([-0.0262,  0.1481])\n",
      "Epoch 1767, Loss 2.994019\n",
      "    Params: tensor([  5.2142, -16.4358])\n",
      "    Grad:   tensor([-0.0261,  0.1479])\n",
      "Epoch 1768, Loss 2.993794\n",
      "    Params: tensor([  5.2145, -16.4372])\n",
      "    Grad:   tensor([-0.0261,  0.1476])\n",
      "Epoch 1769, Loss 2.993569\n",
      "    Params: tensor([  5.2147, -16.4387])\n",
      "    Grad:   tensor([-0.0260,  0.1474])\n",
      "Epoch 1770, Loss 2.993344\n",
      "    Params: tensor([  5.2150, -16.4402])\n",
      "    Grad:   tensor([-0.0260,  0.1471])\n",
      "Epoch 1771, Loss 2.993121\n",
      "    Params: tensor([  5.2152, -16.4417])\n",
      "    Grad:   tensor([-0.0260,  0.1469])\n",
      "Epoch 1772, Loss 2.992900\n",
      "    Params: tensor([  5.2155, -16.4431])\n",
      "    Grad:   tensor([-0.0259,  0.1466])\n",
      "Epoch 1773, Loss 2.992678\n",
      "    Params: tensor([  5.2158, -16.4446])\n",
      "    Grad:   tensor([-0.0259,  0.1464])\n",
      "Epoch 1774, Loss 2.992457\n",
      "    Params: tensor([  5.2160, -16.4460])\n",
      "    Grad:   tensor([-0.0258,  0.1461])\n",
      "Epoch 1775, Loss 2.992237\n",
      "    Params: tensor([  5.2163, -16.4475])\n",
      "    Grad:   tensor([-0.0258,  0.1459])\n",
      "Epoch 1776, Loss 2.992017\n",
      "    Params: tensor([  5.2165, -16.4490])\n",
      "    Grad:   tensor([-0.0257,  0.1456])\n",
      "Epoch 1777, Loss 2.991798\n",
      "    Params: tensor([  5.2168, -16.4504])\n",
      "    Grad:   tensor([-0.0257,  0.1454])\n",
      "Epoch 1778, Loss 2.991582\n",
      "    Params: tensor([  5.2170, -16.4519])\n",
      "    Grad:   tensor([-0.0256,  0.1451])\n",
      "Epoch 1779, Loss 2.991366\n",
      "    Params: tensor([  5.2173, -16.4533])\n",
      "    Grad:   tensor([-0.0256,  0.1449])\n",
      "Epoch 1780, Loss 2.991146\n",
      "    Params: tensor([  5.2176, -16.4548])\n",
      "    Grad:   tensor([-0.0256,  0.1446])\n",
      "Epoch 1781, Loss 2.990932\n",
      "    Params: tensor([  5.2178, -16.4562])\n",
      "    Grad:   tensor([-0.0255,  0.1444])\n",
      "Epoch 1782, Loss 2.990719\n",
      "    Params: tensor([  5.2181, -16.4576])\n",
      "    Grad:   tensor([-0.0255,  0.1442])\n",
      "Epoch 1783, Loss 2.990503\n",
      "    Params: tensor([  5.2183, -16.4591])\n",
      "    Grad:   tensor([-0.0254,  0.1439])\n",
      "Epoch 1784, Loss 2.990288\n",
      "    Params: tensor([  5.2186, -16.4605])\n",
      "    Grad:   tensor([-0.0254,  0.1437])\n",
      "Epoch 1785, Loss 2.990078\n",
      "    Params: tensor([  5.2188, -16.4620])\n",
      "    Grad:   tensor([-0.0253,  0.1434])\n",
      "Epoch 1786, Loss 2.989866\n",
      "    Params: tensor([  5.2191, -16.4634])\n",
      "    Grad:   tensor([-0.0253,  0.1432])\n",
      "Epoch 1787, Loss 2.989655\n",
      "    Params: tensor([  5.2193, -16.4648])\n",
      "    Grad:   tensor([-0.0252,  0.1429])\n",
      "Epoch 1788, Loss 2.989443\n",
      "    Params: tensor([  5.2196, -16.4662])\n",
      "    Grad:   tensor([-0.0252,  0.1427])\n",
      "Epoch 1789, Loss 2.989233\n",
      "    Params: tensor([  5.2198, -16.4677])\n",
      "    Grad:   tensor([-0.0252,  0.1424])\n",
      "Epoch 1790, Loss 2.989025\n",
      "    Params: tensor([  5.2201, -16.4691])\n",
      "    Grad:   tensor([-0.0251,  0.1422])\n",
      "Epoch 1791, Loss 2.988817\n",
      "    Params: tensor([  5.2203, -16.4705])\n",
      "    Grad:   tensor([-0.0251,  0.1420])\n",
      "Epoch 1792, Loss 2.988609\n",
      "    Params: tensor([  5.2206, -16.4719])\n",
      "    Grad:   tensor([-0.0250,  0.1417])\n",
      "Epoch 1793, Loss 2.988401\n",
      "    Params: tensor([  5.2208, -16.4733])\n",
      "    Grad:   tensor([-0.0250,  0.1415])\n",
      "Epoch 1794, Loss 2.988195\n",
      "    Params: tensor([  5.2211, -16.4748])\n",
      "    Grad:   tensor([-0.0249,  0.1412])\n",
      "Epoch 1795, Loss 2.987989\n",
      "    Params: tensor([  5.2213, -16.4762])\n",
      "    Grad:   tensor([-0.0249,  0.1410])\n",
      "Epoch 1796, Loss 2.987785\n",
      "    Params: tensor([  5.2216, -16.4776])\n",
      "    Grad:   tensor([-0.0249,  0.1408])\n",
      "Epoch 1797, Loss 2.987582\n",
      "    Params: tensor([  5.2218, -16.4790])\n",
      "    Grad:   tensor([-0.0248,  0.1405])\n",
      "Epoch 1798, Loss 2.987377\n",
      "    Params: tensor([  5.2221, -16.4804])\n",
      "    Grad:   tensor([-0.0248,  0.1403])\n",
      "Epoch 1799, Loss 2.987174\n",
      "    Params: tensor([  5.2223, -16.4818])\n",
      "    Grad:   tensor([-0.0247,  0.1400])\n",
      "Epoch 1800, Loss 2.986974\n",
      "    Params: tensor([  5.2226, -16.4832])\n",
      "    Grad:   tensor([-0.0247,  0.1398])\n",
      "Epoch 1801, Loss 2.986771\n",
      "    Params: tensor([  5.2228, -16.4846])\n",
      "    Grad:   tensor([-0.0246,  0.1396])\n",
      "Epoch 1802, Loss 2.986570\n",
      "    Params: tensor([  5.2231, -16.4860])\n",
      "    Grad:   tensor([-0.0246,  0.1393])\n",
      "Epoch 1803, Loss 2.986371\n",
      "    Params: tensor([  5.2233, -16.4874])\n",
      "    Grad:   tensor([-0.0246,  0.1391])\n",
      "Epoch 1804, Loss 2.986171\n",
      "    Params: tensor([  5.2236, -16.4888])\n",
      "    Grad:   tensor([-0.0245,  0.1389])\n",
      "Epoch 1805, Loss 2.985972\n",
      "    Params: tensor([  5.2238, -16.4901])\n",
      "    Grad:   tensor([-0.0245,  0.1386])\n",
      "Epoch 1806, Loss 2.985774\n",
      "    Params: tensor([  5.2241, -16.4915])\n",
      "    Grad:   tensor([-0.0245,  0.1384])\n",
      "Epoch 1807, Loss 2.985578\n",
      "    Params: tensor([  5.2243, -16.4929])\n",
      "    Grad:   tensor([-0.0244,  0.1382])\n",
      "Epoch 1808, Loss 2.985381\n",
      "    Params: tensor([  5.2245, -16.4943])\n",
      "    Grad:   tensor([-0.0244,  0.1379])\n",
      "Epoch 1809, Loss 2.985184\n",
      "    Params: tensor([  5.2248, -16.4957])\n",
      "    Grad:   tensor([-0.0243,  0.1377])\n",
      "Epoch 1810, Loss 2.984989\n",
      "    Params: tensor([  5.2250, -16.4970])\n",
      "    Grad:   tensor([-0.0243,  0.1374])\n",
      "Epoch 1811, Loss 2.984793\n",
      "    Params: tensor([  5.2253, -16.4984])\n",
      "    Grad:   tensor([-0.0243,  0.1372])\n",
      "Epoch 1812, Loss 2.984601\n",
      "    Params: tensor([  5.2255, -16.4998])\n",
      "    Grad:   tensor([-0.0242,  0.1370])\n",
      "Epoch 1813, Loss 2.984407\n",
      "    Params: tensor([  5.2258, -16.5011])\n",
      "    Grad:   tensor([-0.0242,  0.1368])\n",
      "Epoch 1814, Loss 2.984215\n",
      "    Params: tensor([  5.2260, -16.5025])\n",
      "    Grad:   tensor([-0.0241,  0.1365])\n",
      "Epoch 1815, Loss 2.984022\n",
      "    Params: tensor([  5.2262, -16.5039])\n",
      "    Grad:   tensor([-0.0241,  0.1363])\n",
      "Epoch 1816, Loss 2.983831\n",
      "    Params: tensor([  5.2265, -16.5052])\n",
      "    Grad:   tensor([-0.0240,  0.1361])\n",
      "Epoch 1817, Loss 2.983639\n",
      "    Params: tensor([  5.2267, -16.5066])\n",
      "    Grad:   tensor([-0.0240,  0.1358])\n",
      "Epoch 1818, Loss 2.983449\n",
      "    Params: tensor([  5.2270, -16.5079])\n",
      "    Grad:   tensor([-0.0239,  0.1356])\n",
      "Epoch 1819, Loss 2.983259\n",
      "    Params: tensor([  5.2272, -16.5093])\n",
      "    Grad:   tensor([-0.0239,  0.1354])\n",
      "Epoch 1820, Loss 2.983073\n",
      "    Params: tensor([  5.2274, -16.5107])\n",
      "    Grad:   tensor([-0.0239,  0.1351])\n",
      "Epoch 1821, Loss 2.982884\n",
      "    Params: tensor([  5.2277, -16.5120])\n",
      "    Grad:   tensor([-0.0238,  0.1349])\n",
      "Epoch 1822, Loss 2.982697\n",
      "    Params: tensor([  5.2279, -16.5133])\n",
      "    Grad:   tensor([-0.0238,  0.1347])\n",
      "Epoch 1823, Loss 2.982510\n",
      "    Params: tensor([  5.2281, -16.5147])\n",
      "    Grad:   tensor([-0.0237,  0.1344])\n",
      "Epoch 1824, Loss 2.982322\n",
      "    Params: tensor([  5.2284, -16.5160])\n",
      "    Grad:   tensor([-0.0237,  0.1342])\n",
      "Epoch 1825, Loss 2.982137\n",
      "    Params: tensor([  5.2286, -16.5174])\n",
      "    Grad:   tensor([-0.0237,  0.1340])\n",
      "Epoch 1826, Loss 2.981953\n",
      "    Params: tensor([  5.2289, -16.5187])\n",
      "    Grad:   tensor([-0.0236,  0.1338])\n",
      "Epoch 1827, Loss 2.981769\n",
      "    Params: tensor([  5.2291, -16.5200])\n",
      "    Grad:   tensor([-0.0236,  0.1335])\n",
      "Epoch 1828, Loss 2.981586\n",
      "    Params: tensor([  5.2293, -16.5214])\n",
      "    Grad:   tensor([-0.0236,  0.1333])\n",
      "Epoch 1829, Loss 2.981402\n",
      "    Params: tensor([  5.2296, -16.5227])\n",
      "    Grad:   tensor([-0.0235,  0.1331])\n",
      "Epoch 1830, Loss 2.981219\n",
      "    Params: tensor([  5.2298, -16.5240])\n",
      "    Grad:   tensor([-0.0235,  0.1329])\n",
      "Epoch 1831, Loss 2.981037\n",
      "    Params: tensor([  5.2300, -16.5254])\n",
      "    Grad:   tensor([-0.0235,  0.1326])\n",
      "Epoch 1832, Loss 2.980856\n",
      "    Params: tensor([  5.2303, -16.5267])\n",
      "    Grad:   tensor([-0.0234,  0.1324])\n",
      "Epoch 1833, Loss 2.980675\n",
      "    Params: tensor([  5.2305, -16.5280])\n",
      "    Grad:   tensor([-0.0234,  0.1322])\n",
      "Epoch 1834, Loss 2.980495\n",
      "    Params: tensor([  5.2307, -16.5293])\n",
      "    Grad:   tensor([-0.0233,  0.1320])\n",
      "Epoch 1835, Loss 2.980315\n",
      "    Params: tensor([  5.2310, -16.5306])\n",
      "    Grad:   tensor([-0.0233,  0.1317])\n",
      "Epoch 1836, Loss 2.980137\n",
      "    Params: tensor([  5.2312, -16.5320])\n",
      "    Grad:   tensor([-0.0232,  0.1315])\n",
      "Epoch 1837, Loss 2.979958\n",
      "    Params: tensor([  5.2314, -16.5333])\n",
      "    Grad:   tensor([-0.0232,  0.1313])\n",
      "Epoch 1838, Loss 2.979782\n",
      "    Params: tensor([  5.2317, -16.5346])\n",
      "    Grad:   tensor([-0.0232,  0.1311])\n",
      "Epoch 1839, Loss 2.979604\n",
      "    Params: tensor([  5.2319, -16.5359])\n",
      "    Grad:   tensor([-0.0231,  0.1308])\n",
      "Epoch 1840, Loss 2.979428\n",
      "    Params: tensor([  5.2321, -16.5372])\n",
      "    Grad:   tensor([-0.0231,  0.1306])\n",
      "Epoch 1841, Loss 2.979253\n",
      "    Params: tensor([  5.2324, -16.5385])\n",
      "    Grad:   tensor([-0.0230,  0.1304])\n",
      "Epoch 1842, Loss 2.979078\n",
      "    Params: tensor([  5.2326, -16.5398])\n",
      "    Grad:   tensor([-0.0230,  0.1302])\n",
      "Epoch 1843, Loss 2.978902\n",
      "    Params: tensor([  5.2328, -16.5411])\n",
      "    Grad:   tensor([-0.0229,  0.1300])\n",
      "Epoch 1844, Loss 2.978729\n",
      "    Params: tensor([  5.2330, -16.5424])\n",
      "    Grad:   tensor([-0.0229,  0.1297])\n",
      "Epoch 1845, Loss 2.978556\n",
      "    Params: tensor([  5.2333, -16.5437])\n",
      "    Grad:   tensor([-0.0229,  0.1295])\n",
      "Epoch 1846, Loss 2.978382\n",
      "    Params: tensor([  5.2335, -16.5450])\n",
      "    Grad:   tensor([-0.0228,  0.1293])\n",
      "Epoch 1847, Loss 2.978211\n",
      "    Params: tensor([  5.2337, -16.5463])\n",
      "    Grad:   tensor([-0.0228,  0.1291])\n",
      "Epoch 1848, Loss 2.978039\n",
      "    Params: tensor([  5.2340, -16.5476])\n",
      "    Grad:   tensor([-0.0228,  0.1288])\n",
      "Epoch 1849, Loss 2.977867\n",
      "    Params: tensor([  5.2342, -16.5489])\n",
      "    Grad:   tensor([-0.0227,  0.1286])\n",
      "Epoch 1850, Loss 2.977696\n",
      "    Params: tensor([  5.2344, -16.5501])\n",
      "    Grad:   tensor([-0.0227,  0.1284])\n",
      "Epoch 1851, Loss 2.977527\n",
      "    Params: tensor([  5.2346, -16.5514])\n",
      "    Grad:   tensor([-0.0227,  0.1282])\n",
      "Epoch 1852, Loss 2.977357\n",
      "    Params: tensor([  5.2349, -16.5527])\n",
      "    Grad:   tensor([-0.0226,  0.1280])\n",
      "Epoch 1853, Loss 2.977188\n",
      "    Params: tensor([  5.2351, -16.5540])\n",
      "    Grad:   tensor([-0.0226,  0.1278])\n",
      "Epoch 1854, Loss 2.977021\n",
      "    Params: tensor([  5.2353, -16.5553])\n",
      "    Grad:   tensor([-0.0225,  0.1275])\n",
      "Epoch 1855, Loss 2.976853\n",
      "    Params: tensor([  5.2355, -16.5565])\n",
      "    Grad:   tensor([-0.0225,  0.1273])\n",
      "Epoch 1856, Loss 2.976687\n",
      "    Params: tensor([  5.2358, -16.5578])\n",
      "    Grad:   tensor([-0.0225,  0.1271])\n",
      "Epoch 1857, Loss 2.976520\n",
      "    Params: tensor([  5.2360, -16.5591])\n",
      "    Grad:   tensor([-0.0224,  0.1269])\n",
      "Epoch 1858, Loss 2.976354\n",
      "    Params: tensor([  5.2362, -16.5603])\n",
      "    Grad:   tensor([-0.0224,  0.1267])\n",
      "Epoch 1859, Loss 2.976189\n",
      "    Params: tensor([  5.2364, -16.5616])\n",
      "    Grad:   tensor([-0.0223,  0.1265])\n",
      "Epoch 1860, Loss 2.976023\n",
      "    Params: tensor([  5.2367, -16.5629])\n",
      "    Grad:   tensor([-0.0223,  0.1263])\n",
      "Epoch 1861, Loss 2.975860\n",
      "    Params: tensor([  5.2369, -16.5641])\n",
      "    Grad:   tensor([-0.0223,  0.1260])\n",
      "Epoch 1862, Loss 2.975697\n",
      "    Params: tensor([  5.2371, -16.5654])\n",
      "    Grad:   tensor([-0.0222,  0.1258])\n",
      "Epoch 1863, Loss 2.975533\n",
      "    Params: tensor([  5.2373, -16.5666])\n",
      "    Grad:   tensor([-0.0222,  0.1256])\n",
      "Epoch 1864, Loss 2.975369\n",
      "    Params: tensor([  5.2375, -16.5679])\n",
      "    Grad:   tensor([-0.0222,  0.1254])\n",
      "Epoch 1865, Loss 2.975208\n",
      "    Params: tensor([  5.2378, -16.5691])\n",
      "    Grad:   tensor([-0.0221,  0.1252])\n",
      "Epoch 1866, Loss 2.975046\n",
      "    Params: tensor([  5.2380, -16.5704])\n",
      "    Grad:   tensor([-0.0221,  0.1250])\n",
      "Epoch 1867, Loss 2.974886\n",
      "    Params: tensor([  5.2382, -16.5716])\n",
      "    Grad:   tensor([-0.0220,  0.1248])\n",
      "Epoch 1868, Loss 2.974725\n",
      "    Params: tensor([  5.2384, -16.5729])\n",
      "    Grad:   tensor([-0.0220,  0.1245])\n",
      "Epoch 1869, Loss 2.974565\n",
      "    Params: tensor([  5.2386, -16.5741])\n",
      "    Grad:   tensor([-0.0220,  0.1243])\n",
      "Epoch 1870, Loss 2.974406\n",
      "    Params: tensor([  5.2389, -16.5754])\n",
      "    Grad:   tensor([-0.0219,  0.1241])\n",
      "Epoch 1871, Loss 2.974248\n",
      "    Params: tensor([  5.2391, -16.5766])\n",
      "    Grad:   tensor([-0.0219,  0.1239])\n",
      "Epoch 1872, Loss 2.974088\n",
      "    Params: tensor([  5.2393, -16.5778])\n",
      "    Grad:   tensor([-0.0219,  0.1237])\n",
      "Epoch 1873, Loss 2.973930\n",
      "    Params: tensor([  5.2395, -16.5791])\n",
      "    Grad:   tensor([-0.0218,  0.1235])\n",
      "Epoch 1874, Loss 2.973776\n",
      "    Params: tensor([  5.2397, -16.5803])\n",
      "    Grad:   tensor([-0.0218,  0.1233])\n",
      "Epoch 1875, Loss 2.973618\n",
      "    Params: tensor([  5.2400, -16.5815])\n",
      "    Grad:   tensor([-0.0217,  0.1231])\n",
      "Epoch 1876, Loss 2.973463\n",
      "    Params: tensor([  5.2402, -16.5828])\n",
      "    Grad:   tensor([-0.0217,  0.1229])\n",
      "Epoch 1877, Loss 2.973307\n",
      "    Params: tensor([  5.2404, -16.5840])\n",
      "    Grad:   tensor([-0.0217,  0.1227])\n",
      "Epoch 1878, Loss 2.973151\n",
      "    Params: tensor([  5.2406, -16.5852])\n",
      "    Grad:   tensor([-0.0216,  0.1224])\n",
      "Epoch 1879, Loss 2.972996\n",
      "    Params: tensor([  5.2408, -16.5864])\n",
      "    Grad:   tensor([-0.0216,  0.1222])\n",
      "Epoch 1880, Loss 2.972843\n",
      "    Params: tensor([  5.2410, -16.5877])\n",
      "    Grad:   tensor([-0.0215,  0.1220])\n",
      "Epoch 1881, Loss 2.972690\n",
      "    Params: tensor([  5.2413, -16.5889])\n",
      "    Grad:   tensor([-0.0215,  0.1218])\n",
      "Epoch 1882, Loss 2.972536\n",
      "    Params: tensor([  5.2415, -16.5901])\n",
      "    Grad:   tensor([-0.0215,  0.1216])\n",
      "Epoch 1883, Loss 2.972383\n",
      "    Params: tensor([  5.2417, -16.5913])\n",
      "    Grad:   tensor([-0.0214,  0.1214])\n",
      "Epoch 1884, Loss 2.972232\n",
      "    Params: tensor([  5.2419, -16.5925])\n",
      "    Grad:   tensor([-0.0214,  0.1212])\n",
      "Epoch 1885, Loss 2.972081\n",
      "    Params: tensor([  5.2421, -16.5937])\n",
      "    Grad:   tensor([-0.0214,  0.1210])\n",
      "Epoch 1886, Loss 2.971931\n",
      "    Params: tensor([  5.2423, -16.5949])\n",
      "    Grad:   tensor([-0.0213,  0.1208])\n",
      "Epoch 1887, Loss 2.971780\n",
      "    Params: tensor([  5.2425, -16.5961])\n",
      "    Grad:   tensor([-0.0213,  0.1206])\n",
      "Epoch 1888, Loss 2.971630\n",
      "    Params: tensor([  5.2427, -16.5974])\n",
      "    Grad:   tensor([-0.0213,  0.1204])\n",
      "Epoch 1889, Loss 2.971481\n",
      "    Params: tensor([  5.2430, -16.5986])\n",
      "    Grad:   tensor([-0.0212,  0.1202])\n",
      "Epoch 1890, Loss 2.971332\n",
      "    Params: tensor([  5.2432, -16.5998])\n",
      "    Grad:   tensor([-0.0212,  0.1200])\n",
      "Epoch 1891, Loss 2.971184\n",
      "    Params: tensor([  5.2434, -16.6010])\n",
      "    Grad:   tensor([-0.0212,  0.1198])\n",
      "Epoch 1892, Loss 2.971035\n",
      "    Params: tensor([  5.2436, -16.6021])\n",
      "    Grad:   tensor([-0.0211,  0.1196])\n",
      "Epoch 1893, Loss 2.970888\n",
      "    Params: tensor([  5.2438, -16.6033])\n",
      "    Grad:   tensor([-0.0211,  0.1194])\n",
      "Epoch 1894, Loss 2.970741\n",
      "    Params: tensor([  5.2440, -16.6045])\n",
      "    Grad:   tensor([-0.0211,  0.1192])\n",
      "Epoch 1895, Loss 2.970596\n",
      "    Params: tensor([  5.2442, -16.6057])\n",
      "    Grad:   tensor([-0.0210,  0.1190])\n",
      "Epoch 1896, Loss 2.970449\n",
      "    Params: tensor([  5.2444, -16.6069])\n",
      "    Grad:   tensor([-0.0210,  0.1188])\n",
      "Epoch 1897, Loss 2.970304\n",
      "    Params: tensor([  5.2446, -16.6081])\n",
      "    Grad:   tensor([-0.0209,  0.1186])\n",
      "Epoch 1898, Loss 2.970159\n",
      "    Params: tensor([  5.2449, -16.6093])\n",
      "    Grad:   tensor([-0.0209,  0.1183])\n",
      "Epoch 1899, Loss 2.970016\n",
      "    Params: tensor([  5.2451, -16.6105])\n",
      "    Grad:   tensor([-0.0209,  0.1182])\n",
      "Epoch 1900, Loss 2.969871\n",
      "    Params: tensor([  5.2453, -16.6116])\n",
      "    Grad:   tensor([-0.0208,  0.1180])\n",
      "Epoch 1901, Loss 2.969727\n",
      "    Params: tensor([  5.2455, -16.6128])\n",
      "    Grad:   tensor([-0.0208,  0.1178])\n",
      "Epoch 1902, Loss 2.969586\n",
      "    Params: tensor([  5.2457, -16.6140])\n",
      "    Grad:   tensor([-0.0208,  0.1175])\n",
      "Epoch 1903, Loss 2.969443\n",
      "    Params: tensor([  5.2459, -16.6152])\n",
      "    Grad:   tensor([-0.0207,  0.1173])\n",
      "Epoch 1904, Loss 2.969302\n",
      "    Params: tensor([  5.2461, -16.6163])\n",
      "    Grad:   tensor([-0.0207,  0.1172])\n",
      "Epoch 1905, Loss 2.969160\n",
      "    Params: tensor([  5.2463, -16.6175])\n",
      "    Grad:   tensor([-0.0206,  0.1170])\n",
      "Epoch 1906, Loss 2.969017\n",
      "    Params: tensor([  5.2465, -16.6187])\n",
      "    Grad:   tensor([-0.0206,  0.1168])\n",
      "Epoch 1907, Loss 2.968879\n",
      "    Params: tensor([  5.2467, -16.6198])\n",
      "    Grad:   tensor([-0.0206,  0.1166])\n",
      "Epoch 1908, Loss 2.968739\n",
      "    Params: tensor([  5.2469, -16.6210])\n",
      "    Grad:   tensor([-0.0205,  0.1164])\n",
      "Epoch 1909, Loss 2.968599\n",
      "    Params: tensor([  5.2471, -16.6222])\n",
      "    Grad:   tensor([-0.0205,  0.1162])\n",
      "Epoch 1910, Loss 2.968460\n",
      "    Params: tensor([  5.2473, -16.6233])\n",
      "    Grad:   tensor([-0.0205,  0.1160])\n",
      "Epoch 1911, Loss 2.968321\n",
      "    Params: tensor([  5.2475, -16.6245])\n",
      "    Grad:   tensor([-0.0204,  0.1158])\n",
      "Epoch 1912, Loss 2.968183\n",
      "    Params: tensor([  5.2477, -16.6256])\n",
      "    Grad:   tensor([-0.0204,  0.1156])\n",
      "Epoch 1913, Loss 2.968046\n",
      "    Params: tensor([  5.2479, -16.6268])\n",
      "    Grad:   tensor([-0.0204,  0.1154])\n",
      "Epoch 1914, Loss 2.967908\n",
      "    Params: tensor([  5.2482, -16.6279])\n",
      "    Grad:   tensor([-0.0204,  0.1152])\n",
      "Epoch 1915, Loss 2.967772\n",
      "    Params: tensor([  5.2484, -16.6291])\n",
      "    Grad:   tensor([-0.0203,  0.1150])\n",
      "Epoch 1916, Loss 2.967636\n",
      "    Params: tensor([  5.2486, -16.6302])\n",
      "    Grad:   tensor([-0.0203,  0.1148])\n",
      "Epoch 1917, Loss 2.967499\n",
      "    Params: tensor([  5.2488, -16.6314])\n",
      "    Grad:   tensor([-0.0202,  0.1146])\n",
      "Epoch 1918, Loss 2.967365\n",
      "    Params: tensor([  5.2490, -16.6325])\n",
      "    Grad:   tensor([-0.0202,  0.1144])\n",
      "Epoch 1919, Loss 2.967230\n",
      "    Params: tensor([  5.2492, -16.6337])\n",
      "    Grad:   tensor([-0.0202,  0.1142])\n",
      "Epoch 1920, Loss 2.967095\n",
      "    Params: tensor([  5.2494, -16.6348])\n",
      "    Grad:   tensor([-0.0202,  0.1140])\n",
      "Epoch 1921, Loss 2.966961\n",
      "    Params: tensor([  5.2496, -16.6360])\n",
      "    Grad:   tensor([-0.0201,  0.1138])\n",
      "Epoch 1922, Loss 2.966828\n",
      "    Params: tensor([  5.2498, -16.6371])\n",
      "    Grad:   tensor([-0.0201,  0.1136])\n",
      "Epoch 1923, Loss 2.966693\n",
      "    Params: tensor([  5.2500, -16.6382])\n",
      "    Grad:   tensor([-0.0200,  0.1134])\n",
      "Epoch 1924, Loss 2.966561\n",
      "    Params: tensor([  5.2502, -16.6394])\n",
      "    Grad:   tensor([-0.0200,  0.1132])\n",
      "Epoch 1925, Loss 2.966429\n",
      "    Params: tensor([  5.2504, -16.6405])\n",
      "    Grad:   tensor([-0.0200,  0.1130])\n",
      "Epoch 1926, Loss 2.966297\n",
      "    Params: tensor([  5.2506, -16.6416])\n",
      "    Grad:   tensor([-0.0199,  0.1128])\n",
      "Epoch 1927, Loss 2.966168\n",
      "    Params: tensor([  5.2508, -16.6427])\n",
      "    Grad:   tensor([-0.0199,  0.1127])\n",
      "Epoch 1928, Loss 2.966036\n",
      "    Params: tensor([  5.2510, -16.6439])\n",
      "    Grad:   tensor([-0.0199,  0.1125])\n",
      "Epoch 1929, Loss 2.965904\n",
      "    Params: tensor([  5.2512, -16.6450])\n",
      "    Grad:   tensor([-0.0198,  0.1123])\n",
      "Epoch 1930, Loss 2.965777\n",
      "    Params: tensor([  5.2514, -16.6461])\n",
      "    Grad:   tensor([-0.0198,  0.1121])\n",
      "Epoch 1931, Loss 2.965647\n",
      "    Params: tensor([  5.2516, -16.6472])\n",
      "    Grad:   tensor([-0.0198,  0.1119])\n",
      "Epoch 1932, Loss 2.965516\n",
      "    Params: tensor([  5.2518, -16.6484])\n",
      "    Grad:   tensor([-0.0197,  0.1117])\n",
      "Epoch 1933, Loss 2.965388\n",
      "    Params: tensor([  5.2520, -16.6495])\n",
      "    Grad:   tensor([-0.0197,  0.1115])\n",
      "Epoch 1934, Loss 2.965261\n",
      "    Params: tensor([  5.2522, -16.6506])\n",
      "    Grad:   tensor([-0.0197,  0.1113])\n",
      "Epoch 1935, Loss 2.965131\n",
      "    Params: tensor([  5.2523, -16.6517])\n",
      "    Grad:   tensor([-0.0196,  0.1111])\n",
      "Epoch 1936, Loss 2.965006\n",
      "    Params: tensor([  5.2525, -16.6528])\n",
      "    Grad:   tensor([-0.0196,  0.1109])\n",
      "Epoch 1937, Loss 2.964877\n",
      "    Params: tensor([  5.2527, -16.6539])\n",
      "    Grad:   tensor([-0.0196,  0.1108])\n",
      "Epoch 1938, Loss 2.964751\n",
      "    Params: tensor([  5.2529, -16.6550])\n",
      "    Grad:   tensor([-0.0195,  0.1106])\n",
      "Epoch 1939, Loss 2.964625\n",
      "    Params: tensor([  5.2531, -16.6561])\n",
      "    Grad:   tensor([-0.0195,  0.1104])\n",
      "Epoch 1940, Loss 2.964500\n",
      "    Params: tensor([  5.2533, -16.6572])\n",
      "    Grad:   tensor([-0.0195,  0.1102])\n",
      "Epoch 1941, Loss 2.964375\n",
      "    Params: tensor([  5.2535, -16.6583])\n",
      "    Grad:   tensor([-0.0195,  0.1100])\n",
      "Epoch 1942, Loss 2.964250\n",
      "    Params: tensor([  5.2537, -16.6594])\n",
      "    Grad:   tensor([-0.0194,  0.1098])\n",
      "Epoch 1943, Loss 2.964126\n",
      "    Params: tensor([  5.2539, -16.6605])\n",
      "    Grad:   tensor([-0.0194,  0.1096])\n",
      "Epoch 1944, Loss 2.964001\n",
      "    Params: tensor([  5.2541, -16.6616])\n",
      "    Grad:   tensor([-0.0194,  0.1094])\n",
      "Epoch 1945, Loss 2.963879\n",
      "    Params: tensor([  5.2543, -16.6627])\n",
      "    Grad:   tensor([-0.0193,  0.1093])\n",
      "Epoch 1946, Loss 2.963756\n",
      "    Params: tensor([  5.2545, -16.6638])\n",
      "    Grad:   tensor([-0.0193,  0.1091])\n",
      "Epoch 1947, Loss 2.963632\n",
      "    Params: tensor([  5.2547, -16.6649])\n",
      "    Grad:   tensor([-0.0192,  0.1089])\n",
      "Epoch 1948, Loss 2.963511\n",
      "    Params: tensor([  5.2549, -16.6660])\n",
      "    Grad:   tensor([-0.0192,  0.1087])\n",
      "Epoch 1949, Loss 2.963388\n",
      "    Params: tensor([  5.2551, -16.6671])\n",
      "    Grad:   tensor([-0.0192,  0.1085])\n",
      "Epoch 1950, Loss 2.963266\n",
      "    Params: tensor([  5.2553, -16.6681])\n",
      "    Grad:   tensor([-0.0191,  0.1083])\n",
      "Epoch 1951, Loss 2.963149\n",
      "    Params: tensor([  5.2554, -16.6692])\n",
      "    Grad:   tensor([-0.0191,  0.1081])\n",
      "Epoch 1952, Loss 2.963026\n",
      "    Params: tensor([  5.2556, -16.6703])\n",
      "    Grad:   tensor([-0.0191,  0.1080])\n",
      "Epoch 1953, Loss 2.962907\n",
      "    Params: tensor([  5.2558, -16.6714])\n",
      "    Grad:   tensor([-0.0190,  0.1078])\n",
      "Epoch 1954, Loss 2.962788\n",
      "    Params: tensor([  5.2560, -16.6725])\n",
      "    Grad:   tensor([-0.0190,  0.1076])\n",
      "Epoch 1955, Loss 2.962667\n",
      "    Params: tensor([  5.2562, -16.6735])\n",
      "    Grad:   tensor([-0.0190,  0.1074])\n",
      "Epoch 1956, Loss 2.962547\n",
      "    Params: tensor([  5.2564, -16.6746])\n",
      "    Grad:   tensor([-0.0189,  0.1072])\n",
      "Epoch 1957, Loss 2.962429\n",
      "    Params: tensor([  5.2566, -16.6757])\n",
      "    Grad:   tensor([-0.0189,  0.1071])\n",
      "Epoch 1958, Loss 2.962312\n",
      "    Params: tensor([  5.2568, -16.6767])\n",
      "    Grad:   tensor([-0.0189,  0.1069])\n",
      "Epoch 1959, Loss 2.962195\n",
      "    Params: tensor([  5.2570, -16.6778])\n",
      "    Grad:   tensor([-0.0188,  0.1067])\n",
      "Epoch 1960, Loss 2.962078\n",
      "    Params: tensor([  5.2572, -16.6789])\n",
      "    Grad:   tensor([-0.0188,  0.1065])\n",
      "Epoch 1961, Loss 2.961959\n",
      "    Params: tensor([  5.2573, -16.6799])\n",
      "    Grad:   tensor([-0.0188,  0.1063])\n",
      "Epoch 1962, Loss 2.961843\n",
      "    Params: tensor([  5.2575, -16.6810])\n",
      "    Grad:   tensor([-0.0187,  0.1062])\n",
      "Epoch 1963, Loss 2.961728\n",
      "    Params: tensor([  5.2577, -16.6821])\n",
      "    Grad:   tensor([-0.0187,  0.1060])\n",
      "Epoch 1964, Loss 2.961611\n",
      "    Params: tensor([  5.2579, -16.6831])\n",
      "    Grad:   tensor([-0.0187,  0.1058])\n",
      "Epoch 1965, Loss 2.961496\n",
      "    Params: tensor([  5.2581, -16.6842])\n",
      "    Grad:   tensor([-0.0187,  0.1056])\n",
      "Epoch 1966, Loss 2.961382\n",
      "    Params: tensor([  5.2583, -16.6852])\n",
      "    Grad:   tensor([-0.0186,  0.1054])\n",
      "Epoch 1967, Loss 2.961267\n",
      "    Params: tensor([  5.2585, -16.6863])\n",
      "    Grad:   tensor([-0.0186,  0.1052])\n",
      "Epoch 1968, Loss 2.961153\n",
      "    Params: tensor([  5.2586, -16.6873])\n",
      "    Grad:   tensor([-0.0186,  0.1051])\n",
      "Epoch 1969, Loss 2.961038\n",
      "    Params: tensor([  5.2588, -16.6884])\n",
      "    Grad:   tensor([-0.0185,  0.1049])\n",
      "Epoch 1970, Loss 2.960926\n",
      "    Params: tensor([  5.2590, -16.6894])\n",
      "    Grad:   tensor([-0.0185,  0.1047])\n",
      "Epoch 1971, Loss 2.960813\n",
      "    Params: tensor([  5.2592, -16.6905])\n",
      "    Grad:   tensor([-0.0185,  0.1045])\n",
      "Epoch 1972, Loss 2.960700\n",
      "    Params: tensor([  5.2594, -16.6915])\n",
      "    Grad:   tensor([-0.0184,  0.1044])\n",
      "Epoch 1973, Loss 2.960587\n",
      "    Params: tensor([  5.2596, -16.6926])\n",
      "    Grad:   tensor([-0.0184,  0.1042])\n",
      "Epoch 1974, Loss 2.960475\n",
      "    Params: tensor([  5.2598, -16.6936])\n",
      "    Grad:   tensor([-0.0184,  0.1040])\n",
      "Epoch 1975, Loss 2.960365\n",
      "    Params: tensor([  5.2599, -16.6946])\n",
      "    Grad:   tensor([-0.0183,  0.1038])\n",
      "Epoch 1976, Loss 2.960255\n",
      "    Params: tensor([  5.2601, -16.6957])\n",
      "    Grad:   tensor([-0.0183,  0.1037])\n",
      "Epoch 1977, Loss 2.960143\n",
      "    Params: tensor([  5.2603, -16.6967])\n",
      "    Grad:   tensor([-0.0183,  0.1035])\n",
      "Epoch 1978, Loss 2.960033\n",
      "    Params: tensor([  5.2605, -16.6977])\n",
      "    Grad:   tensor([-0.0182,  0.1033])\n",
      "Epoch 1979, Loss 2.959923\n",
      "    Params: tensor([  5.2607, -16.6988])\n",
      "    Grad:   tensor([-0.0182,  0.1031])\n",
      "Epoch 1980, Loss 2.959812\n",
      "    Params: tensor([  5.2608, -16.6998])\n",
      "    Grad:   tensor([-0.0182,  0.1029])\n",
      "Epoch 1981, Loss 2.959703\n",
      "    Params: tensor([  5.2610, -16.7008])\n",
      "    Grad:   tensor([-0.0182,  0.1028])\n",
      "Epoch 1982, Loss 2.959594\n",
      "    Params: tensor([  5.2612, -16.7019])\n",
      "    Grad:   tensor([-0.0181,  0.1026])\n",
      "Epoch 1983, Loss 2.959486\n",
      "    Params: tensor([  5.2614, -16.7029])\n",
      "    Grad:   tensor([-0.0181,  0.1024])\n",
      "Epoch 1984, Loss 2.959378\n",
      "    Params: tensor([  5.2616, -16.7039])\n",
      "    Grad:   tensor([-0.0181,  0.1022])\n",
      "Epoch 1985, Loss 2.959271\n",
      "    Params: tensor([  5.2618, -16.7049])\n",
      "    Grad:   tensor([-0.0180,  0.1021])\n",
      "Epoch 1986, Loss 2.959162\n",
      "    Params: tensor([  5.2619, -16.7059])\n",
      "    Grad:   tensor([-0.0180,  0.1019])\n",
      "Epoch 1987, Loss 2.959055\n",
      "    Params: tensor([  5.2621, -16.7070])\n",
      "    Grad:   tensor([-0.0180,  0.1017])\n",
      "Epoch 1988, Loss 2.958950\n",
      "    Params: tensor([  5.2623, -16.7080])\n",
      "    Grad:   tensor([-0.0179,  0.1016])\n",
      "Epoch 1989, Loss 2.958842\n",
      "    Params: tensor([  5.2625, -16.7090])\n",
      "    Grad:   tensor([-0.0179,  0.1014])\n",
      "Epoch 1990, Loss 2.958738\n",
      "    Params: tensor([  5.2626, -16.7100])\n",
      "    Grad:   tensor([-0.0179,  0.1012])\n",
      "Epoch 1991, Loss 2.958632\n",
      "    Params: tensor([  5.2628, -16.7110])\n",
      "    Grad:   tensor([-0.0179,  0.1010])\n",
      "Epoch 1992, Loss 2.958526\n",
      "    Params: tensor([  5.2630, -16.7120])\n",
      "    Grad:   tensor([-0.0178,  0.1009])\n",
      "Epoch 1993, Loss 2.958422\n",
      "    Params: tensor([  5.2632, -16.7130])\n",
      "    Grad:   tensor([-0.0178,  0.1007])\n",
      "Epoch 1994, Loss 2.958317\n",
      "    Params: tensor([  5.2634, -16.7140])\n",
      "    Grad:   tensor([-0.0178,  0.1005])\n",
      "Epoch 1995, Loss 2.958212\n",
      "    Params: tensor([  5.2635, -16.7150])\n",
      "    Grad:   tensor([-0.0177,  0.1004])\n",
      "Epoch 1996, Loss 2.958109\n",
      "    Params: tensor([  5.2637, -16.7160])\n",
      "    Grad:   tensor([-0.0177,  0.1002])\n",
      "Epoch 1997, Loss 2.958006\n",
      "    Params: tensor([  5.2639, -16.7170])\n",
      "    Grad:   tensor([-0.0176,  0.1000])\n",
      "Epoch 1998, Loss 2.957904\n",
      "    Params: tensor([  5.2641, -16.7180])\n",
      "    Grad:   tensor([-0.0176,  0.0998])\n",
      "Epoch 1999, Loss 2.957801\n",
      "    Params: tensor([  5.2642, -16.7190])\n",
      "    Grad:   tensor([-0.0176,  0.0997])\n",
      "Epoch 2000, Loss 2.957698\n",
      "    Params: tensor([  5.2644, -16.7200])\n",
      "    Grad:   tensor([-0.0176,  0.0995])\n",
      "Epoch 2001, Loss 2.957596\n",
      "    Params: tensor([  5.2646, -16.7210])\n",
      "    Grad:   tensor([-0.0176,  0.0993])\n",
      "Epoch 2002, Loss 2.957494\n",
      "    Params: tensor([  5.2648, -16.7220])\n",
      "    Grad:   tensor([-0.0175,  0.0992])\n",
      "Epoch 2003, Loss 2.957393\n",
      "    Params: tensor([  5.2649, -16.7230])\n",
      "    Grad:   tensor([-0.0175,  0.0990])\n",
      "Epoch 2004, Loss 2.957292\n",
      "    Params: tensor([  5.2651, -16.7240])\n",
      "    Grad:   tensor([-0.0174,  0.0988])\n",
      "Epoch 2005, Loss 2.957193\n",
      "    Params: tensor([  5.2653, -16.7250])\n",
      "    Grad:   tensor([-0.0174,  0.0987])\n",
      "Epoch 2006, Loss 2.957091\n",
      "    Params: tensor([  5.2655, -16.7260])\n",
      "    Grad:   tensor([-0.0174,  0.0985])\n",
      "Epoch 2007, Loss 2.956992\n",
      "    Params: tensor([  5.2656, -16.7269])\n",
      "    Grad:   tensor([-0.0174,  0.0983])\n",
      "Epoch 2008, Loss 2.956892\n",
      "    Params: tensor([  5.2658, -16.7279])\n",
      "    Grad:   tensor([-0.0173,  0.0982])\n",
      "Epoch 2009, Loss 2.956792\n",
      "    Params: tensor([  5.2660, -16.7289])\n",
      "    Grad:   tensor([-0.0173,  0.0980])\n",
      "Epoch 2010, Loss 2.956694\n",
      "    Params: tensor([  5.2662, -16.7299])\n",
      "    Grad:   tensor([-0.0173,  0.0978])\n",
      "Epoch 2011, Loss 2.956595\n",
      "    Params: tensor([  5.2663, -16.7309])\n",
      "    Grad:   tensor([-0.0172,  0.0977])\n",
      "Epoch 2012, Loss 2.956496\n",
      "    Params: tensor([  5.2665, -16.7318])\n",
      "    Grad:   tensor([-0.0172,  0.0975])\n",
      "Epoch 2013, Loss 2.956397\n",
      "    Params: tensor([  5.2667, -16.7328])\n",
      "    Grad:   tensor([-0.0172,  0.0973])\n",
      "Epoch 2014, Loss 2.956300\n",
      "    Params: tensor([  5.2668, -16.7338])\n",
      "    Grad:   tensor([-0.0172,  0.0972])\n",
      "Epoch 2015, Loss 2.956204\n",
      "    Params: tensor([  5.2670, -16.7348])\n",
      "    Grad:   tensor([-0.0171,  0.0970])\n",
      "Epoch 2016, Loss 2.956108\n",
      "    Params: tensor([  5.2672, -16.7357])\n",
      "    Grad:   tensor([-0.0171,  0.0968])\n",
      "Epoch 2017, Loss 2.956010\n",
      "    Params: tensor([  5.2674, -16.7367])\n",
      "    Grad:   tensor([-0.0171,  0.0967])\n",
      "Epoch 2018, Loss 2.955914\n",
      "    Params: tensor([  5.2675, -16.7377])\n",
      "    Grad:   tensor([-0.0171,  0.0965])\n",
      "Epoch 2019, Loss 2.955817\n",
      "    Params: tensor([  5.2677, -16.7386])\n",
      "    Grad:   tensor([-0.0170,  0.0963])\n",
      "Epoch 2020, Loss 2.955722\n",
      "    Params: tensor([  5.2679, -16.7396])\n",
      "    Grad:   tensor([-0.0170,  0.0962])\n",
      "Epoch 2021, Loss 2.955627\n",
      "    Params: tensor([  5.2680, -16.7405])\n",
      "    Grad:   tensor([-0.0170,  0.0960])\n",
      "Epoch 2022, Loss 2.955533\n",
      "    Params: tensor([  5.2682, -16.7415])\n",
      "    Grad:   tensor([-0.0169,  0.0959])\n",
      "Epoch 2023, Loss 2.955436\n",
      "    Params: tensor([  5.2684, -16.7425])\n",
      "    Grad:   tensor([-0.0169,  0.0957])\n",
      "Epoch 2024, Loss 2.955343\n",
      "    Params: tensor([  5.2686, -16.7434])\n",
      "    Grad:   tensor([-0.0169,  0.0955])\n",
      "Epoch 2025, Loss 2.955250\n",
      "    Params: tensor([  5.2687, -16.7444])\n",
      "    Grad:   tensor([-0.0169,  0.0954])\n",
      "Epoch 2026, Loss 2.955154\n",
      "    Params: tensor([  5.2689, -16.7453])\n",
      "    Grad:   tensor([-0.0168,  0.0952])\n",
      "Epoch 2027, Loss 2.955062\n",
      "    Params: tensor([  5.2691, -16.7463])\n",
      "    Grad:   tensor([-0.0168,  0.0950])\n",
      "Epoch 2028, Loss 2.954969\n",
      "    Params: tensor([  5.2692, -16.7472])\n",
      "    Grad:   tensor([-0.0168,  0.0949])\n",
      "Epoch 2029, Loss 2.954875\n",
      "    Params: tensor([  5.2694, -16.7482])\n",
      "    Grad:   tensor([-0.0167,  0.0947])\n",
      "Epoch 2030, Loss 2.954783\n",
      "    Params: tensor([  5.2696, -16.7491])\n",
      "    Grad:   tensor([-0.0167,  0.0946])\n",
      "Epoch 2031, Loss 2.954691\n",
      "    Params: tensor([  5.2697, -16.7501])\n",
      "    Grad:   tensor([-0.0167,  0.0944])\n",
      "Epoch 2032, Loss 2.954600\n",
      "    Params: tensor([  5.2699, -16.7510])\n",
      "    Grad:   tensor([-0.0167,  0.0942])\n",
      "Epoch 2033, Loss 2.954507\n",
      "    Params: tensor([  5.2701, -16.7519])\n",
      "    Grad:   tensor([-0.0166,  0.0941])\n",
      "Epoch 2034, Loss 2.954417\n",
      "    Params: tensor([  5.2702, -16.7529])\n",
      "    Grad:   tensor([-0.0166,  0.0939])\n",
      "Epoch 2035, Loss 2.954326\n",
      "    Params: tensor([  5.2704, -16.7538])\n",
      "    Grad:   tensor([-0.0165,  0.0938])\n",
      "Epoch 2036, Loss 2.954235\n",
      "    Params: tensor([  5.2706, -16.7547])\n",
      "    Grad:   tensor([-0.0165,  0.0936])\n",
      "Epoch 2037, Loss 2.954145\n",
      "    Params: tensor([  5.2707, -16.7557])\n",
      "    Grad:   tensor([-0.0165,  0.0934])\n",
      "Epoch 2038, Loss 2.954055\n",
      "    Params: tensor([  5.2709, -16.7566])\n",
      "    Grad:   tensor([-0.0165,  0.0933])\n",
      "Epoch 2039, Loss 2.953966\n",
      "    Params: tensor([  5.2710, -16.7575])\n",
      "    Grad:   tensor([-0.0164,  0.0931])\n",
      "Epoch 2040, Loss 2.953876\n",
      "    Params: tensor([  5.2712, -16.7585])\n",
      "    Grad:   tensor([-0.0164,  0.0930])\n",
      "Epoch 2041, Loss 2.953787\n",
      "    Params: tensor([  5.2714, -16.7594])\n",
      "    Grad:   tensor([-0.0164,  0.0928])\n",
      "Epoch 2042, Loss 2.953698\n",
      "    Params: tensor([  5.2715, -16.7603])\n",
      "    Grad:   tensor([-0.0164,  0.0926])\n",
      "Epoch 2043, Loss 2.953610\n",
      "    Params: tensor([  5.2717, -16.7613])\n",
      "    Grad:   tensor([-0.0163,  0.0925])\n",
      "Epoch 2044, Loss 2.953521\n",
      "    Params: tensor([  5.2719, -16.7622])\n",
      "    Grad:   tensor([-0.0163,  0.0923])\n",
      "Epoch 2045, Loss 2.953434\n",
      "    Params: tensor([  5.2720, -16.7631])\n",
      "    Grad:   tensor([-0.0163,  0.0922])\n",
      "Epoch 2046, Loss 2.953346\n",
      "    Params: tensor([  5.2722, -16.7640])\n",
      "    Grad:   tensor([-0.0163,  0.0920])\n",
      "Epoch 2047, Loss 2.953259\n",
      "    Params: tensor([  5.2724, -16.7649])\n",
      "    Grad:   tensor([-0.0162,  0.0919])\n",
      "Epoch 2048, Loss 2.953171\n",
      "    Params: tensor([  5.2725, -16.7659])\n",
      "    Grad:   tensor([-0.0162,  0.0917])\n",
      "Epoch 2049, Loss 2.953085\n",
      "    Params: tensor([  5.2727, -16.7668])\n",
      "    Grad:   tensor([-0.0162,  0.0915])\n",
      "Epoch 2050, Loss 2.953000\n",
      "    Params: tensor([  5.2728, -16.7677])\n",
      "    Grad:   tensor([-0.0162,  0.0914])\n",
      "Epoch 2051, Loss 2.952913\n",
      "    Params: tensor([  5.2730, -16.7686])\n",
      "    Grad:   tensor([-0.0161,  0.0912])\n",
      "Epoch 2052, Loss 2.952828\n",
      "    Params: tensor([  5.2732, -16.7695])\n",
      "    Grad:   tensor([-0.0161,  0.0911])\n",
      "Epoch 2053, Loss 2.952742\n",
      "    Params: tensor([  5.2733, -16.7704])\n",
      "    Grad:   tensor([-0.0161,  0.0909])\n",
      "Epoch 2054, Loss 2.952657\n",
      "    Params: tensor([  5.2735, -16.7713])\n",
      "    Grad:   tensor([-0.0160,  0.0908])\n",
      "Epoch 2055, Loss 2.952571\n",
      "    Params: tensor([  5.2736, -16.7722])\n",
      "    Grad:   tensor([-0.0160,  0.0906])\n",
      "Epoch 2056, Loss 2.952487\n",
      "    Params: tensor([  5.2738, -16.7731])\n",
      "    Grad:   tensor([-0.0160,  0.0905])\n",
      "Epoch 2057, Loss 2.952403\n",
      "    Params: tensor([  5.2740, -16.7740])\n",
      "    Grad:   tensor([-0.0160,  0.0903])\n",
      "Epoch 2058, Loss 2.952318\n",
      "    Params: tensor([  5.2741, -16.7749])\n",
      "    Grad:   tensor([-0.0159,  0.0902])\n",
      "Epoch 2059, Loss 2.952235\n",
      "    Params: tensor([  5.2743, -16.7758])\n",
      "    Grad:   tensor([-0.0159,  0.0900])\n",
      "Epoch 2060, Loss 2.952152\n",
      "    Params: tensor([  5.2744, -16.7767])\n",
      "    Grad:   tensor([-0.0159,  0.0899])\n",
      "Epoch 2061, Loss 2.952068\n",
      "    Params: tensor([  5.2746, -16.7776])\n",
      "    Grad:   tensor([-0.0158,  0.0897])\n",
      "Epoch 2062, Loss 2.951985\n",
      "    Params: tensor([  5.2748, -16.7785])\n",
      "    Grad:   tensor([-0.0158,  0.0895])\n",
      "Epoch 2063, Loss 2.951902\n",
      "    Params: tensor([  5.2749, -16.7794])\n",
      "    Grad:   tensor([-0.0158,  0.0894])\n",
      "Epoch 2064, Loss 2.951820\n",
      "    Params: tensor([  5.2751, -16.7803])\n",
      "    Grad:   tensor([-0.0158,  0.0892])\n",
      "Epoch 2065, Loss 2.951738\n",
      "    Params: tensor([  5.2752, -16.7812])\n",
      "    Grad:   tensor([-0.0157,  0.0891])\n",
      "Epoch 2066, Loss 2.951656\n",
      "    Params: tensor([  5.2754, -16.7821])\n",
      "    Grad:   tensor([-0.0157,  0.0889])\n",
      "Epoch 2067, Loss 2.951576\n",
      "    Params: tensor([  5.2755, -16.7830])\n",
      "    Grad:   tensor([-0.0157,  0.0888])\n",
      "Epoch 2068, Loss 2.951494\n",
      "    Params: tensor([  5.2757, -16.7839])\n",
      "    Grad:   tensor([-0.0157,  0.0886])\n",
      "Epoch 2069, Loss 2.951413\n",
      "    Params: tensor([  5.2759, -16.7848])\n",
      "    Grad:   tensor([-0.0157,  0.0885])\n",
      "Epoch 2070, Loss 2.951333\n",
      "    Params: tensor([  5.2760, -16.7856])\n",
      "    Grad:   tensor([-0.0156,  0.0883])\n",
      "Epoch 2071, Loss 2.951252\n",
      "    Params: tensor([  5.2762, -16.7865])\n",
      "    Grad:   tensor([-0.0156,  0.0882])\n",
      "Epoch 2072, Loss 2.951171\n",
      "    Params: tensor([  5.2763, -16.7874])\n",
      "    Grad:   tensor([-0.0155,  0.0880])\n",
      "Epoch 2073, Loss 2.951093\n",
      "    Params: tensor([  5.2765, -16.7883])\n",
      "    Grad:   tensor([-0.0155,  0.0879])\n",
      "Epoch 2074, Loss 2.951012\n",
      "    Params: tensor([  5.2766, -16.7892])\n",
      "    Grad:   tensor([-0.0155,  0.0877])\n",
      "Epoch 2075, Loss 2.950932\n",
      "    Params: tensor([  5.2768, -16.7900])\n",
      "    Grad:   tensor([-0.0155,  0.0876])\n",
      "Epoch 2076, Loss 2.950853\n",
      "    Params: tensor([  5.2769, -16.7909])\n",
      "    Grad:   tensor([-0.0154,  0.0874])\n",
      "Epoch 2077, Loss 2.950774\n",
      "    Params: tensor([  5.2771, -16.7918])\n",
      "    Grad:   tensor([-0.0154,  0.0873])\n",
      "Epoch 2078, Loss 2.950697\n",
      "    Params: tensor([  5.2772, -16.7927])\n",
      "    Grad:   tensor([-0.0154,  0.0871])\n",
      "Epoch 2079, Loss 2.950618\n",
      "    Params: tensor([  5.2774, -16.7935])\n",
      "    Grad:   tensor([-0.0154,  0.0870])\n",
      "Epoch 2080, Loss 2.950540\n",
      "    Params: tensor([  5.2776, -16.7944])\n",
      "    Grad:   tensor([-0.0154,  0.0868])\n",
      "Epoch 2081, Loss 2.950463\n",
      "    Params: tensor([  5.2777, -16.7953])\n",
      "    Grad:   tensor([-0.0153,  0.0867])\n",
      "Epoch 2082, Loss 2.950385\n",
      "    Params: tensor([  5.2779, -16.7961])\n",
      "    Grad:   tensor([-0.0153,  0.0866])\n",
      "Epoch 2083, Loss 2.950308\n",
      "    Params: tensor([  5.2780, -16.7970])\n",
      "    Grad:   tensor([-0.0153,  0.0864])\n",
      "Epoch 2084, Loss 2.950231\n",
      "    Params: tensor([  5.2782, -16.7979])\n",
      "    Grad:   tensor([-0.0152,  0.0863])\n",
      "Epoch 2085, Loss 2.950154\n",
      "    Params: tensor([  5.2783, -16.7987])\n",
      "    Grad:   tensor([-0.0152,  0.0861])\n",
      "Epoch 2086, Loss 2.950078\n",
      "    Params: tensor([  5.2785, -16.7996])\n",
      "    Grad:   tensor([-0.0152,  0.0860])\n",
      "Epoch 2087, Loss 2.950003\n",
      "    Params: tensor([  5.2786, -16.8004])\n",
      "    Grad:   tensor([-0.0152,  0.0858])\n",
      "Epoch 2088, Loss 2.949925\n",
      "    Params: tensor([  5.2788, -16.8013])\n",
      "    Grad:   tensor([-0.0152,  0.0857])\n",
      "Epoch 2089, Loss 2.949850\n",
      "    Params: tensor([  5.2789, -16.8021])\n",
      "    Grad:   tensor([-0.0151,  0.0855])\n",
      "Epoch 2090, Loss 2.949776\n",
      "    Params: tensor([  5.2791, -16.8030])\n",
      "    Grad:   tensor([-0.0151,  0.0854])\n",
      "Epoch 2091, Loss 2.949699\n",
      "    Params: tensor([  5.2792, -16.8039])\n",
      "    Grad:   tensor([-0.0151,  0.0852])\n",
      "Epoch 2092, Loss 2.949626\n",
      "    Params: tensor([  5.2794, -16.8047])\n",
      "    Grad:   tensor([-0.0150,  0.0851])\n",
      "Epoch 2093, Loss 2.949550\n",
      "    Params: tensor([  5.2795, -16.8056])\n",
      "    Grad:   tensor([-0.0150,  0.0850])\n",
      "Epoch 2094, Loss 2.949476\n",
      "    Params: tensor([  5.2797, -16.8064])\n",
      "    Grad:   tensor([-0.0150,  0.0848])\n",
      "Epoch 2095, Loss 2.949401\n",
      "    Params: tensor([  5.2798, -16.8072])\n",
      "    Grad:   tensor([-0.0149,  0.0847])\n",
      "Epoch 2096, Loss 2.949328\n",
      "    Params: tensor([  5.2800, -16.8081])\n",
      "    Grad:   tensor([-0.0150,  0.0845])\n",
      "Epoch 2097, Loss 2.949254\n",
      "    Params: tensor([  5.2801, -16.8089])\n",
      "    Grad:   tensor([-0.0149,  0.0844])\n",
      "Epoch 2098, Loss 2.949182\n",
      "    Params: tensor([  5.2803, -16.8098])\n",
      "    Grad:   tensor([-0.0149,  0.0842])\n",
      "Epoch 2099, Loss 2.949108\n",
      "    Params: tensor([  5.2804, -16.8106])\n",
      "    Grad:   tensor([-0.0149,  0.0841])\n",
      "Epoch 2100, Loss 2.949035\n",
      "    Params: tensor([  5.2806, -16.8115])\n",
      "    Grad:   tensor([-0.0148,  0.0839])\n",
      "Epoch 2101, Loss 2.948962\n",
      "    Params: tensor([  5.2807, -16.8123])\n",
      "    Grad:   tensor([-0.0148,  0.0838])\n",
      "Epoch 2102, Loss 2.948890\n",
      "    Params: tensor([  5.2809, -16.8131])\n",
      "    Grad:   tensor([-0.0148,  0.0837])\n",
      "Epoch 2103, Loss 2.948818\n",
      "    Params: tensor([  5.2810, -16.8140])\n",
      "    Grad:   tensor([-0.0148,  0.0835])\n",
      "Epoch 2104, Loss 2.948745\n",
      "    Params: tensor([  5.2812, -16.8148])\n",
      "    Grad:   tensor([-0.0148,  0.0834])\n",
      "Epoch 2105, Loss 2.948675\n",
      "    Params: tensor([  5.2813, -16.8156])\n",
      "    Grad:   tensor([-0.0147,  0.0832])\n",
      "Epoch 2106, Loss 2.948602\n",
      "    Params: tensor([  5.2815, -16.8165])\n",
      "    Grad:   tensor([-0.0147,  0.0831])\n",
      "Epoch 2107, Loss 2.948532\n",
      "    Params: tensor([  5.2816, -16.8173])\n",
      "    Grad:   tensor([-0.0146,  0.0830])\n",
      "Epoch 2108, Loss 2.948462\n",
      "    Params: tensor([  5.2817, -16.8181])\n",
      "    Grad:   tensor([-0.0146,  0.0828])\n",
      "Epoch 2109, Loss 2.948391\n",
      "    Params: tensor([  5.2819, -16.8189])\n",
      "    Grad:   tensor([-0.0146,  0.0827])\n",
      "Epoch 2110, Loss 2.948321\n",
      "    Params: tensor([  5.2820, -16.8198])\n",
      "    Grad:   tensor([-0.0146,  0.0825])\n",
      "Epoch 2111, Loss 2.948250\n",
      "    Params: tensor([  5.2822, -16.8206])\n",
      "    Grad:   tensor([-0.0145,  0.0824])\n",
      "Epoch 2112, Loss 2.948181\n",
      "    Params: tensor([  5.2823, -16.8214])\n",
      "    Grad:   tensor([-0.0145,  0.0823])\n",
      "Epoch 2113, Loss 2.948109\n",
      "    Params: tensor([  5.2825, -16.8222])\n",
      "    Grad:   tensor([-0.0145,  0.0821])\n",
      "Epoch 2114, Loss 2.948041\n",
      "    Params: tensor([  5.2826, -16.8231])\n",
      "    Grad:   tensor([-0.0145,  0.0820])\n",
      "Epoch 2115, Loss 2.947971\n",
      "    Params: tensor([  5.2828, -16.8239])\n",
      "    Grad:   tensor([-0.0144,  0.0818])\n",
      "Epoch 2116, Loss 2.947902\n",
      "    Params: tensor([  5.2829, -16.8247])\n",
      "    Grad:   tensor([-0.0144,  0.0817])\n",
      "Epoch 2117, Loss 2.947833\n",
      "    Params: tensor([  5.2831, -16.8255])\n",
      "    Grad:   tensor([-0.0144,  0.0816])\n",
      "Epoch 2118, Loss 2.947765\n",
      "    Params: tensor([  5.2832, -16.8263])\n",
      "    Grad:   tensor([-0.0144,  0.0814])\n",
      "Epoch 2119, Loss 2.947696\n",
      "    Params: tensor([  5.2833, -16.8271])\n",
      "    Grad:   tensor([-0.0144,  0.0813])\n",
      "Epoch 2120, Loss 2.947628\n",
      "    Params: tensor([  5.2835, -16.8280])\n",
      "    Grad:   tensor([-0.0143,  0.0811])\n",
      "Epoch 2121, Loss 2.947560\n",
      "    Params: tensor([  5.2836, -16.8288])\n",
      "    Grad:   tensor([-0.0143,  0.0810])\n",
      "Epoch 2122, Loss 2.947494\n",
      "    Params: tensor([  5.2838, -16.8296])\n",
      "    Grad:   tensor([-0.0143,  0.0809])\n",
      "Epoch 2123, Loss 2.947426\n",
      "    Params: tensor([  5.2839, -16.8304])\n",
      "    Grad:   tensor([-0.0143,  0.0807])\n",
      "Epoch 2124, Loss 2.947357\n",
      "    Params: tensor([  5.2841, -16.8312])\n",
      "    Grad:   tensor([-0.0142,  0.0806])\n",
      "Epoch 2125, Loss 2.947293\n",
      "    Params: tensor([  5.2842, -16.8320])\n",
      "    Grad:   tensor([-0.0142,  0.0805])\n",
      "Epoch 2126, Loss 2.947225\n",
      "    Params: tensor([  5.2843, -16.8328])\n",
      "    Grad:   tensor([-0.0142,  0.0803])\n",
      "Epoch 2127, Loss 2.947158\n",
      "    Params: tensor([  5.2845, -16.8336])\n",
      "    Grad:   tensor([-0.0142,  0.0802])\n",
      "Epoch 2128, Loss 2.947092\n",
      "    Params: tensor([  5.2846, -16.8344])\n",
      "    Grad:   tensor([-0.0141,  0.0800])\n",
      "Epoch 2129, Loss 2.947026\n",
      "    Params: tensor([  5.2848, -16.8352])\n",
      "    Grad:   tensor([-0.0141,  0.0799])\n",
      "Epoch 2130, Loss 2.946960\n",
      "    Params: tensor([  5.2849, -16.8360])\n",
      "    Grad:   tensor([-0.0141,  0.0798])\n",
      "Epoch 2131, Loss 2.946895\n",
      "    Params: tensor([  5.2850, -16.8368])\n",
      "    Grad:   tensor([-0.0141,  0.0796])\n",
      "Epoch 2132, Loss 2.946830\n",
      "    Params: tensor([  5.2852, -16.8376])\n",
      "    Grad:   tensor([-0.0141,  0.0795])\n",
      "Epoch 2133, Loss 2.946764\n",
      "    Params: tensor([  5.2853, -16.8384])\n",
      "    Grad:   tensor([-0.0140,  0.0794])\n",
      "Epoch 2134, Loss 2.946700\n",
      "    Params: tensor([  5.2855, -16.8392])\n",
      "    Grad:   tensor([-0.0140,  0.0792])\n",
      "Epoch 2135, Loss 2.946635\n",
      "    Params: tensor([  5.2856, -16.8400])\n",
      "    Grad:   tensor([-0.0140,  0.0791])\n",
      "Epoch 2136, Loss 2.946571\n",
      "    Params: tensor([  5.2857, -16.8407])\n",
      "    Grad:   tensor([-0.0139,  0.0790])\n",
      "Epoch 2137, Loss 2.946507\n",
      "    Params: tensor([  5.2859, -16.8415])\n",
      "    Grad:   tensor([-0.0139,  0.0788])\n",
      "Epoch 2138, Loss 2.946442\n",
      "    Params: tensor([  5.2860, -16.8423])\n",
      "    Grad:   tensor([-0.0139,  0.0787])\n",
      "Epoch 2139, Loss 2.946378\n",
      "    Params: tensor([  5.2862, -16.8431])\n",
      "    Grad:   tensor([-0.0139,  0.0786])\n",
      "Epoch 2140, Loss 2.946314\n",
      "    Params: tensor([  5.2863, -16.8439])\n",
      "    Grad:   tensor([-0.0138,  0.0784])\n",
      "Epoch 2141, Loss 2.946251\n",
      "    Params: tensor([  5.2864, -16.8447])\n",
      "    Grad:   tensor([-0.0138,  0.0783])\n",
      "Epoch 2142, Loss 2.946189\n",
      "    Params: tensor([  5.2866, -16.8455])\n",
      "    Grad:   tensor([-0.0138,  0.0782])\n",
      "Epoch 2143, Loss 2.946126\n",
      "    Params: tensor([  5.2867, -16.8462])\n",
      "    Grad:   tensor([-0.0138,  0.0780])\n",
      "Epoch 2144, Loss 2.946063\n",
      "    Params: tensor([  5.2869, -16.8470])\n",
      "    Grad:   tensor([-0.0138,  0.0779])\n",
      "Epoch 2145, Loss 2.946001\n",
      "    Params: tensor([  5.2870, -16.8478])\n",
      "    Grad:   tensor([-0.0137,  0.0778])\n",
      "Epoch 2146, Loss 2.945937\n",
      "    Params: tensor([  5.2871, -16.8486])\n",
      "    Grad:   tensor([-0.0137,  0.0776])\n",
      "Epoch 2147, Loss 2.945876\n",
      "    Params: tensor([  5.2873, -16.8493])\n",
      "    Grad:   tensor([-0.0137,  0.0775])\n",
      "Epoch 2148, Loss 2.945815\n",
      "    Params: tensor([  5.2874, -16.8501])\n",
      "    Grad:   tensor([-0.0137,  0.0774])\n",
      "Epoch 2149, Loss 2.945753\n",
      "    Params: tensor([  5.2875, -16.8509])\n",
      "    Grad:   tensor([-0.0136,  0.0772])\n",
      "Epoch 2150, Loss 2.945690\n",
      "    Params: tensor([  5.2877, -16.8517])\n",
      "    Grad:   tensor([-0.0136,  0.0771])\n",
      "Epoch 2151, Loss 2.945630\n",
      "    Params: tensor([  5.2878, -16.8524])\n",
      "    Grad:   tensor([-0.0136,  0.0770])\n",
      "Epoch 2152, Loss 2.945567\n",
      "    Params: tensor([  5.2879, -16.8532])\n",
      "    Grad:   tensor([-0.0136,  0.0768])\n",
      "Epoch 2153, Loss 2.945508\n",
      "    Params: tensor([  5.2881, -16.8540])\n",
      "    Grad:   tensor([-0.0135,  0.0767])\n",
      "Epoch 2154, Loss 2.945447\n",
      "    Params: tensor([  5.2882, -16.8547])\n",
      "    Grad:   tensor([-0.0135,  0.0766])\n",
      "Epoch 2155, Loss 2.945385\n",
      "    Params: tensor([  5.2884, -16.8555])\n",
      "    Grad:   tensor([-0.0135,  0.0765])\n",
      "Epoch 2156, Loss 2.945325\n",
      "    Params: tensor([  5.2885, -16.8563])\n",
      "    Grad:   tensor([-0.0135,  0.0763])\n",
      "Epoch 2157, Loss 2.945267\n",
      "    Params: tensor([  5.2886, -16.8570])\n",
      "    Grad:   tensor([-0.0135,  0.0762])\n",
      "Epoch 2158, Loss 2.945206\n",
      "    Params: tensor([  5.2888, -16.8578])\n",
      "    Grad:   tensor([-0.0134,  0.0761])\n",
      "Epoch 2159, Loss 2.945146\n",
      "    Params: tensor([  5.2889, -16.8585])\n",
      "    Grad:   tensor([-0.0134,  0.0759])\n",
      "Epoch 2160, Loss 2.945088\n",
      "    Params: tensor([  5.2890, -16.8593])\n",
      "    Grad:   tensor([-0.0134,  0.0758])\n",
      "Epoch 2161, Loss 2.945028\n",
      "    Params: tensor([  5.2892, -16.8601])\n",
      "    Grad:   tensor([-0.0134,  0.0757])\n",
      "Epoch 2162, Loss 2.944969\n",
      "    Params: tensor([  5.2893, -16.8608])\n",
      "    Grad:   tensor([-0.0133,  0.0755])\n",
      "Epoch 2163, Loss 2.944911\n",
      "    Params: tensor([  5.2894, -16.8616])\n",
      "    Grad:   tensor([-0.0133,  0.0754])\n",
      "Epoch 2164, Loss 2.944852\n",
      "    Params: tensor([  5.2896, -16.8623])\n",
      "    Grad:   tensor([-0.0133,  0.0753])\n",
      "Epoch 2165, Loss 2.944792\n",
      "    Params: tensor([  5.2897, -16.8631])\n",
      "    Grad:   tensor([-0.0133,  0.0752])\n",
      "Epoch 2166, Loss 2.944736\n",
      "    Params: tensor([  5.2898, -16.8638])\n",
      "    Grad:   tensor([-0.0133,  0.0750])\n",
      "Epoch 2167, Loss 2.944678\n",
      "    Params: tensor([  5.2900, -16.8646])\n",
      "    Grad:   tensor([-0.0132,  0.0749])\n",
      "Epoch 2168, Loss 2.944619\n",
      "    Params: tensor([  5.2901, -16.8653])\n",
      "    Grad:   tensor([-0.0132,  0.0748])\n",
      "Epoch 2169, Loss 2.944562\n",
      "    Params: tensor([  5.2902, -16.8661])\n",
      "    Grad:   tensor([-0.0132,  0.0747])\n",
      "Epoch 2170, Loss 2.944504\n",
      "    Params: tensor([  5.2903, -16.8668])\n",
      "    Grad:   tensor([-0.0132,  0.0745])\n",
      "Epoch 2171, Loss 2.944447\n",
      "    Params: tensor([  5.2905, -16.8676])\n",
      "    Grad:   tensor([-0.0132,  0.0744])\n",
      "Epoch 2172, Loss 2.944391\n",
      "    Params: tensor([  5.2906, -16.8683])\n",
      "    Grad:   tensor([-0.0131,  0.0743])\n",
      "Epoch 2173, Loss 2.944332\n",
      "    Params: tensor([  5.2907, -16.8690])\n",
      "    Grad:   tensor([-0.0131,  0.0742])\n",
      "Epoch 2174, Loss 2.944276\n",
      "    Params: tensor([  5.2909, -16.8698])\n",
      "    Grad:   tensor([-0.0131,  0.0740])\n",
      "Epoch 2175, Loss 2.944220\n",
      "    Params: tensor([  5.2910, -16.8705])\n",
      "    Grad:   tensor([-0.0131,  0.0739])\n",
      "Epoch 2176, Loss 2.944164\n",
      "    Params: tensor([  5.2911, -16.8713])\n",
      "    Grad:   tensor([-0.0130,  0.0738])\n",
      "Epoch 2177, Loss 2.944108\n",
      "    Params: tensor([  5.2913, -16.8720])\n",
      "    Grad:   tensor([-0.0130,  0.0736])\n",
      "Epoch 2178, Loss 2.944053\n",
      "    Params: tensor([  5.2914, -16.8727])\n",
      "    Grad:   tensor([-0.0130,  0.0735])\n",
      "Epoch 2179, Loss 2.943996\n",
      "    Params: tensor([  5.2915, -16.8735])\n",
      "    Grad:   tensor([-0.0130,  0.0734])\n",
      "Epoch 2180, Loss 2.943941\n",
      "    Params: tensor([  5.2917, -16.8742])\n",
      "    Grad:   tensor([-0.0129,  0.0733])\n",
      "Epoch 2181, Loss 2.943887\n",
      "    Params: tensor([  5.2918, -16.8749])\n",
      "    Grad:   tensor([-0.0129,  0.0731])\n",
      "Epoch 2182, Loss 2.943831\n",
      "    Params: tensor([  5.2919, -16.8757])\n",
      "    Grad:   tensor([-0.0129,  0.0730])\n",
      "Epoch 2183, Loss 2.943776\n",
      "    Params: tensor([  5.2920, -16.8764])\n",
      "    Grad:   tensor([-0.0129,  0.0729])\n",
      "Epoch 2184, Loss 2.943721\n",
      "    Params: tensor([  5.2922, -16.8771])\n",
      "    Grad:   tensor([-0.0129,  0.0728])\n",
      "Epoch 2185, Loss 2.943666\n",
      "    Params: tensor([  5.2923, -16.8778])\n",
      "    Grad:   tensor([-0.0128,  0.0727])\n",
      "Epoch 2186, Loss 2.943613\n",
      "    Params: tensor([  5.2924, -16.8786])\n",
      "    Grad:   tensor([-0.0128,  0.0725])\n",
      "Epoch 2187, Loss 2.943558\n",
      "    Params: tensor([  5.2926, -16.8793])\n",
      "    Grad:   tensor([-0.0128,  0.0724])\n",
      "Epoch 2188, Loss 2.943503\n",
      "    Params: tensor([  5.2927, -16.8800])\n",
      "    Grad:   tensor([-0.0128,  0.0723])\n",
      "Epoch 2189, Loss 2.943451\n",
      "    Params: tensor([  5.2928, -16.8807])\n",
      "    Grad:   tensor([-0.0127,  0.0722])\n",
      "Epoch 2190, Loss 2.943395\n",
      "    Params: tensor([  5.2929, -16.8815])\n",
      "    Grad:   tensor([-0.0127,  0.0720])\n",
      "Epoch 2191, Loss 2.943343\n",
      "    Params: tensor([  5.2931, -16.8822])\n",
      "    Grad:   tensor([-0.0127,  0.0719])\n",
      "Epoch 2192, Loss 2.943290\n",
      "    Params: tensor([  5.2932, -16.8829])\n",
      "    Grad:   tensor([-0.0127,  0.0718])\n",
      "Epoch 2193, Loss 2.943235\n",
      "    Params: tensor([  5.2933, -16.8836])\n",
      "    Grad:   tensor([-0.0127,  0.0717])\n",
      "Epoch 2194, Loss 2.943183\n",
      "    Params: tensor([  5.2934, -16.8843])\n",
      "    Grad:   tensor([-0.0126,  0.0715])\n",
      "Epoch 2195, Loss 2.943130\n",
      "    Params: tensor([  5.2936, -16.8850])\n",
      "    Grad:   tensor([-0.0126,  0.0714])\n",
      "Epoch 2196, Loss 2.943079\n",
      "    Params: tensor([  5.2937, -16.8857])\n",
      "    Grad:   tensor([-0.0126,  0.0713])\n",
      "Epoch 2197, Loss 2.943027\n",
      "    Params: tensor([  5.2938, -16.8865])\n",
      "    Grad:   tensor([-0.0126,  0.0712])\n",
      "Epoch 2198, Loss 2.942973\n",
      "    Params: tensor([  5.2939, -16.8872])\n",
      "    Grad:   tensor([-0.0126,  0.0711])\n",
      "Epoch 2199, Loss 2.942922\n",
      "    Params: tensor([  5.2941, -16.8879])\n",
      "    Grad:   tensor([-0.0125,  0.0709])\n",
      "Epoch 2200, Loss 2.942870\n",
      "    Params: tensor([  5.2942, -16.8886])\n",
      "    Grad:   tensor([-0.0125,  0.0708])\n",
      "Epoch 2201, Loss 2.942818\n",
      "    Params: tensor([  5.2943, -16.8893])\n",
      "    Grad:   tensor([-0.0125,  0.0707])\n",
      "Epoch 2202, Loss 2.942766\n",
      "    Params: tensor([  5.2944, -16.8900])\n",
      "    Grad:   tensor([-0.0125,  0.0706])\n",
      "Epoch 2203, Loss 2.942714\n",
      "    Params: tensor([  5.2946, -16.8907])\n",
      "    Grad:   tensor([-0.0124,  0.0705])\n",
      "Epoch 2204, Loss 2.942665\n",
      "    Params: tensor([  5.2947, -16.8914])\n",
      "    Grad:   tensor([-0.0124,  0.0703])\n",
      "Epoch 2205, Loss 2.942612\n",
      "    Params: tensor([  5.2948, -16.8921])\n",
      "    Grad:   tensor([-0.0124,  0.0702])\n",
      "Epoch 2206, Loss 2.942564\n",
      "    Params: tensor([  5.2949, -16.8928])\n",
      "    Grad:   tensor([-0.0124,  0.0701])\n",
      "Epoch 2207, Loss 2.942510\n",
      "    Params: tensor([  5.2951, -16.8935])\n",
      "    Grad:   tensor([-0.0124,  0.0700])\n",
      "Epoch 2208, Loss 2.942461\n",
      "    Params: tensor([  5.2952, -16.8942])\n",
      "    Grad:   tensor([-0.0123,  0.0699])\n",
      "Epoch 2209, Loss 2.942411\n",
      "    Params: tensor([  5.2953, -16.8949])\n",
      "    Grad:   tensor([-0.0123,  0.0697])\n",
      "Epoch 2210, Loss 2.942361\n",
      "    Params: tensor([  5.2954, -16.8956])\n",
      "    Grad:   tensor([-0.0123,  0.0696])\n",
      "Epoch 2211, Loss 2.942310\n",
      "    Params: tensor([  5.2956, -16.8963])\n",
      "    Grad:   tensor([-0.0123,  0.0695])\n",
      "Epoch 2212, Loss 2.942261\n",
      "    Params: tensor([  5.2957, -16.8970])\n",
      "    Grad:   tensor([-0.0122,  0.0694])\n",
      "Epoch 2213, Loss 2.942211\n",
      "    Params: tensor([  5.2958, -16.8977])\n",
      "    Grad:   tensor([-0.0122,  0.0693])\n",
      "Epoch 2214, Loss 2.942162\n",
      "    Params: tensor([  5.2959, -16.8984])\n",
      "    Grad:   tensor([-0.0122,  0.0692])\n",
      "Epoch 2215, Loss 2.942112\n",
      "    Params: tensor([  5.2960, -16.8991])\n",
      "    Grad:   tensor([-0.0122,  0.0690])\n",
      "Epoch 2216, Loss 2.942062\n",
      "    Params: tensor([  5.2962, -16.8998])\n",
      "    Grad:   tensor([-0.0122,  0.0689])\n",
      "Epoch 2217, Loss 2.942014\n",
      "    Params: tensor([  5.2963, -16.9004])\n",
      "    Grad:   tensor([-0.0122,  0.0688])\n",
      "Epoch 2218, Loss 2.941965\n",
      "    Params: tensor([  5.2964, -16.9011])\n",
      "    Grad:   tensor([-0.0121,  0.0687])\n",
      "Epoch 2219, Loss 2.941918\n",
      "    Params: tensor([  5.2965, -16.9018])\n",
      "    Grad:   tensor([-0.0121,  0.0686])\n",
      "Epoch 2220, Loss 2.941868\n",
      "    Params: tensor([  5.2967, -16.9025])\n",
      "    Grad:   tensor([-0.0121,  0.0685])\n",
      "Epoch 2221, Loss 2.941821\n",
      "    Params: tensor([  5.2968, -16.9032])\n",
      "    Grad:   tensor([-0.0121,  0.0683])\n",
      "Epoch 2222, Loss 2.941773\n",
      "    Params: tensor([  5.2969, -16.9039])\n",
      "    Grad:   tensor([-0.0120,  0.0682])\n",
      "Epoch 2223, Loss 2.941724\n",
      "    Params: tensor([  5.2970, -16.9046])\n",
      "    Grad:   tensor([-0.0120,  0.0681])\n",
      "Epoch 2224, Loss 2.941677\n",
      "    Params: tensor([  5.2971, -16.9052])\n",
      "    Grad:   tensor([-0.0120,  0.0680])\n",
      "Epoch 2225, Loss 2.941629\n",
      "    Params: tensor([  5.2973, -16.9059])\n",
      "    Grad:   tensor([-0.0120,  0.0679])\n",
      "Epoch 2226, Loss 2.941582\n",
      "    Params: tensor([  5.2974, -16.9066])\n",
      "    Grad:   tensor([-0.0120,  0.0678])\n",
      "Epoch 2227, Loss 2.941534\n",
      "    Params: tensor([  5.2975, -16.9073])\n",
      "    Grad:   tensor([-0.0119,  0.0676])\n",
      "Epoch 2228, Loss 2.941488\n",
      "    Params: tensor([  5.2976, -16.9079])\n",
      "    Grad:   tensor([-0.0119,  0.0675])\n",
      "Epoch 2229, Loss 2.941440\n",
      "    Params: tensor([  5.2977, -16.9086])\n",
      "    Grad:   tensor([-0.0119,  0.0674])\n",
      "Epoch 2230, Loss 2.941393\n",
      "    Params: tensor([  5.2979, -16.9093])\n",
      "    Grad:   tensor([-0.0119,  0.0673])\n",
      "Epoch 2231, Loss 2.941346\n",
      "    Params: tensor([  5.2980, -16.9100])\n",
      "    Grad:   tensor([-0.0119,  0.0672])\n",
      "Epoch 2232, Loss 2.941299\n",
      "    Params: tensor([  5.2981, -16.9106])\n",
      "    Grad:   tensor([-0.0118,  0.0671])\n",
      "Epoch 2233, Loss 2.941253\n",
      "    Params: tensor([  5.2982, -16.9113])\n",
      "    Grad:   tensor([-0.0118,  0.0670])\n",
      "Epoch 2234, Loss 2.941206\n",
      "    Params: tensor([  5.2983, -16.9120])\n",
      "    Grad:   tensor([-0.0118,  0.0668])\n",
      "Epoch 2235, Loss 2.941163\n",
      "    Params: tensor([  5.2984, -16.9126])\n",
      "    Grad:   tensor([-0.0118,  0.0667])\n",
      "Epoch 2236, Loss 2.941116\n",
      "    Params: tensor([  5.2986, -16.9133])\n",
      "    Grad:   tensor([-0.0118,  0.0666])\n",
      "Epoch 2237, Loss 2.941070\n",
      "    Params: tensor([  5.2987, -16.9140])\n",
      "    Grad:   tensor([-0.0117,  0.0665])\n",
      "Epoch 2238, Loss 2.941025\n",
      "    Params: tensor([  5.2988, -16.9146])\n",
      "    Grad:   tensor([-0.0117,  0.0664])\n",
      "Epoch 2239, Loss 2.940979\n",
      "    Params: tensor([  5.2989, -16.9153])\n",
      "    Grad:   tensor([-0.0117,  0.0663])\n",
      "Epoch 2240, Loss 2.940933\n",
      "    Params: tensor([  5.2990, -16.9160])\n",
      "    Grad:   tensor([-0.0117,  0.0662])\n",
      "Epoch 2241, Loss 2.940890\n",
      "    Params: tensor([  5.2991, -16.9166])\n",
      "    Grad:   tensor([-0.0117,  0.0661])\n",
      "Epoch 2242, Loss 2.940844\n",
      "    Params: tensor([  5.2993, -16.9173])\n",
      "    Grad:   tensor([-0.0117,  0.0659])\n",
      "Epoch 2243, Loss 2.940798\n",
      "    Params: tensor([  5.2994, -16.9179])\n",
      "    Grad:   tensor([-0.0116,  0.0658])\n",
      "Epoch 2244, Loss 2.940753\n",
      "    Params: tensor([  5.2995, -16.9186])\n",
      "    Grad:   tensor([-0.0116,  0.0657])\n",
      "Epoch 2245, Loss 2.940711\n",
      "    Params: tensor([  5.2996, -16.9192])\n",
      "    Grad:   tensor([-0.0116,  0.0656])\n",
      "Epoch 2246, Loss 2.940666\n",
      "    Params: tensor([  5.2997, -16.9199])\n",
      "    Grad:   tensor([-0.0116,  0.0655])\n",
      "Epoch 2247, Loss 2.940621\n",
      "    Params: tensor([  5.2998, -16.9206])\n",
      "    Grad:   tensor([-0.0115,  0.0654])\n",
      "Epoch 2248, Loss 2.940576\n",
      "    Params: tensor([  5.3000, -16.9212])\n",
      "    Grad:   tensor([-0.0115,  0.0653])\n",
      "Epoch 2249, Loss 2.940533\n",
      "    Params: tensor([  5.3001, -16.9219])\n",
      "    Grad:   tensor([-0.0115,  0.0652])\n",
      "Epoch 2250, Loss 2.940489\n",
      "    Params: tensor([  5.3002, -16.9225])\n",
      "    Grad:   tensor([-0.0115,  0.0650])\n",
      "Epoch 2251, Loss 2.940446\n",
      "    Params: tensor([  5.3003, -16.9232])\n",
      "    Grad:   tensor([-0.0115,  0.0649])\n",
      "Epoch 2252, Loss 2.940403\n",
      "    Params: tensor([  5.3004, -16.9238])\n",
      "    Grad:   tensor([-0.0114,  0.0648])\n",
      "Epoch 2253, Loss 2.940358\n",
      "    Params: tensor([  5.3005, -16.9245])\n",
      "    Grad:   tensor([-0.0114,  0.0647])\n",
      "Epoch 2254, Loss 2.940316\n",
      "    Params: tensor([  5.3006, -16.9251])\n",
      "    Grad:   tensor([-0.0114,  0.0646])\n",
      "Epoch 2255, Loss 2.940274\n",
      "    Params: tensor([  5.3008, -16.9257])\n",
      "    Grad:   tensor([-0.0114,  0.0645])\n",
      "Epoch 2256, Loss 2.940229\n",
      "    Params: tensor([  5.3009, -16.9264])\n",
      "    Grad:   tensor([-0.0114,  0.0644])\n",
      "Epoch 2257, Loss 2.940188\n",
      "    Params: tensor([  5.3010, -16.9270])\n",
      "    Grad:   tensor([-0.0114,  0.0643])\n",
      "Epoch 2258, Loss 2.940144\n",
      "    Params: tensor([  5.3011, -16.9277])\n",
      "    Grad:   tensor([-0.0114,  0.0642])\n",
      "Epoch 2259, Loss 2.940102\n",
      "    Params: tensor([  5.3012, -16.9283])\n",
      "    Grad:   tensor([-0.0113,  0.0641])\n",
      "Epoch 2260, Loss 2.940060\n",
      "    Params: tensor([  5.3013, -16.9290])\n",
      "    Grad:   tensor([-0.0113,  0.0640])\n",
      "Epoch 2261, Loss 2.940018\n",
      "    Params: tensor([  5.3014, -16.9296])\n",
      "    Grad:   tensor([-0.0113,  0.0638])\n",
      "Epoch 2262, Loss 2.939977\n",
      "    Params: tensor([  5.3016, -16.9302])\n",
      "    Grad:   tensor([-0.0113,  0.0637])\n",
      "Epoch 2263, Loss 2.939934\n",
      "    Params: tensor([  5.3017, -16.9309])\n",
      "    Grad:   tensor([-0.0112,  0.0636])\n",
      "Epoch 2264, Loss 2.939891\n",
      "    Params: tensor([  5.3018, -16.9315])\n",
      "    Grad:   tensor([-0.0112,  0.0635])\n",
      "Epoch 2265, Loss 2.939851\n",
      "    Params: tensor([  5.3019, -16.9321])\n",
      "    Grad:   tensor([-0.0112,  0.0634])\n",
      "Epoch 2266, Loss 2.939809\n",
      "    Params: tensor([  5.3020, -16.9328])\n",
      "    Grad:   tensor([-0.0112,  0.0633])\n",
      "Epoch 2267, Loss 2.939770\n",
      "    Params: tensor([  5.3021, -16.9334])\n",
      "    Grad:   tensor([-0.0112,  0.0632])\n",
      "Epoch 2268, Loss 2.939727\n",
      "    Params: tensor([  5.3022, -16.9340])\n",
      "    Grad:   tensor([-0.0111,  0.0631])\n",
      "Epoch 2269, Loss 2.939686\n",
      "    Params: tensor([  5.3023, -16.9347])\n",
      "    Grad:   tensor([-0.0111,  0.0630])\n",
      "Epoch 2270, Loss 2.939646\n",
      "    Params: tensor([  5.3024, -16.9353])\n",
      "    Grad:   tensor([-0.0111,  0.0629])\n",
      "Epoch 2271, Loss 2.939605\n",
      "    Params: tensor([  5.3026, -16.9359])\n",
      "    Grad:   tensor([-0.0111,  0.0628])\n",
      "Epoch 2272, Loss 2.939566\n",
      "    Params: tensor([  5.3027, -16.9365])\n",
      "    Grad:   tensor([-0.0111,  0.0627])\n",
      "Epoch 2273, Loss 2.939522\n",
      "    Params: tensor([  5.3028, -16.9372])\n",
      "    Grad:   tensor([-0.0111,  0.0626])\n",
      "Epoch 2274, Loss 2.939483\n",
      "    Params: tensor([  5.3029, -16.9378])\n",
      "    Grad:   tensor([-0.0110,  0.0624])\n",
      "Epoch 2275, Loss 2.939443\n",
      "    Params: tensor([  5.3030, -16.9384])\n",
      "    Grad:   tensor([-0.0110,  0.0623])\n",
      "Epoch 2276, Loss 2.939403\n",
      "    Params: tensor([  5.3031, -16.9390])\n",
      "    Grad:   tensor([-0.0110,  0.0622])\n",
      "Epoch 2277, Loss 2.939361\n",
      "    Params: tensor([  5.3032, -16.9397])\n",
      "    Grad:   tensor([-0.0110,  0.0621])\n",
      "Epoch 2278, Loss 2.939323\n",
      "    Params: tensor([  5.3033, -16.9403])\n",
      "    Grad:   tensor([-0.0110,  0.0620])\n",
      "Epoch 2279, Loss 2.939282\n",
      "    Params: tensor([  5.3034, -16.9409])\n",
      "    Grad:   tensor([-0.0109,  0.0619])\n",
      "Epoch 2280, Loss 2.939243\n",
      "    Params: tensor([  5.3035, -16.9415])\n",
      "    Grad:   tensor([-0.0109,  0.0618])\n",
      "Epoch 2281, Loss 2.939205\n",
      "    Params: tensor([  5.3037, -16.9421])\n",
      "    Grad:   tensor([-0.0109,  0.0617])\n",
      "Epoch 2282, Loss 2.939165\n",
      "    Params: tensor([  5.3038, -16.9428])\n",
      "    Grad:   tensor([-0.0109,  0.0616])\n",
      "Epoch 2283, Loss 2.939127\n",
      "    Params: tensor([  5.3039, -16.9434])\n",
      "    Grad:   tensor([-0.0109,  0.0615])\n",
      "Epoch 2284, Loss 2.939087\n",
      "    Params: tensor([  5.3040, -16.9440])\n",
      "    Grad:   tensor([-0.0108,  0.0614])\n",
      "Epoch 2285, Loss 2.939049\n",
      "    Params: tensor([  5.3041, -16.9446])\n",
      "    Grad:   tensor([-0.0108,  0.0613])\n",
      "Epoch 2286, Loss 2.939011\n",
      "    Params: tensor([  5.3042, -16.9452])\n",
      "    Grad:   tensor([-0.0108,  0.0612])\n",
      "Epoch 2287, Loss 2.938971\n",
      "    Params: tensor([  5.3043, -16.9458])\n",
      "    Grad:   tensor([-0.0108,  0.0611])\n",
      "Epoch 2288, Loss 2.938933\n",
      "    Params: tensor([  5.3044, -16.9464])\n",
      "    Grad:   tensor([-0.0108,  0.0610])\n",
      "Epoch 2289, Loss 2.938893\n",
      "    Params: tensor([  5.3045, -16.9470])\n",
      "    Grad:   tensor([-0.0108,  0.0609])\n",
      "Epoch 2290, Loss 2.938857\n",
      "    Params: tensor([  5.3046, -16.9476])\n",
      "    Grad:   tensor([-0.0107,  0.0608])\n",
      "Epoch 2291, Loss 2.938820\n",
      "    Params: tensor([  5.3047, -16.9482])\n",
      "    Grad:   tensor([-0.0107,  0.0607])\n",
      "Epoch 2292, Loss 2.938779\n",
      "    Params: tensor([  5.3048, -16.9489])\n",
      "    Grad:   tensor([-0.0107,  0.0606])\n",
      "Epoch 2293, Loss 2.938743\n",
      "    Params: tensor([  5.3049, -16.9495])\n",
      "    Grad:   tensor([-0.0107,  0.0605])\n",
      "Epoch 2294, Loss 2.938705\n",
      "    Params: tensor([  5.3051, -16.9501])\n",
      "    Grad:   tensor([-0.0107,  0.0604])\n",
      "Epoch 2295, Loss 2.938667\n",
      "    Params: tensor([  5.3052, -16.9507])\n",
      "    Grad:   tensor([-0.0106,  0.0603])\n",
      "Epoch 2296, Loss 2.938629\n",
      "    Params: tensor([  5.3053, -16.9513])\n",
      "    Grad:   tensor([-0.0106,  0.0602])\n",
      "Epoch 2297, Loss 2.938593\n",
      "    Params: tensor([  5.3054, -16.9519])\n",
      "    Grad:   tensor([-0.0106,  0.0601])\n",
      "Epoch 2298, Loss 2.938555\n",
      "    Params: tensor([  5.3055, -16.9525])\n",
      "    Grad:   tensor([-0.0106,  0.0600])\n",
      "Epoch 2299, Loss 2.938519\n",
      "    Params: tensor([  5.3056, -16.9531])\n",
      "    Grad:   tensor([-0.0106,  0.0598])\n",
      "Epoch 2300, Loss 2.938481\n",
      "    Params: tensor([  5.3057, -16.9537])\n",
      "    Grad:   tensor([-0.0106,  0.0597])\n",
      "Epoch 2301, Loss 2.938444\n",
      "    Params: tensor([  5.3058, -16.9543])\n",
      "    Grad:   tensor([-0.0105,  0.0596])\n",
      "Epoch 2302, Loss 2.938408\n",
      "    Params: tensor([  5.3059, -16.9549])\n",
      "    Grad:   tensor([-0.0105,  0.0595])\n",
      "Epoch 2303, Loss 2.938371\n",
      "    Params: tensor([  5.3060, -16.9554])\n",
      "    Grad:   tensor([-0.0105,  0.0594])\n",
      "Epoch 2304, Loss 2.938335\n",
      "    Params: tensor([  5.3061, -16.9560])\n",
      "    Grad:   tensor([-0.0105,  0.0593])\n",
      "Epoch 2305, Loss 2.938299\n",
      "    Params: tensor([  5.3062, -16.9566])\n",
      "    Grad:   tensor([-0.0105,  0.0592])\n",
      "Epoch 2306, Loss 2.938263\n",
      "    Params: tensor([  5.3063, -16.9572])\n",
      "    Grad:   tensor([-0.0105,  0.0591])\n",
      "Epoch 2307, Loss 2.938227\n",
      "    Params: tensor([  5.3064, -16.9578])\n",
      "    Grad:   tensor([-0.0104,  0.0590])\n",
      "Epoch 2308, Loss 2.938190\n",
      "    Params: tensor([  5.3065, -16.9584])\n",
      "    Grad:   tensor([-0.0104,  0.0589])\n",
      "Epoch 2309, Loss 2.938155\n",
      "    Params: tensor([  5.3066, -16.9590])\n",
      "    Grad:   tensor([-0.0104,  0.0588])\n",
      "Epoch 2310, Loss 2.938118\n",
      "    Params: tensor([  5.3067, -16.9596])\n",
      "    Grad:   tensor([-0.0104,  0.0587])\n",
      "Epoch 2311, Loss 2.938084\n",
      "    Params: tensor([  5.3068, -16.9602])\n",
      "    Grad:   tensor([-0.0104,  0.0586])\n",
      "Epoch 2312, Loss 2.938049\n",
      "    Params: tensor([  5.3069, -16.9608])\n",
      "    Grad:   tensor([-0.0103,  0.0585])\n",
      "Epoch 2313, Loss 2.938014\n",
      "    Params: tensor([  5.3070, -16.9613])\n",
      "    Grad:   tensor([-0.0103,  0.0584])\n",
      "Epoch 2314, Loss 2.937977\n",
      "    Params: tensor([  5.3072, -16.9619])\n",
      "    Grad:   tensor([-0.0103,  0.0583])\n",
      "Epoch 2315, Loss 2.937943\n",
      "    Params: tensor([  5.3073, -16.9625])\n",
      "    Grad:   tensor([-0.0103,  0.0582])\n",
      "Epoch 2316, Loss 2.937908\n",
      "    Params: tensor([  5.3074, -16.9631])\n",
      "    Grad:   tensor([-0.0103,  0.0581])\n",
      "Epoch 2317, Loss 2.937872\n",
      "    Params: tensor([  5.3075, -16.9637])\n",
      "    Grad:   tensor([-0.0103,  0.0580])\n",
      "Epoch 2318, Loss 2.937839\n",
      "    Params: tensor([  5.3076, -16.9642])\n",
      "    Grad:   tensor([-0.0102,  0.0580])\n",
      "Epoch 2319, Loss 2.937804\n",
      "    Params: tensor([  5.3077, -16.9648])\n",
      "    Grad:   tensor([-0.0102,  0.0578])\n",
      "Epoch 2320, Loss 2.937769\n",
      "    Params: tensor([  5.3078, -16.9654])\n",
      "    Grad:   tensor([-0.0102,  0.0578])\n",
      "Epoch 2321, Loss 2.937734\n",
      "    Params: tensor([  5.3079, -16.9660])\n",
      "    Grad:   tensor([-0.0102,  0.0577])\n",
      "Epoch 2322, Loss 2.937700\n",
      "    Params: tensor([  5.3080, -16.9666])\n",
      "    Grad:   tensor([-0.0102,  0.0576])\n",
      "Epoch 2323, Loss 2.937665\n",
      "    Params: tensor([  5.3081, -16.9671])\n",
      "    Grad:   tensor([-0.0102,  0.0575])\n",
      "Epoch 2324, Loss 2.937632\n",
      "    Params: tensor([  5.3082, -16.9677])\n",
      "    Grad:   tensor([-0.0101,  0.0574])\n",
      "Epoch 2325, Loss 2.937598\n",
      "    Params: tensor([  5.3083, -16.9683])\n",
      "    Grad:   tensor([-0.0101,  0.0573])\n",
      "Epoch 2326, Loss 2.937565\n",
      "    Params: tensor([  5.3084, -16.9688])\n",
      "    Grad:   tensor([-0.0101,  0.0572])\n",
      "Epoch 2327, Loss 2.937531\n",
      "    Params: tensor([  5.3085, -16.9694])\n",
      "    Grad:   tensor([-0.0101,  0.0571])\n",
      "Epoch 2328, Loss 2.937499\n",
      "    Params: tensor([  5.3086, -16.9700])\n",
      "    Grad:   tensor([-0.0101,  0.0570])\n",
      "Epoch 2329, Loss 2.937465\n",
      "    Params: tensor([  5.3087, -16.9706])\n",
      "    Grad:   tensor([-0.0101,  0.0569])\n",
      "Epoch 2330, Loss 2.937430\n",
      "    Params: tensor([  5.3088, -16.9711])\n",
      "    Grad:   tensor([-0.0100,  0.0568])\n",
      "Epoch 2331, Loss 2.937398\n",
      "    Params: tensor([  5.3089, -16.9717])\n",
      "    Grad:   tensor([-0.0100,  0.0567])\n",
      "Epoch 2332, Loss 2.937364\n",
      "    Params: tensor([  5.3090, -16.9723])\n",
      "    Grad:   tensor([-0.0100,  0.0566])\n",
      "Epoch 2333, Loss 2.937332\n",
      "    Params: tensor([  5.3091, -16.9728])\n",
      "    Grad:   tensor([-0.0100,  0.0565])\n",
      "Epoch 2334, Loss 2.937299\n",
      "    Params: tensor([  5.3092, -16.9734])\n",
      "    Grad:   tensor([-0.0100,  0.0564])\n",
      "Epoch 2335, Loss 2.937265\n",
      "    Params: tensor([  5.3093, -16.9739])\n",
      "    Grad:   tensor([-0.0100,  0.0563])\n",
      "Epoch 2336, Loss 2.937232\n",
      "    Params: tensor([  5.3094, -16.9745])\n",
      "    Grad:   tensor([-0.0099,  0.0562])\n",
      "Epoch 2337, Loss 2.937201\n",
      "    Params: tensor([  5.3095, -16.9751])\n",
      "    Grad:   tensor([-0.0099,  0.0561])\n",
      "Epoch 2338, Loss 2.937167\n",
      "    Params: tensor([  5.3096, -16.9756])\n",
      "    Grad:   tensor([-0.0099,  0.0560])\n",
      "Epoch 2339, Loss 2.937134\n",
      "    Params: tensor([  5.3097, -16.9762])\n",
      "    Grad:   tensor([-0.0099,  0.0559])\n",
      "Epoch 2340, Loss 2.937104\n",
      "    Params: tensor([  5.3098, -16.9767])\n",
      "    Grad:   tensor([-0.0099,  0.0558])\n",
      "Epoch 2341, Loss 2.937071\n",
      "    Params: tensor([  5.3099, -16.9773])\n",
      "    Grad:   tensor([-0.0098,  0.0557])\n",
      "Epoch 2342, Loss 2.937039\n",
      "    Params: tensor([  5.3100, -16.9779])\n",
      "    Grad:   tensor([-0.0098,  0.0556])\n",
      "Epoch 2343, Loss 2.937008\n",
      "    Params: tensor([  5.3101, -16.9784])\n",
      "    Grad:   tensor([-0.0098,  0.0555])\n",
      "Epoch 2344, Loss 2.936976\n",
      "    Params: tensor([  5.3102, -16.9790])\n",
      "    Grad:   tensor([-0.0098,  0.0554])\n",
      "Epoch 2345, Loss 2.936945\n",
      "    Params: tensor([  5.3103, -16.9795])\n",
      "    Grad:   tensor([-0.0098,  0.0553])\n",
      "Epoch 2346, Loss 2.936912\n",
      "    Params: tensor([  5.3104, -16.9801])\n",
      "    Grad:   tensor([-0.0098,  0.0553])\n",
      "Epoch 2347, Loss 2.936883\n",
      "    Params: tensor([  5.3105, -16.9806])\n",
      "    Grad:   tensor([-0.0097,  0.0552])\n",
      "Epoch 2348, Loss 2.936851\n",
      "    Params: tensor([  5.3106, -16.9812])\n",
      "    Grad:   tensor([-0.0097,  0.0551])\n",
      "Epoch 2349, Loss 2.936819\n",
      "    Params: tensor([  5.3107, -16.9817])\n",
      "    Grad:   tensor([-0.0097,  0.0550])\n",
      "Epoch 2350, Loss 2.936788\n",
      "    Params: tensor([  5.3107, -16.9823])\n",
      "    Grad:   tensor([-0.0097,  0.0549])\n",
      "Epoch 2351, Loss 2.936757\n",
      "    Params: tensor([  5.3108, -16.9828])\n",
      "    Grad:   tensor([-0.0097,  0.0548])\n",
      "Epoch 2352, Loss 2.936725\n",
      "    Params: tensor([  5.3109, -16.9834])\n",
      "    Grad:   tensor([-0.0097,  0.0547])\n",
      "Epoch 2353, Loss 2.936694\n",
      "    Params: tensor([  5.3110, -16.9839])\n",
      "    Grad:   tensor([-0.0096,  0.0546])\n",
      "Epoch 2354, Loss 2.936665\n",
      "    Params: tensor([  5.3111, -16.9845])\n",
      "    Grad:   tensor([-0.0096,  0.0545])\n",
      "Epoch 2355, Loss 2.936633\n",
      "    Params: tensor([  5.3112, -16.9850])\n",
      "    Grad:   tensor([-0.0096,  0.0544])\n",
      "Epoch 2356, Loss 2.936602\n",
      "    Params: tensor([  5.3113, -16.9856])\n",
      "    Grad:   tensor([-0.0096,  0.0543])\n",
      "Epoch 2357, Loss 2.936572\n",
      "    Params: tensor([  5.3114, -16.9861])\n",
      "    Grad:   tensor([-0.0096,  0.0542])\n",
      "Epoch 2358, Loss 2.936542\n",
      "    Params: tensor([  5.3115, -16.9866])\n",
      "    Grad:   tensor([-0.0095,  0.0541])\n",
      "Epoch 2359, Loss 2.936511\n",
      "    Params: tensor([  5.3116, -16.9872])\n",
      "    Grad:   tensor([-0.0096,  0.0540])\n",
      "Epoch 2360, Loss 2.936481\n",
      "    Params: tensor([  5.3117, -16.9877])\n",
      "    Grad:   tensor([-0.0095,  0.0540])\n",
      "Epoch 2361, Loss 2.936451\n",
      "    Params: tensor([  5.3118, -16.9883])\n",
      "    Grad:   tensor([-0.0095,  0.0539])\n",
      "Epoch 2362, Loss 2.936421\n",
      "    Params: tensor([  5.3119, -16.9888])\n",
      "    Grad:   tensor([-0.0095,  0.0538])\n",
      "Epoch 2363, Loss 2.936392\n",
      "    Params: tensor([  5.3120, -16.9893])\n",
      "    Grad:   tensor([-0.0095,  0.0537])\n",
      "Epoch 2364, Loss 2.936362\n",
      "    Params: tensor([  5.3121, -16.9899])\n",
      "    Grad:   tensor([-0.0094,  0.0536])\n",
      "Epoch 2365, Loss 2.936332\n",
      "    Params: tensor([  5.3122, -16.9904])\n",
      "    Grad:   tensor([-0.0094,  0.0535])\n",
      "Epoch 2366, Loss 2.936304\n",
      "    Params: tensor([  5.3123, -16.9909])\n",
      "    Grad:   tensor([-0.0094,  0.0534])\n",
      "Epoch 2367, Loss 2.936274\n",
      "    Params: tensor([  5.3124, -16.9915])\n",
      "    Grad:   tensor([-0.0094,  0.0533])\n",
      "Epoch 2368, Loss 2.936244\n",
      "    Params: tensor([  5.3125, -16.9920])\n",
      "    Grad:   tensor([-0.0094,  0.0532])\n",
      "Epoch 2369, Loss 2.936216\n",
      "    Params: tensor([  5.3126, -16.9925])\n",
      "    Grad:   tensor([-0.0094,  0.0531])\n",
      "Epoch 2370, Loss 2.936188\n",
      "    Params: tensor([  5.3127, -16.9931])\n",
      "    Grad:   tensor([-0.0094,  0.0530])\n",
      "Epoch 2371, Loss 2.936156\n",
      "    Params: tensor([  5.3127, -16.9936])\n",
      "    Grad:   tensor([-0.0094,  0.0530])\n",
      "Epoch 2372, Loss 2.936128\n",
      "    Params: tensor([  5.3128, -16.9941])\n",
      "    Grad:   tensor([-0.0093,  0.0529])\n",
      "Epoch 2373, Loss 2.936100\n",
      "    Params: tensor([  5.3129, -16.9946])\n",
      "    Grad:   tensor([-0.0093,  0.0528])\n",
      "Epoch 2374, Loss 2.936072\n",
      "    Params: tensor([  5.3130, -16.9952])\n",
      "    Grad:   tensor([-0.0093,  0.0527])\n",
      "Epoch 2375, Loss 2.936042\n",
      "    Params: tensor([  5.3131, -16.9957])\n",
      "    Grad:   tensor([-0.0093,  0.0526])\n",
      "Epoch 2376, Loss 2.936014\n",
      "    Params: tensor([  5.3132, -16.9962])\n",
      "    Grad:   tensor([-0.0093,  0.0525])\n",
      "Epoch 2377, Loss 2.935986\n",
      "    Params: tensor([  5.3133, -16.9967])\n",
      "    Grad:   tensor([-0.0093,  0.0524])\n",
      "Epoch 2378, Loss 2.935957\n",
      "    Params: tensor([  5.3134, -16.9973])\n",
      "    Grad:   tensor([-0.0093,  0.0523])\n",
      "Epoch 2379, Loss 2.935928\n",
      "    Params: tensor([  5.3135, -16.9978])\n",
      "    Grad:   tensor([-0.0092,  0.0522])\n",
      "Epoch 2380, Loss 2.935901\n",
      "    Params: tensor([  5.3136, -16.9983])\n",
      "    Grad:   tensor([-0.0092,  0.0522])\n",
      "Epoch 2381, Loss 2.935873\n",
      "    Params: tensor([  5.3137, -16.9988])\n",
      "    Grad:   tensor([-0.0092,  0.0521])\n",
      "Epoch 2382, Loss 2.935845\n",
      "    Params: tensor([  5.3138, -16.9994])\n",
      "    Grad:   tensor([-0.0092,  0.0520])\n",
      "Epoch 2383, Loss 2.935817\n",
      "    Params: tensor([  5.3139, -16.9999])\n",
      "    Grad:   tensor([-0.0092,  0.0519])\n",
      "Epoch 2384, Loss 2.935789\n",
      "    Params: tensor([  5.3139, -17.0004])\n",
      "    Grad:   tensor([-0.0092,  0.0518])\n",
      "Epoch 2385, Loss 2.935762\n",
      "    Params: tensor([  5.3140, -17.0009])\n",
      "    Grad:   tensor([-0.0092,  0.0517])\n",
      "Epoch 2386, Loss 2.935734\n",
      "    Params: tensor([  5.3141, -17.0014])\n",
      "    Grad:   tensor([-0.0091,  0.0516])\n",
      "Epoch 2387, Loss 2.935707\n",
      "    Params: tensor([  5.3142, -17.0019])\n",
      "    Grad:   tensor([-0.0091,  0.0515])\n",
      "Epoch 2388, Loss 2.935679\n",
      "    Params: tensor([  5.3143, -17.0025])\n",
      "    Grad:   tensor([-0.0091,  0.0514])\n",
      "Epoch 2389, Loss 2.935650\n",
      "    Params: tensor([  5.3144, -17.0030])\n",
      "    Grad:   tensor([-0.0091,  0.0514])\n",
      "Epoch 2390, Loss 2.935626\n",
      "    Params: tensor([  5.3145, -17.0035])\n",
      "    Grad:   tensor([-0.0090,  0.0513])\n",
      "Epoch 2391, Loss 2.935596\n",
      "    Params: tensor([  5.3146, -17.0040])\n",
      "    Grad:   tensor([-0.0090,  0.0512])\n",
      "Epoch 2392, Loss 2.935571\n",
      "    Params: tensor([  5.3147, -17.0045])\n",
      "    Grad:   tensor([-0.0090,  0.0511])\n",
      "Epoch 2393, Loss 2.935544\n",
      "    Params: tensor([  5.3148, -17.0050])\n",
      "    Grad:   tensor([-0.0090,  0.0510])\n",
      "Epoch 2394, Loss 2.935516\n",
      "    Params: tensor([  5.3149, -17.0055])\n",
      "    Grad:   tensor([-0.0090,  0.0509])\n",
      "Epoch 2395, Loss 2.935489\n",
      "    Params: tensor([  5.3149, -17.0060])\n",
      "    Grad:   tensor([-0.0090,  0.0508])\n",
      "Epoch 2396, Loss 2.935465\n",
      "    Params: tensor([  5.3150, -17.0065])\n",
      "    Grad:   tensor([-0.0090,  0.0507])\n",
      "Epoch 2397, Loss 2.935436\n",
      "    Params: tensor([  5.3151, -17.0070])\n",
      "    Grad:   tensor([-0.0090,  0.0507])\n",
      "Epoch 2398, Loss 2.935411\n",
      "    Params: tensor([  5.3152, -17.0076])\n",
      "    Grad:   tensor([-0.0089,  0.0506])\n",
      "Epoch 2399, Loss 2.935385\n",
      "    Params: tensor([  5.3153, -17.0081])\n",
      "    Grad:   tensor([-0.0089,  0.0505])\n",
      "Epoch 2400, Loss 2.935356\n",
      "    Params: tensor([  5.3154, -17.0086])\n",
      "    Grad:   tensor([-0.0089,  0.0504])\n",
      "Epoch 2401, Loss 2.935332\n",
      "    Params: tensor([  5.3155, -17.0091])\n",
      "    Grad:   tensor([-0.0089,  0.0503])\n",
      "Epoch 2402, Loss 2.935304\n",
      "    Params: tensor([  5.3156, -17.0096])\n",
      "    Grad:   tensor([-0.0089,  0.0502])\n",
      "Epoch 2403, Loss 2.935281\n",
      "    Params: tensor([  5.3157, -17.0101])\n",
      "    Grad:   tensor([-0.0088,  0.0502])\n",
      "Epoch 2404, Loss 2.935252\n",
      "    Params: tensor([  5.3157, -17.0106])\n",
      "    Grad:   tensor([-0.0088,  0.0501])\n",
      "Epoch 2405, Loss 2.935228\n",
      "    Params: tensor([  5.3158, -17.0111])\n",
      "    Grad:   tensor([-0.0088,  0.0500])\n",
      "Epoch 2406, Loss 2.935203\n",
      "    Params: tensor([  5.3159, -17.0116])\n",
      "    Grad:   tensor([-0.0088,  0.0499])\n",
      "Epoch 2407, Loss 2.935177\n",
      "    Params: tensor([  5.3160, -17.0121])\n",
      "    Grad:   tensor([-0.0088,  0.0498])\n",
      "Epoch 2408, Loss 2.935152\n",
      "    Params: tensor([  5.3161, -17.0126])\n",
      "    Grad:   tensor([-0.0088,  0.0497])\n",
      "Epoch 2409, Loss 2.935126\n",
      "    Params: tensor([  5.3162, -17.0131])\n",
      "    Grad:   tensor([-0.0088,  0.0496])\n",
      "Epoch 2410, Loss 2.935100\n",
      "    Params: tensor([  5.3163, -17.0136])\n",
      "    Grad:   tensor([-0.0088,  0.0496])\n",
      "Epoch 2411, Loss 2.935075\n",
      "    Params: tensor([  5.3164, -17.0140])\n",
      "    Grad:   tensor([-0.0087,  0.0495])\n",
      "Epoch 2412, Loss 2.935049\n",
      "    Params: tensor([  5.3164, -17.0145])\n",
      "    Grad:   tensor([-0.0087,  0.0494])\n",
      "Epoch 2413, Loss 2.935024\n",
      "    Params: tensor([  5.3165, -17.0150])\n",
      "    Grad:   tensor([-0.0087,  0.0493])\n",
      "Epoch 2414, Loss 2.935001\n",
      "    Params: tensor([  5.3166, -17.0155])\n",
      "    Grad:   tensor([-0.0087,  0.0492])\n",
      "Epoch 2415, Loss 2.934973\n",
      "    Params: tensor([  5.3167, -17.0160])\n",
      "    Grad:   tensor([-0.0087,  0.0491])\n",
      "Epoch 2416, Loss 2.934949\n",
      "    Params: tensor([  5.3168, -17.0165])\n",
      "    Grad:   tensor([-0.0087,  0.0491])\n",
      "Epoch 2417, Loss 2.934925\n",
      "    Params: tensor([  5.3169, -17.0170])\n",
      "    Grad:   tensor([-0.0086,  0.0490])\n",
      "Epoch 2418, Loss 2.934899\n",
      "    Params: tensor([  5.3170, -17.0175])\n",
      "    Grad:   tensor([-0.0086,  0.0489])\n",
      "Epoch 2419, Loss 2.934876\n",
      "    Params: tensor([  5.3171, -17.0180])\n",
      "    Grad:   tensor([-0.0086,  0.0488])\n",
      "Epoch 2420, Loss 2.934853\n",
      "    Params: tensor([  5.3171, -17.0185])\n",
      "    Grad:   tensor([-0.0086,  0.0487])\n",
      "Epoch 2421, Loss 2.934826\n",
      "    Params: tensor([  5.3172, -17.0189])\n",
      "    Grad:   tensor([-0.0086,  0.0486])\n",
      "Epoch 2422, Loss 2.934802\n",
      "    Params: tensor([  5.3173, -17.0194])\n",
      "    Grad:   tensor([-0.0086,  0.0486])\n",
      "Epoch 2423, Loss 2.934777\n",
      "    Params: tensor([  5.3174, -17.0199])\n",
      "    Grad:   tensor([-0.0086,  0.0485])\n",
      "Epoch 2424, Loss 2.934753\n",
      "    Params: tensor([  5.3175, -17.0204])\n",
      "    Grad:   tensor([-0.0086,  0.0484])\n",
      "Epoch 2425, Loss 2.934730\n",
      "    Params: tensor([  5.3176, -17.0209])\n",
      "    Grad:   tensor([-0.0086,  0.0483])\n",
      "Epoch 2426, Loss 2.934705\n",
      "    Params: tensor([  5.3177, -17.0214])\n",
      "    Grad:   tensor([-0.0085,  0.0482])\n",
      "Epoch 2427, Loss 2.934681\n",
      "    Params: tensor([  5.3177, -17.0219])\n",
      "    Grad:   tensor([-0.0085,  0.0481])\n",
      "Epoch 2428, Loss 2.934658\n",
      "    Params: tensor([  5.3178, -17.0223])\n",
      "    Grad:   tensor([-0.0085,  0.0481])\n",
      "Epoch 2429, Loss 2.934635\n",
      "    Params: tensor([  5.3179, -17.0228])\n",
      "    Grad:   tensor([-0.0085,  0.0480])\n",
      "Epoch 2430, Loss 2.934609\n",
      "    Params: tensor([  5.3180, -17.0233])\n",
      "    Grad:   tensor([-0.0085,  0.0479])\n",
      "Epoch 2431, Loss 2.934585\n",
      "    Params: tensor([  5.3181, -17.0238])\n",
      "    Grad:   tensor([-0.0084,  0.0478])\n",
      "Epoch 2432, Loss 2.934563\n",
      "    Params: tensor([  5.3182, -17.0242])\n",
      "    Grad:   tensor([-0.0084,  0.0477])\n",
      "Epoch 2433, Loss 2.934541\n",
      "    Params: tensor([  5.3182, -17.0247])\n",
      "    Grad:   tensor([-0.0084,  0.0477])\n",
      "Epoch 2434, Loss 2.934516\n",
      "    Params: tensor([  5.3183, -17.0252])\n",
      "    Grad:   tensor([-0.0084,  0.0476])\n",
      "Epoch 2435, Loss 2.934493\n",
      "    Params: tensor([  5.3184, -17.0257])\n",
      "    Grad:   tensor([-0.0084,  0.0475])\n",
      "Epoch 2436, Loss 2.934469\n",
      "    Params: tensor([  5.3185, -17.0261])\n",
      "    Grad:   tensor([-0.0084,  0.0474])\n",
      "Epoch 2437, Loss 2.934446\n",
      "    Params: tensor([  5.3186, -17.0266])\n",
      "    Grad:   tensor([-0.0084,  0.0473])\n",
      "Epoch 2438, Loss 2.934423\n",
      "    Params: tensor([  5.3187, -17.0271])\n",
      "    Grad:   tensor([-0.0083,  0.0473])\n",
      "Epoch 2439, Loss 2.934400\n",
      "    Params: tensor([  5.3187, -17.0276])\n",
      "    Grad:   tensor([-0.0083,  0.0472])\n",
      "Epoch 2440, Loss 2.934377\n",
      "    Params: tensor([  5.3188, -17.0280])\n",
      "    Grad:   tensor([-0.0083,  0.0471])\n",
      "Epoch 2441, Loss 2.934355\n",
      "    Params: tensor([  5.3189, -17.0285])\n",
      "    Grad:   tensor([-0.0083,  0.0470])\n",
      "Epoch 2442, Loss 2.934331\n",
      "    Params: tensor([  5.3190, -17.0290])\n",
      "    Grad:   tensor([-0.0083,  0.0469])\n",
      "Epoch 2443, Loss 2.934309\n",
      "    Params: tensor([  5.3191, -17.0294])\n",
      "    Grad:   tensor([-0.0083,  0.0469])\n",
      "Epoch 2444, Loss 2.934287\n",
      "    Params: tensor([  5.3192, -17.0299])\n",
      "    Grad:   tensor([-0.0083,  0.0468])\n",
      "Epoch 2445, Loss 2.934264\n",
      "    Params: tensor([  5.3192, -17.0304])\n",
      "    Grad:   tensor([-0.0083,  0.0467])\n",
      "Epoch 2446, Loss 2.934242\n",
      "    Params: tensor([  5.3193, -17.0308])\n",
      "    Grad:   tensor([-0.0083,  0.0466])\n",
      "Epoch 2447, Loss 2.934219\n",
      "    Params: tensor([  5.3194, -17.0313])\n",
      "    Grad:   tensor([-0.0082,  0.0465])\n",
      "Epoch 2448, Loss 2.934198\n",
      "    Params: tensor([  5.3195, -17.0318])\n",
      "    Grad:   tensor([-0.0082,  0.0465])\n",
      "Epoch 2449, Loss 2.934175\n",
      "    Params: tensor([  5.3196, -17.0322])\n",
      "    Grad:   tensor([-0.0082,  0.0464])\n",
      "Epoch 2450, Loss 2.934151\n",
      "    Params: tensor([  5.3197, -17.0327])\n",
      "    Grad:   tensor([-0.0082,  0.0463])\n",
      "Epoch 2451, Loss 2.934129\n",
      "    Params: tensor([  5.3197, -17.0332])\n",
      "    Grad:   tensor([-0.0082,  0.0462])\n",
      "Epoch 2452, Loss 2.934108\n",
      "    Params: tensor([  5.3198, -17.0336])\n",
      "    Grad:   tensor([-0.0082,  0.0461])\n",
      "Epoch 2453, Loss 2.934084\n",
      "    Params: tensor([  5.3199, -17.0341])\n",
      "    Grad:   tensor([-0.0081,  0.0461])\n",
      "Epoch 2454, Loss 2.934065\n",
      "    Params: tensor([  5.3200, -17.0345])\n",
      "    Grad:   tensor([-0.0081,  0.0460])\n",
      "Epoch 2455, Loss 2.934043\n",
      "    Params: tensor([  5.3201, -17.0350])\n",
      "    Grad:   tensor([-0.0081,  0.0459])\n",
      "Epoch 2456, Loss 2.934020\n",
      "    Params: tensor([  5.3201, -17.0355])\n",
      "    Grad:   tensor([-0.0081,  0.0458])\n",
      "Epoch 2457, Loss 2.934000\n",
      "    Params: tensor([  5.3202, -17.0359])\n",
      "    Grad:   tensor([-0.0081,  0.0457])\n",
      "Epoch 2458, Loss 2.933978\n",
      "    Params: tensor([  5.3203, -17.0364])\n",
      "    Grad:   tensor([-0.0081,  0.0457])\n",
      "Epoch 2459, Loss 2.933956\n",
      "    Params: tensor([  5.3204, -17.0368])\n",
      "    Grad:   tensor([-0.0080,  0.0456])\n",
      "Epoch 2460, Loss 2.933935\n",
      "    Params: tensor([  5.3205, -17.0373])\n",
      "    Grad:   tensor([-0.0080,  0.0455])\n",
      "Epoch 2461, Loss 2.933914\n",
      "    Params: tensor([  5.3205, -17.0377])\n",
      "    Grad:   tensor([-0.0080,  0.0454])\n",
      "Epoch 2462, Loss 2.933893\n",
      "    Params: tensor([  5.3206, -17.0382])\n",
      "    Grad:   tensor([-0.0080,  0.0454])\n",
      "Epoch 2463, Loss 2.933871\n",
      "    Params: tensor([  5.3207, -17.0386])\n",
      "    Grad:   tensor([-0.0080,  0.0453])\n",
      "Epoch 2464, Loss 2.933849\n",
      "    Params: tensor([  5.3208, -17.0391])\n",
      "    Grad:   tensor([-0.0080,  0.0452])\n",
      "Epoch 2465, Loss 2.933828\n",
      "    Params: tensor([  5.3209, -17.0396])\n",
      "    Grad:   tensor([-0.0080,  0.0451])\n",
      "Epoch 2466, Loss 2.933807\n",
      "    Params: tensor([  5.3209, -17.0400])\n",
      "    Grad:   tensor([-0.0080,  0.0451])\n",
      "Epoch 2467, Loss 2.933787\n",
      "    Params: tensor([  5.3210, -17.0405])\n",
      "    Grad:   tensor([-0.0079,  0.0450])\n",
      "Epoch 2468, Loss 2.933767\n",
      "    Params: tensor([  5.3211, -17.0409])\n",
      "    Grad:   tensor([-0.0079,  0.0449])\n",
      "Epoch 2469, Loss 2.933746\n",
      "    Params: tensor([  5.3212, -17.0413])\n",
      "    Grad:   tensor([-0.0079,  0.0448])\n",
      "Epoch 2470, Loss 2.933723\n",
      "    Params: tensor([  5.3213, -17.0418])\n",
      "    Grad:   tensor([-0.0079,  0.0448])\n",
      "Epoch 2471, Loss 2.933704\n",
      "    Params: tensor([  5.3213, -17.0422])\n",
      "    Grad:   tensor([-0.0079,  0.0447])\n",
      "Epoch 2472, Loss 2.933682\n",
      "    Params: tensor([  5.3214, -17.0427])\n",
      "    Grad:   tensor([-0.0079,  0.0446])\n",
      "Epoch 2473, Loss 2.933662\n",
      "    Params: tensor([  5.3215, -17.0431])\n",
      "    Grad:   tensor([-0.0079,  0.0445])\n",
      "Epoch 2474, Loss 2.933643\n",
      "    Params: tensor([  5.3216, -17.0436])\n",
      "    Grad:   tensor([-0.0079,  0.0444])\n",
      "Epoch 2475, Loss 2.933622\n",
      "    Params: tensor([  5.3217, -17.0440])\n",
      "    Grad:   tensor([-0.0078,  0.0444])\n",
      "Epoch 2476, Loss 2.933602\n",
      "    Params: tensor([  5.3217, -17.0445])\n",
      "    Grad:   tensor([-0.0078,  0.0443])\n",
      "Epoch 2477, Loss 2.933583\n",
      "    Params: tensor([  5.3218, -17.0449])\n",
      "    Grad:   tensor([-0.0078,  0.0442])\n",
      "Epoch 2478, Loss 2.933561\n",
      "    Params: tensor([  5.3219, -17.0453])\n",
      "    Grad:   tensor([-0.0078,  0.0441])\n",
      "Epoch 2479, Loss 2.933541\n",
      "    Params: tensor([  5.3220, -17.0458])\n",
      "    Grad:   tensor([-0.0078,  0.0441])\n",
      "Epoch 2480, Loss 2.933521\n",
      "    Params: tensor([  5.3220, -17.0462])\n",
      "    Grad:   tensor([-0.0078,  0.0440])\n",
      "Epoch 2481, Loss 2.933501\n",
      "    Params: tensor([  5.3221, -17.0467])\n",
      "    Grad:   tensor([-0.0078,  0.0439])\n",
      "Epoch 2482, Loss 2.933480\n",
      "    Params: tensor([  5.3222, -17.0471])\n",
      "    Grad:   tensor([-0.0077,  0.0438])\n",
      "Epoch 2483, Loss 2.933463\n",
      "    Params: tensor([  5.3223, -17.0475])\n",
      "    Grad:   tensor([-0.0077,  0.0438])\n",
      "Epoch 2484, Loss 2.933442\n",
      "    Params: tensor([  5.3224, -17.0480])\n",
      "    Grad:   tensor([-0.0077,  0.0437])\n",
      "Epoch 2485, Loss 2.933422\n",
      "    Params: tensor([  5.3224, -17.0484])\n",
      "    Grad:   tensor([-0.0077,  0.0436])\n",
      "Epoch 2486, Loss 2.933403\n",
      "    Params: tensor([  5.3225, -17.0489])\n",
      "    Grad:   tensor([-0.0077,  0.0436])\n",
      "Epoch 2487, Loss 2.933382\n",
      "    Params: tensor([  5.3226, -17.0493])\n",
      "    Grad:   tensor([-0.0077,  0.0435])\n",
      "Epoch 2488, Loss 2.933365\n",
      "    Params: tensor([  5.3227, -17.0497])\n",
      "    Grad:   tensor([-0.0077,  0.0434])\n",
      "Epoch 2489, Loss 2.933345\n",
      "    Params: tensor([  5.3227, -17.0502])\n",
      "    Grad:   tensor([-0.0077,  0.0433])\n",
      "Epoch 2490, Loss 2.933325\n",
      "    Params: tensor([  5.3228, -17.0506])\n",
      "    Grad:   tensor([-0.0076,  0.0433])\n",
      "Epoch 2491, Loss 2.933306\n",
      "    Params: tensor([  5.3229, -17.0510])\n",
      "    Grad:   tensor([-0.0076,  0.0432])\n",
      "Epoch 2492, Loss 2.933287\n",
      "    Params: tensor([  5.3230, -17.0515])\n",
      "    Grad:   tensor([-0.0076,  0.0431])\n",
      "Epoch 2493, Loss 2.933266\n",
      "    Params: tensor([  5.3230, -17.0519])\n",
      "    Grad:   tensor([-0.0076,  0.0430])\n",
      "Epoch 2494, Loss 2.933249\n",
      "    Params: tensor([  5.3231, -17.0523])\n",
      "    Grad:   tensor([-0.0076,  0.0430])\n",
      "Epoch 2495, Loss 2.933229\n",
      "    Params: tensor([  5.3232, -17.0527])\n",
      "    Grad:   tensor([-0.0076,  0.0429])\n",
      "Epoch 2496, Loss 2.933209\n",
      "    Params: tensor([  5.3233, -17.0532])\n",
      "    Grad:   tensor([-0.0076,  0.0428])\n",
      "Epoch 2497, Loss 2.933190\n",
      "    Params: tensor([  5.3233, -17.0536])\n",
      "    Grad:   tensor([-0.0075,  0.0427])\n",
      "Epoch 2498, Loss 2.933172\n",
      "    Params: tensor([  5.3234, -17.0540])\n",
      "    Grad:   tensor([-0.0075,  0.0427])\n",
      "Epoch 2499, Loss 2.933154\n",
      "    Params: tensor([  5.3235, -17.0544])\n",
      "    Grad:   tensor([-0.0075,  0.0426])\n",
      "Epoch 2500, Loss 2.933134\n",
      "    Params: tensor([  5.3236, -17.0549])\n",
      "    Grad:   tensor([-0.0075,  0.0425])\n",
      "Epoch 2501, Loss 2.933116\n",
      "    Params: tensor([  5.3236, -17.0553])\n",
      "    Grad:   tensor([-0.0075,  0.0425])\n",
      "Epoch 2502, Loss 2.933097\n",
      "    Params: tensor([  5.3237, -17.0557])\n",
      "    Grad:   tensor([-0.0075,  0.0424])\n",
      "Epoch 2503, Loss 2.933079\n",
      "    Params: tensor([  5.3238, -17.0561])\n",
      "    Grad:   tensor([-0.0075,  0.0423])\n",
      "Epoch 2504, Loss 2.933060\n",
      "    Params: tensor([  5.3239, -17.0566])\n",
      "    Grad:   tensor([-0.0075,  0.0422])\n",
      "Epoch 2505, Loss 2.933043\n",
      "    Params: tensor([  5.3239, -17.0570])\n",
      "    Grad:   tensor([-0.0074,  0.0422])\n",
      "Epoch 2506, Loss 2.933025\n",
      "    Params: tensor([  5.3240, -17.0574])\n",
      "    Grad:   tensor([-0.0074,  0.0421])\n",
      "Epoch 2507, Loss 2.933007\n",
      "    Params: tensor([  5.3241, -17.0578])\n",
      "    Grad:   tensor([-0.0074,  0.0420])\n",
      "Epoch 2508, Loss 2.932988\n",
      "    Params: tensor([  5.3242, -17.0582])\n",
      "    Grad:   tensor([-0.0074,  0.0420])\n",
      "Epoch 2509, Loss 2.932970\n",
      "    Params: tensor([  5.3242, -17.0587])\n",
      "    Grad:   tensor([-0.0074,  0.0419])\n",
      "Epoch 2510, Loss 2.932953\n",
      "    Params: tensor([  5.3243, -17.0591])\n",
      "    Grad:   tensor([-0.0074,  0.0418])\n",
      "Epoch 2511, Loss 2.932932\n",
      "    Params: tensor([  5.3244, -17.0595])\n",
      "    Grad:   tensor([-0.0074,  0.0417])\n",
      "Epoch 2512, Loss 2.932915\n",
      "    Params: tensor([  5.3245, -17.0599])\n",
      "    Grad:   tensor([-0.0073,  0.0417])\n",
      "Epoch 2513, Loss 2.932898\n",
      "    Params: tensor([  5.3245, -17.0603])\n",
      "    Grad:   tensor([-0.0073,  0.0416])\n",
      "Epoch 2514, Loss 2.932880\n",
      "    Params: tensor([  5.3246, -17.0608])\n",
      "    Grad:   tensor([-0.0073,  0.0415])\n",
      "Epoch 2515, Loss 2.932862\n",
      "    Params: tensor([  5.3247, -17.0612])\n",
      "    Grad:   tensor([-0.0073,  0.0415])\n",
      "Epoch 2516, Loss 2.932846\n",
      "    Params: tensor([  5.3248, -17.0616])\n",
      "    Grad:   tensor([-0.0073,  0.0414])\n",
      "Epoch 2517, Loss 2.932826\n",
      "    Params: tensor([  5.3248, -17.0620])\n",
      "    Grad:   tensor([-0.0073,  0.0413])\n",
      "Epoch 2518, Loss 2.932810\n",
      "    Params: tensor([  5.3249, -17.0624])\n",
      "    Grad:   tensor([-0.0073,  0.0412])\n",
      "Epoch 2519, Loss 2.932790\n",
      "    Params: tensor([  5.3250, -17.0628])\n",
      "    Grad:   tensor([-0.0073,  0.0412])\n",
      "Epoch 2520, Loss 2.932774\n",
      "    Params: tensor([  5.3250, -17.0632])\n",
      "    Grad:   tensor([-0.0073,  0.0411])\n",
      "Epoch 2521, Loss 2.932758\n",
      "    Params: tensor([  5.3251, -17.0636])\n",
      "    Grad:   tensor([-0.0073,  0.0410])\n",
      "Epoch 2522, Loss 2.932739\n",
      "    Params: tensor([  5.3252, -17.0640])\n",
      "    Grad:   tensor([-0.0073,  0.0410])\n",
      "Epoch 2523, Loss 2.932723\n",
      "    Params: tensor([  5.3253, -17.0645])\n",
      "    Grad:   tensor([-0.0072,  0.0409])\n",
      "Epoch 2524, Loss 2.932706\n",
      "    Params: tensor([  5.3253, -17.0649])\n",
      "    Grad:   tensor([-0.0072,  0.0408])\n",
      "Epoch 2525, Loss 2.932689\n",
      "    Params: tensor([  5.3254, -17.0653])\n",
      "    Grad:   tensor([-0.0072,  0.0408])\n",
      "Epoch 2526, Loss 2.932671\n",
      "    Params: tensor([  5.3255, -17.0657])\n",
      "    Grad:   tensor([-0.0072,  0.0407])\n",
      "Epoch 2527, Loss 2.932654\n",
      "    Params: tensor([  5.3256, -17.0661])\n",
      "    Grad:   tensor([-0.0072,  0.0406])\n",
      "Epoch 2528, Loss 2.932637\n",
      "    Params: tensor([  5.3256, -17.0665])\n",
      "    Grad:   tensor([-0.0072,  0.0405])\n",
      "Epoch 2529, Loss 2.932619\n",
      "    Params: tensor([  5.3257, -17.0669])\n",
      "    Grad:   tensor([-0.0072,  0.0405])\n",
      "Epoch 2530, Loss 2.932603\n",
      "    Params: tensor([  5.3258, -17.0673])\n",
      "    Grad:   tensor([-0.0071,  0.0404])\n",
      "Epoch 2531, Loss 2.932585\n",
      "    Params: tensor([  5.3258, -17.0677])\n",
      "    Grad:   tensor([-0.0071,  0.0403])\n",
      "Epoch 2532, Loss 2.932569\n",
      "    Params: tensor([  5.3259, -17.0681])\n",
      "    Grad:   tensor([-0.0071,  0.0403])\n",
      "Epoch 2533, Loss 2.932553\n",
      "    Params: tensor([  5.3260, -17.0685])\n",
      "    Grad:   tensor([-0.0071,  0.0402])\n",
      "Epoch 2534, Loss 2.932535\n",
      "    Params: tensor([  5.3261, -17.0689])\n",
      "    Grad:   tensor([-0.0071,  0.0401])\n",
      "Epoch 2535, Loss 2.932520\n",
      "    Params: tensor([  5.3261, -17.0693])\n",
      "    Grad:   tensor([-0.0071,  0.0401])\n",
      "Epoch 2536, Loss 2.932502\n",
      "    Params: tensor([  5.3262, -17.0697])\n",
      "    Grad:   tensor([-0.0071,  0.0400])\n",
      "Epoch 2537, Loss 2.932487\n",
      "    Params: tensor([  5.3263, -17.0701])\n",
      "    Grad:   tensor([-0.0071,  0.0399])\n",
      "Epoch 2538, Loss 2.932469\n",
      "    Params: tensor([  5.3263, -17.0705])\n",
      "    Grad:   tensor([-0.0070,  0.0399])\n",
      "Epoch 2539, Loss 2.932455\n",
      "    Params: tensor([  5.3264, -17.0709])\n",
      "    Grad:   tensor([-0.0070,  0.0398])\n",
      "Epoch 2540, Loss 2.932438\n",
      "    Params: tensor([  5.3265, -17.0713])\n",
      "    Grad:   tensor([-0.0070,  0.0397])\n",
      "Epoch 2541, Loss 2.932421\n",
      "    Params: tensor([  5.3265, -17.0717])\n",
      "    Grad:   tensor([-0.0070,  0.0397])\n",
      "Epoch 2542, Loss 2.932404\n",
      "    Params: tensor([  5.3266, -17.0721])\n",
      "    Grad:   tensor([-0.0070,  0.0396])\n",
      "Epoch 2543, Loss 2.932387\n",
      "    Params: tensor([  5.3267, -17.0725])\n",
      "    Grad:   tensor([-0.0070,  0.0395])\n",
      "Epoch 2544, Loss 2.932371\n",
      "    Params: tensor([  5.3268, -17.0729])\n",
      "    Grad:   tensor([-0.0070,  0.0395])\n",
      "Epoch 2545, Loss 2.932358\n",
      "    Params: tensor([  5.3268, -17.0733])\n",
      "    Grad:   tensor([-0.0070,  0.0394])\n",
      "Epoch 2546, Loss 2.932340\n",
      "    Params: tensor([  5.3269, -17.0737])\n",
      "    Grad:   tensor([-0.0069,  0.0393])\n",
      "Epoch 2547, Loss 2.932324\n",
      "    Params: tensor([  5.3270, -17.0741])\n",
      "    Grad:   tensor([-0.0069,  0.0393])\n",
      "Epoch 2548, Loss 2.932310\n",
      "    Params: tensor([  5.3270, -17.0745])\n",
      "    Grad:   tensor([-0.0069,  0.0392])\n",
      "Epoch 2549, Loss 2.932293\n",
      "    Params: tensor([  5.3271, -17.0749])\n",
      "    Grad:   tensor([-0.0069,  0.0391])\n",
      "Epoch 2550, Loss 2.932277\n",
      "    Params: tensor([  5.3272, -17.0752])\n",
      "    Grad:   tensor([-0.0069,  0.0391])\n",
      "Epoch 2551, Loss 2.932261\n",
      "    Params: tensor([  5.3272, -17.0756])\n",
      "    Grad:   tensor([-0.0069,  0.0390])\n",
      "Epoch 2552, Loss 2.932246\n",
      "    Params: tensor([  5.3273, -17.0760])\n",
      "    Grad:   tensor([-0.0069,  0.0389])\n",
      "Epoch 2553, Loss 2.932229\n",
      "    Params: tensor([  5.3274, -17.0764])\n",
      "    Grad:   tensor([-0.0069,  0.0389])\n",
      "Epoch 2554, Loss 2.932215\n",
      "    Params: tensor([  5.3274, -17.0768])\n",
      "    Grad:   tensor([-0.0069,  0.0388])\n",
      "Epoch 2555, Loss 2.932198\n",
      "    Params: tensor([  5.3275, -17.0772])\n",
      "    Grad:   tensor([-0.0068,  0.0387])\n",
      "Epoch 2556, Loss 2.932183\n",
      "    Params: tensor([  5.3276, -17.0776])\n",
      "    Grad:   tensor([-0.0068,  0.0387])\n",
      "Epoch 2557, Loss 2.932167\n",
      "    Params: tensor([  5.3276, -17.0780])\n",
      "    Grad:   tensor([-0.0068,  0.0386])\n",
      "Epoch 2558, Loss 2.932153\n",
      "    Params: tensor([  5.3277, -17.0783])\n",
      "    Grad:   tensor([-0.0068,  0.0385])\n",
      "Epoch 2559, Loss 2.932137\n",
      "    Params: tensor([  5.3278, -17.0787])\n",
      "    Grad:   tensor([-0.0068,  0.0385])\n",
      "Epoch 2560, Loss 2.932122\n",
      "    Params: tensor([  5.3279, -17.0791])\n",
      "    Grad:   tensor([-0.0068,  0.0384])\n",
      "Epoch 2561, Loss 2.932107\n",
      "    Params: tensor([  5.3279, -17.0795])\n",
      "    Grad:   tensor([-0.0068,  0.0383])\n",
      "Epoch 2562, Loss 2.932092\n",
      "    Params: tensor([  5.3280, -17.0799])\n",
      "    Grad:   tensor([-0.0068,  0.0383])\n",
      "Epoch 2563, Loss 2.932076\n",
      "    Params: tensor([  5.3281, -17.0803])\n",
      "    Grad:   tensor([-0.0067,  0.0382])\n",
      "Epoch 2564, Loss 2.932061\n",
      "    Params: tensor([  5.3281, -17.0806])\n",
      "    Grad:   tensor([-0.0067,  0.0381])\n",
      "Epoch 2565, Loss 2.932047\n",
      "    Params: tensor([  5.3282, -17.0810])\n",
      "    Grad:   tensor([-0.0067,  0.0381])\n",
      "Epoch 2566, Loss 2.932031\n",
      "    Params: tensor([  5.3283, -17.0814])\n",
      "    Grad:   tensor([-0.0067,  0.0380])\n",
      "Epoch 2567, Loss 2.932017\n",
      "    Params: tensor([  5.3283, -17.0818])\n",
      "    Grad:   tensor([-0.0067,  0.0379])\n",
      "Epoch 2568, Loss 2.932002\n",
      "    Params: tensor([  5.3284, -17.0822])\n",
      "    Grad:   tensor([-0.0067,  0.0379])\n",
      "Epoch 2569, Loss 2.931986\n",
      "    Params: tensor([  5.3285, -17.0825])\n",
      "    Grad:   tensor([-0.0067,  0.0378])\n",
      "Epoch 2570, Loss 2.931972\n",
      "    Params: tensor([  5.3285, -17.0829])\n",
      "    Grad:   tensor([-0.0067,  0.0378])\n",
      "Epoch 2571, Loss 2.931957\n",
      "    Params: tensor([  5.3286, -17.0833])\n",
      "    Grad:   tensor([-0.0067,  0.0377])\n",
      "Epoch 2572, Loss 2.931941\n",
      "    Params: tensor([  5.3287, -17.0837])\n",
      "    Grad:   tensor([-0.0067,  0.0376])\n",
      "Epoch 2573, Loss 2.931929\n",
      "    Params: tensor([  5.3287, -17.0840])\n",
      "    Grad:   tensor([-0.0066,  0.0376])\n",
      "Epoch 2574, Loss 2.931914\n",
      "    Params: tensor([  5.3288, -17.0844])\n",
      "    Grad:   tensor([-0.0066,  0.0375])\n",
      "Epoch 2575, Loss 2.931900\n",
      "    Params: tensor([  5.3289, -17.0848])\n",
      "    Grad:   tensor([-0.0066,  0.0374])\n",
      "Epoch 2576, Loss 2.931885\n",
      "    Params: tensor([  5.3289, -17.0852])\n",
      "    Grad:   tensor([-0.0066,  0.0374])\n",
      "Epoch 2577, Loss 2.931870\n",
      "    Params: tensor([  5.3290, -17.0855])\n",
      "    Grad:   tensor([-0.0066,  0.0373])\n",
      "Epoch 2578, Loss 2.931855\n",
      "    Params: tensor([  5.3291, -17.0859])\n",
      "    Grad:   tensor([-0.0066,  0.0372])\n",
      "Epoch 2579, Loss 2.931842\n",
      "    Params: tensor([  5.3291, -17.0863])\n",
      "    Grad:   tensor([-0.0066,  0.0372])\n",
      "Epoch 2580, Loss 2.931828\n",
      "    Params: tensor([  5.3292, -17.0867])\n",
      "    Grad:   tensor([-0.0066,  0.0371])\n",
      "Epoch 2581, Loss 2.931813\n",
      "    Params: tensor([  5.3293, -17.0870])\n",
      "    Grad:   tensor([-0.0065,  0.0371])\n",
      "Epoch 2582, Loss 2.931799\n",
      "    Params: tensor([  5.3293, -17.0874])\n",
      "    Grad:   tensor([-0.0065,  0.0370])\n",
      "Epoch 2583, Loss 2.931786\n",
      "    Params: tensor([  5.3294, -17.0878])\n",
      "    Grad:   tensor([-0.0065,  0.0369])\n",
      "Epoch 2584, Loss 2.931771\n",
      "    Params: tensor([  5.3294, -17.0881])\n",
      "    Grad:   tensor([-0.0065,  0.0369])\n",
      "Epoch 2585, Loss 2.931759\n",
      "    Params: tensor([  5.3295, -17.0885])\n",
      "    Grad:   tensor([-0.0065,  0.0368])\n",
      "Epoch 2586, Loss 2.931742\n",
      "    Params: tensor([  5.3296, -17.0889])\n",
      "    Grad:   tensor([-0.0065,  0.0367])\n",
      "Epoch 2587, Loss 2.931729\n",
      "    Params: tensor([  5.3296, -17.0892])\n",
      "    Grad:   tensor([-0.0065,  0.0367])\n",
      "Epoch 2588, Loss 2.931717\n",
      "    Params: tensor([  5.3297, -17.0896])\n",
      "    Grad:   tensor([-0.0065,  0.0366])\n",
      "Epoch 2589, Loss 2.931701\n",
      "    Params: tensor([  5.3298, -17.0900])\n",
      "    Grad:   tensor([-0.0065,  0.0366])\n",
      "Epoch 2590, Loss 2.931687\n",
      "    Params: tensor([  5.3298, -17.0903])\n",
      "    Grad:   tensor([-0.0065,  0.0365])\n",
      "Epoch 2591, Loss 2.931674\n",
      "    Params: tensor([  5.3299, -17.0907])\n",
      "    Grad:   tensor([-0.0064,  0.0364])\n",
      "Epoch 2592, Loss 2.931660\n",
      "    Params: tensor([  5.3300, -17.0911])\n",
      "    Grad:   tensor([-0.0064,  0.0364])\n",
      "Epoch 2593, Loss 2.931648\n",
      "    Params: tensor([  5.3300, -17.0914])\n",
      "    Grad:   tensor([-0.0064,  0.0363])\n",
      "Epoch 2594, Loss 2.931632\n",
      "    Params: tensor([  5.3301, -17.0918])\n",
      "    Grad:   tensor([-0.0064,  0.0362])\n",
      "Epoch 2595, Loss 2.931619\n",
      "    Params: tensor([  5.3302, -17.0921])\n",
      "    Grad:   tensor([-0.0064,  0.0362])\n",
      "Epoch 2596, Loss 2.931606\n",
      "    Params: tensor([  5.3302, -17.0925])\n",
      "    Grad:   tensor([-0.0064,  0.0361])\n",
      "Epoch 2597, Loss 2.931593\n",
      "    Params: tensor([  5.3303, -17.0929])\n",
      "    Grad:   tensor([-0.0064,  0.0361])\n",
      "Epoch 2598, Loss 2.931580\n",
      "    Params: tensor([  5.3303, -17.0932])\n",
      "    Grad:   tensor([-0.0064,  0.0360])\n",
      "Epoch 2599, Loss 2.931566\n",
      "    Params: tensor([  5.3304, -17.0936])\n",
      "    Grad:   tensor([-0.0064,  0.0359])\n",
      "Epoch 2600, Loss 2.931554\n",
      "    Params: tensor([  5.3305, -17.0939])\n",
      "    Grad:   tensor([-0.0064,  0.0359])\n",
      "Epoch 2601, Loss 2.931538\n",
      "    Params: tensor([  5.3305, -17.0943])\n",
      "    Grad:   tensor([-0.0063,  0.0358])\n",
      "Epoch 2602, Loss 2.931526\n",
      "    Params: tensor([  5.3306, -17.0947])\n",
      "    Grad:   tensor([-0.0063,  0.0358])\n",
      "Epoch 2603, Loss 2.931512\n",
      "    Params: tensor([  5.3307, -17.0950])\n",
      "    Grad:   tensor([-0.0063,  0.0357])\n",
      "Epoch 2604, Loss 2.931499\n",
      "    Params: tensor([  5.3307, -17.0954])\n",
      "    Grad:   tensor([-0.0063,  0.0356])\n",
      "Epoch 2605, Loss 2.931488\n",
      "    Params: tensor([  5.3308, -17.0957])\n",
      "    Grad:   tensor([-0.0063,  0.0356])\n",
      "Epoch 2606, Loss 2.931474\n",
      "    Params: tensor([  5.3309, -17.0961])\n",
      "    Grad:   tensor([-0.0063,  0.0355])\n",
      "Epoch 2607, Loss 2.931462\n",
      "    Params: tensor([  5.3309, -17.0964])\n",
      "    Grad:   tensor([-0.0062,  0.0355])\n",
      "Epoch 2608, Loss 2.931448\n",
      "    Params: tensor([  5.3310, -17.0968])\n",
      "    Grad:   tensor([-0.0062,  0.0354])\n",
      "Epoch 2609, Loss 2.931436\n",
      "    Params: tensor([  5.3310, -17.0971])\n",
      "    Grad:   tensor([-0.0062,  0.0353])\n",
      "Epoch 2610, Loss 2.931423\n",
      "    Params: tensor([  5.3311, -17.0975])\n",
      "    Grad:   tensor([-0.0062,  0.0353])\n",
      "Epoch 2611, Loss 2.931411\n",
      "    Params: tensor([  5.3312, -17.0979])\n",
      "    Grad:   tensor([-0.0062,  0.0352])\n",
      "Epoch 2612, Loss 2.931397\n",
      "    Params: tensor([  5.3312, -17.0982])\n",
      "    Grad:   tensor([-0.0062,  0.0352])\n",
      "Epoch 2613, Loss 2.931384\n",
      "    Params: tensor([  5.3313, -17.0986])\n",
      "    Grad:   tensor([-0.0062,  0.0351])\n",
      "Epoch 2614, Loss 2.931371\n",
      "    Params: tensor([  5.3313, -17.0989])\n",
      "    Grad:   tensor([-0.0062,  0.0350])\n",
      "Epoch 2615, Loss 2.931358\n",
      "    Params: tensor([  5.3314, -17.0993])\n",
      "    Grad:   tensor([-0.0062,  0.0350])\n",
      "Epoch 2616, Loss 2.931346\n",
      "    Params: tensor([  5.3315, -17.0996])\n",
      "    Grad:   tensor([-0.0062,  0.0349])\n",
      "Epoch 2617, Loss 2.931335\n",
      "    Params: tensor([  5.3315, -17.1000])\n",
      "    Grad:   tensor([-0.0062,  0.0349])\n",
      "Epoch 2618, Loss 2.931322\n",
      "    Params: tensor([  5.3316, -17.1003])\n",
      "    Grad:   tensor([-0.0062,  0.0348])\n",
      "Epoch 2619, Loss 2.931308\n",
      "    Params: tensor([  5.3317, -17.1006])\n",
      "    Grad:   tensor([-0.0061,  0.0347])\n",
      "Epoch 2620, Loss 2.931296\n",
      "    Params: tensor([  5.3317, -17.1010])\n",
      "    Grad:   tensor([-0.0061,  0.0347])\n",
      "Epoch 2621, Loss 2.931282\n",
      "    Params: tensor([  5.3318, -17.1013])\n",
      "    Grad:   tensor([-0.0061,  0.0346])\n",
      "Epoch 2622, Loss 2.931272\n",
      "    Params: tensor([  5.3318, -17.1017])\n",
      "    Grad:   tensor([-0.0061,  0.0346])\n",
      "Epoch 2623, Loss 2.931258\n",
      "    Params: tensor([  5.3319, -17.1020])\n",
      "    Grad:   tensor([-0.0061,  0.0345])\n",
      "Epoch 2624, Loss 2.931245\n",
      "    Params: tensor([  5.3320, -17.1024])\n",
      "    Grad:   tensor([-0.0061,  0.0344])\n",
      "Epoch 2625, Loss 2.931234\n",
      "    Params: tensor([  5.3320, -17.1027])\n",
      "    Grad:   tensor([-0.0061,  0.0344])\n",
      "Epoch 2626, Loss 2.931222\n",
      "    Params: tensor([  5.3321, -17.1031])\n",
      "    Grad:   tensor([-0.0061,  0.0343])\n",
      "Epoch 2627, Loss 2.931211\n",
      "    Params: tensor([  5.3321, -17.1034])\n",
      "    Grad:   tensor([-0.0060,  0.0343])\n",
      "Epoch 2628, Loss 2.931196\n",
      "    Params: tensor([  5.3322, -17.1038])\n",
      "    Grad:   tensor([-0.0060,  0.0342])\n",
      "Epoch 2629, Loss 2.931185\n",
      "    Params: tensor([  5.3323, -17.1041])\n",
      "    Grad:   tensor([-0.0060,  0.0342])\n",
      "Epoch 2630, Loss 2.931173\n",
      "    Params: tensor([  5.3323, -17.1044])\n",
      "    Grad:   tensor([-0.0060,  0.0341])\n",
      "Epoch 2631, Loss 2.931162\n",
      "    Params: tensor([  5.3324, -17.1048])\n",
      "    Grad:   tensor([-0.0060,  0.0340])\n",
      "Epoch 2632, Loss 2.931149\n",
      "    Params: tensor([  5.3324, -17.1051])\n",
      "    Grad:   tensor([-0.0060,  0.0340])\n",
      "Epoch 2633, Loss 2.931138\n",
      "    Params: tensor([  5.3325, -17.1055])\n",
      "    Grad:   tensor([-0.0060,  0.0339])\n",
      "Epoch 2634, Loss 2.931126\n",
      "    Params: tensor([  5.3326, -17.1058])\n",
      "    Grad:   tensor([-0.0060,  0.0339])\n",
      "Epoch 2635, Loss 2.931114\n",
      "    Params: tensor([  5.3326, -17.1061])\n",
      "    Grad:   tensor([-0.0060,  0.0338])\n",
      "Epoch 2636, Loss 2.931101\n",
      "    Params: tensor([  5.3327, -17.1065])\n",
      "    Grad:   tensor([-0.0060,  0.0337])\n",
      "Epoch 2637, Loss 2.931090\n",
      "    Params: tensor([  5.3327, -17.1068])\n",
      "    Grad:   tensor([-0.0059,  0.0337])\n",
      "Epoch 2638, Loss 2.931079\n",
      "    Params: tensor([  5.3328, -17.1071])\n",
      "    Grad:   tensor([-0.0059,  0.0336])\n",
      "Epoch 2639, Loss 2.931067\n",
      "    Params: tensor([  5.3329, -17.1075])\n",
      "    Grad:   tensor([-0.0059,  0.0336])\n",
      "Epoch 2640, Loss 2.931054\n",
      "    Params: tensor([  5.3329, -17.1078])\n",
      "    Grad:   tensor([-0.0059,  0.0335])\n",
      "Epoch 2641, Loss 2.931044\n",
      "    Params: tensor([  5.3330, -17.1081])\n",
      "    Grad:   tensor([-0.0059,  0.0335])\n",
      "Epoch 2642, Loss 2.931034\n",
      "    Params: tensor([  5.3330, -17.1085])\n",
      "    Grad:   tensor([-0.0059,  0.0334])\n",
      "Epoch 2643, Loss 2.931021\n",
      "    Params: tensor([  5.3331, -17.1088])\n",
      "    Grad:   tensor([-0.0059,  0.0333])\n",
      "Epoch 2644, Loss 2.931010\n",
      "    Params: tensor([  5.3332, -17.1091])\n",
      "    Grad:   tensor([-0.0059,  0.0333])\n",
      "Epoch 2645, Loss 2.930999\n",
      "    Params: tensor([  5.3332, -17.1095])\n",
      "    Grad:   tensor([-0.0059,  0.0332])\n",
      "Epoch 2646, Loss 2.930987\n",
      "    Params: tensor([  5.3333, -17.1098])\n",
      "    Grad:   tensor([-0.0059,  0.0332])\n",
      "Epoch 2647, Loss 2.930976\n",
      "    Params: tensor([  5.3333, -17.1101])\n",
      "    Grad:   tensor([-0.0059,  0.0331])\n",
      "Epoch 2648, Loss 2.930964\n",
      "    Params: tensor([  5.3334, -17.1105])\n",
      "    Grad:   tensor([-0.0059,  0.0331])\n",
      "Epoch 2649, Loss 2.930953\n",
      "    Params: tensor([  5.3335, -17.1108])\n",
      "    Grad:   tensor([-0.0058,  0.0330])\n",
      "Epoch 2650, Loss 2.930941\n",
      "    Params: tensor([  5.3335, -17.1111])\n",
      "    Grad:   tensor([-0.0058,  0.0330])\n",
      "Epoch 2651, Loss 2.930932\n",
      "    Params: tensor([  5.3336, -17.1115])\n",
      "    Grad:   tensor([-0.0058,  0.0329])\n",
      "Epoch 2652, Loss 2.930921\n",
      "    Params: tensor([  5.3336, -17.1118])\n",
      "    Grad:   tensor([-0.0058,  0.0328])\n",
      "Epoch 2653, Loss 2.930908\n",
      "    Params: tensor([  5.3337, -17.1121])\n",
      "    Grad:   tensor([-0.0058,  0.0328])\n",
      "Epoch 2654, Loss 2.930899\n",
      "    Params: tensor([  5.3337, -17.1124])\n",
      "    Grad:   tensor([-0.0058,  0.0327])\n",
      "Epoch 2655, Loss 2.930885\n",
      "    Params: tensor([  5.3338, -17.1128])\n",
      "    Grad:   tensor([-0.0058,  0.0327])\n",
      "Epoch 2656, Loss 2.930876\n",
      "    Params: tensor([  5.3339, -17.1131])\n",
      "    Grad:   tensor([-0.0058,  0.0326])\n",
      "Epoch 2657, Loss 2.930863\n",
      "    Params: tensor([  5.3339, -17.1134])\n",
      "    Grad:   tensor([-0.0057,  0.0326])\n",
      "Epoch 2658, Loss 2.930854\n",
      "    Params: tensor([  5.3340, -17.1137])\n",
      "    Grad:   tensor([-0.0057,  0.0325])\n",
      "Epoch 2659, Loss 2.930841\n",
      "    Params: tensor([  5.3340, -17.1141])\n",
      "    Grad:   tensor([-0.0057,  0.0325])\n",
      "Epoch 2660, Loss 2.930833\n",
      "    Params: tensor([  5.3341, -17.1144])\n",
      "    Grad:   tensor([-0.0057,  0.0324])\n",
      "Epoch 2661, Loss 2.930821\n",
      "    Params: tensor([  5.3341, -17.1147])\n",
      "    Grad:   tensor([-0.0057,  0.0323])\n",
      "Epoch 2662, Loss 2.930811\n",
      "    Params: tensor([  5.3342, -17.1150])\n",
      "    Grad:   tensor([-0.0057,  0.0323])\n",
      "Epoch 2663, Loss 2.930801\n",
      "    Params: tensor([  5.3343, -17.1154])\n",
      "    Grad:   tensor([-0.0057,  0.0322])\n",
      "Epoch 2664, Loss 2.930788\n",
      "    Params: tensor([  5.3343, -17.1157])\n",
      "    Grad:   tensor([-0.0057,  0.0322])\n",
      "Epoch 2665, Loss 2.930778\n",
      "    Params: tensor([  5.3344, -17.1160])\n",
      "    Grad:   tensor([-0.0057,  0.0321])\n",
      "Epoch 2666, Loss 2.930767\n",
      "    Params: tensor([  5.3344, -17.1163])\n",
      "    Grad:   tensor([-0.0057,  0.0321])\n",
      "Epoch 2667, Loss 2.930757\n",
      "    Params: tensor([  5.3345, -17.1166])\n",
      "    Grad:   tensor([-0.0057,  0.0320])\n",
      "Epoch 2668, Loss 2.930746\n",
      "    Params: tensor([  5.3345, -17.1170])\n",
      "    Grad:   tensor([-0.0056,  0.0320])\n",
      "Epoch 2669, Loss 2.930736\n",
      "    Params: tensor([  5.3346, -17.1173])\n",
      "    Grad:   tensor([-0.0056,  0.0319])\n",
      "Epoch 2670, Loss 2.930724\n",
      "    Params: tensor([  5.3347, -17.1176])\n",
      "    Grad:   tensor([-0.0056,  0.0319])\n",
      "Epoch 2671, Loss 2.930715\n",
      "    Params: tensor([  5.3347, -17.1179])\n",
      "    Grad:   tensor([-0.0056,  0.0318])\n",
      "Epoch 2672, Loss 2.930704\n",
      "    Params: tensor([  5.3348, -17.1182])\n",
      "    Grad:   tensor([-0.0056,  0.0317])\n",
      "Epoch 2673, Loss 2.930694\n",
      "    Params: tensor([  5.3348, -17.1186])\n",
      "    Grad:   tensor([-0.0056,  0.0317])\n",
      "Epoch 2674, Loss 2.930685\n",
      "    Params: tensor([  5.3349, -17.1189])\n",
      "    Grad:   tensor([-0.0056,  0.0316])\n",
      "Epoch 2675, Loss 2.930674\n",
      "    Params: tensor([  5.3349, -17.1192])\n",
      "    Grad:   tensor([-0.0056,  0.0316])\n",
      "Epoch 2676, Loss 2.930663\n",
      "    Params: tensor([  5.3350, -17.1195])\n",
      "    Grad:   tensor([-0.0056,  0.0315])\n",
      "Epoch 2677, Loss 2.930654\n",
      "    Params: tensor([  5.3350, -17.1198])\n",
      "    Grad:   tensor([-0.0056,  0.0315])\n",
      "Epoch 2678, Loss 2.930644\n",
      "    Params: tensor([  5.3351, -17.1201])\n",
      "    Grad:   tensor([-0.0055,  0.0314])\n",
      "Epoch 2679, Loss 2.930631\n",
      "    Params: tensor([  5.3352, -17.1204])\n",
      "    Grad:   tensor([-0.0055,  0.0314])\n",
      "Epoch 2680, Loss 2.930621\n",
      "    Params: tensor([  5.3352, -17.1208])\n",
      "    Grad:   tensor([-0.0055,  0.0313])\n",
      "Epoch 2681, Loss 2.930613\n",
      "    Params: tensor([  5.3353, -17.1211])\n",
      "    Grad:   tensor([-0.0055,  0.0313])\n",
      "Epoch 2682, Loss 2.930603\n",
      "    Params: tensor([  5.3353, -17.1214])\n",
      "    Grad:   tensor([-0.0055,  0.0312])\n",
      "Epoch 2683, Loss 2.930593\n",
      "    Params: tensor([  5.3354, -17.1217])\n",
      "    Grad:   tensor([-0.0055,  0.0312])\n",
      "Epoch 2684, Loss 2.930582\n",
      "    Params: tensor([  5.3354, -17.1220])\n",
      "    Grad:   tensor([-0.0055,  0.0311])\n",
      "Epoch 2685, Loss 2.930571\n",
      "    Params: tensor([  5.3355, -17.1223])\n",
      "    Grad:   tensor([-0.0055,  0.0310])\n",
      "Epoch 2686, Loss 2.930562\n",
      "    Params: tensor([  5.3355, -17.1226])\n",
      "    Grad:   tensor([-0.0055,  0.0310])\n",
      "Epoch 2687, Loss 2.930552\n",
      "    Params: tensor([  5.3356, -17.1229])\n",
      "    Grad:   tensor([-0.0055,  0.0309])\n",
      "Epoch 2688, Loss 2.930543\n",
      "    Params: tensor([  5.3356, -17.1232])\n",
      "    Grad:   tensor([-0.0055,  0.0309])\n",
      "Epoch 2689, Loss 2.930534\n",
      "    Params: tensor([  5.3357, -17.1236])\n",
      "    Grad:   tensor([-0.0055,  0.0308])\n",
      "Epoch 2690, Loss 2.930523\n",
      "    Params: tensor([  5.3358, -17.1239])\n",
      "    Grad:   tensor([-0.0054,  0.0308])\n",
      "Epoch 2691, Loss 2.930514\n",
      "    Params: tensor([  5.3358, -17.1242])\n",
      "    Grad:   tensor([-0.0054,  0.0307])\n",
      "Epoch 2692, Loss 2.930502\n",
      "    Params: tensor([  5.3359, -17.1245])\n",
      "    Grad:   tensor([-0.0054,  0.0307])\n",
      "Epoch 2693, Loss 2.930493\n",
      "    Params: tensor([  5.3359, -17.1248])\n",
      "    Grad:   tensor([-0.0054,  0.0306])\n",
      "Epoch 2694, Loss 2.930482\n",
      "    Params: tensor([  5.3360, -17.1251])\n",
      "    Grad:   tensor([-0.0054,  0.0306])\n",
      "Epoch 2695, Loss 2.930474\n",
      "    Params: tensor([  5.3360, -17.1254])\n",
      "    Grad:   tensor([-0.0054,  0.0305])\n",
      "Epoch 2696, Loss 2.930464\n",
      "    Params: tensor([  5.3361, -17.1257])\n",
      "    Grad:   tensor([-0.0054,  0.0305])\n",
      "Epoch 2697, Loss 2.930454\n",
      "    Params: tensor([  5.3361, -17.1260])\n",
      "    Grad:   tensor([-0.0054,  0.0304])\n",
      "Epoch 2698, Loss 2.930445\n",
      "    Params: tensor([  5.3362, -17.1263])\n",
      "    Grad:   tensor([-0.0054,  0.0304])\n",
      "Epoch 2699, Loss 2.930436\n",
      "    Params: tensor([  5.3362, -17.1266])\n",
      "    Grad:   tensor([-0.0054,  0.0303])\n",
      "Epoch 2700, Loss 2.930426\n",
      "    Params: tensor([  5.3363, -17.1269])\n",
      "    Grad:   tensor([-0.0054,  0.0303])\n",
      "Epoch 2701, Loss 2.930416\n",
      "    Params: tensor([  5.3364, -17.1272])\n",
      "    Grad:   tensor([-0.0054,  0.0302])\n",
      "Epoch 2702, Loss 2.930408\n",
      "    Params: tensor([  5.3364, -17.1275])\n",
      "    Grad:   tensor([-0.0053,  0.0302])\n",
      "Epoch 2703, Loss 2.930398\n",
      "    Params: tensor([  5.3365, -17.1278])\n",
      "    Grad:   tensor([-0.0053,  0.0301])\n",
      "Epoch 2704, Loss 2.930388\n",
      "    Params: tensor([  5.3365, -17.1281])\n",
      "    Grad:   tensor([-0.0053,  0.0301])\n",
      "Epoch 2705, Loss 2.930380\n",
      "    Params: tensor([  5.3366, -17.1284])\n",
      "    Grad:   tensor([-0.0053,  0.0300])\n",
      "Epoch 2706, Loss 2.930370\n",
      "    Params: tensor([  5.3366, -17.1287])\n",
      "    Grad:   tensor([-0.0053,  0.0300])\n",
      "Epoch 2707, Loss 2.930360\n",
      "    Params: tensor([  5.3367, -17.1290])\n",
      "    Grad:   tensor([-0.0053,  0.0299])\n",
      "Epoch 2708, Loss 2.930353\n",
      "    Params: tensor([  5.3367, -17.1293])\n",
      "    Grad:   tensor([-0.0053,  0.0299])\n",
      "Epoch 2709, Loss 2.930342\n",
      "    Params: tensor([  5.3368, -17.1296])\n",
      "    Grad:   tensor([-0.0053,  0.0298])\n",
      "Epoch 2710, Loss 2.930335\n",
      "    Params: tensor([  5.3368, -17.1299])\n",
      "    Grad:   tensor([-0.0053,  0.0298])\n",
      "Epoch 2711, Loss 2.930325\n",
      "    Params: tensor([  5.3369, -17.1302])\n",
      "    Grad:   tensor([-0.0053,  0.0297])\n",
      "Epoch 2712, Loss 2.930315\n",
      "    Params: tensor([  5.3369, -17.1305])\n",
      "    Grad:   tensor([-0.0053,  0.0297])\n",
      "Epoch 2713, Loss 2.930306\n",
      "    Params: tensor([  5.3370, -17.1308])\n",
      "    Grad:   tensor([-0.0052,  0.0296])\n",
      "Epoch 2714, Loss 2.930298\n",
      "    Params: tensor([  5.3370, -17.1311])\n",
      "    Grad:   tensor([-0.0052,  0.0296])\n",
      "Epoch 2715, Loss 2.930288\n",
      "    Params: tensor([  5.3371, -17.1314])\n",
      "    Grad:   tensor([-0.0052,  0.0295])\n",
      "Epoch 2716, Loss 2.930279\n",
      "    Params: tensor([  5.3371, -17.1317])\n",
      "    Grad:   tensor([-0.0052,  0.0295])\n",
      "Epoch 2717, Loss 2.930270\n",
      "    Params: tensor([  5.3372, -17.1320])\n",
      "    Grad:   tensor([-0.0052,  0.0294])\n",
      "Epoch 2718, Loss 2.930262\n",
      "    Params: tensor([  5.3372, -17.1323])\n",
      "    Grad:   tensor([-0.0052,  0.0294])\n",
      "Epoch 2719, Loss 2.930254\n",
      "    Params: tensor([  5.3373, -17.1326])\n",
      "    Grad:   tensor([-0.0052,  0.0293])\n",
      "Epoch 2720, Loss 2.930244\n",
      "    Params: tensor([  5.3373, -17.1329])\n",
      "    Grad:   tensor([-0.0052,  0.0293])\n",
      "Epoch 2721, Loss 2.930235\n",
      "    Params: tensor([  5.3374, -17.1332])\n",
      "    Grad:   tensor([-0.0052,  0.0292])\n",
      "Epoch 2722, Loss 2.930226\n",
      "    Params: tensor([  5.3375, -17.1334])\n",
      "    Grad:   tensor([-0.0052,  0.0292])\n",
      "Epoch 2723, Loss 2.930218\n",
      "    Params: tensor([  5.3375, -17.1337])\n",
      "    Grad:   tensor([-0.0051,  0.0291])\n",
      "Epoch 2724, Loss 2.930209\n",
      "    Params: tensor([  5.3376, -17.1340])\n",
      "    Grad:   tensor([-0.0051,  0.0291])\n",
      "Epoch 2725, Loss 2.930201\n",
      "    Params: tensor([  5.3376, -17.1343])\n",
      "    Grad:   tensor([-0.0051,  0.0290])\n",
      "Epoch 2726, Loss 2.930190\n",
      "    Params: tensor([  5.3377, -17.1346])\n",
      "    Grad:   tensor([-0.0051,  0.0290])\n",
      "Epoch 2727, Loss 2.930183\n",
      "    Params: tensor([  5.3377, -17.1349])\n",
      "    Grad:   tensor([-0.0051,  0.0289])\n",
      "Epoch 2728, Loss 2.930173\n",
      "    Params: tensor([  5.3378, -17.1352])\n",
      "    Grad:   tensor([-0.0051,  0.0289])\n",
      "Epoch 2729, Loss 2.930166\n",
      "    Params: tensor([  5.3378, -17.1355])\n",
      "    Grad:   tensor([-0.0051,  0.0288])\n",
      "Epoch 2730, Loss 2.930156\n",
      "    Params: tensor([  5.3379, -17.1358])\n",
      "    Grad:   tensor([-0.0051,  0.0288])\n",
      "Epoch 2731, Loss 2.930149\n",
      "    Params: tensor([  5.3379, -17.1360])\n",
      "    Grad:   tensor([-0.0051,  0.0287])\n",
      "Epoch 2732, Loss 2.930139\n",
      "    Params: tensor([  5.3380, -17.1363])\n",
      "    Grad:   tensor([-0.0051,  0.0287])\n",
      "Epoch 2733, Loss 2.930131\n",
      "    Params: tensor([  5.3380, -17.1366])\n",
      "    Grad:   tensor([-0.0050,  0.0286])\n",
      "Epoch 2734, Loss 2.930123\n",
      "    Params: tensor([  5.3381, -17.1369])\n",
      "    Grad:   tensor([-0.0050,  0.0286])\n",
      "Epoch 2735, Loss 2.930113\n",
      "    Params: tensor([  5.3381, -17.1372])\n",
      "    Grad:   tensor([-0.0050,  0.0285])\n",
      "Epoch 2736, Loss 2.930107\n",
      "    Params: tensor([  5.3382, -17.1375])\n",
      "    Grad:   tensor([-0.0051,  0.0285])\n",
      "Epoch 2737, Loss 2.930099\n",
      "    Params: tensor([  5.3382, -17.1378])\n",
      "    Grad:   tensor([-0.0050,  0.0284])\n",
      "Epoch 2738, Loss 2.930090\n",
      "    Params: tensor([  5.3383, -17.1380])\n",
      "    Grad:   tensor([-0.0050,  0.0284])\n",
      "Epoch 2739, Loss 2.930081\n",
      "    Params: tensor([  5.3383, -17.1383])\n",
      "    Grad:   tensor([-0.0050,  0.0283])\n",
      "Epoch 2740, Loss 2.930073\n",
      "    Params: tensor([  5.3384, -17.1386])\n",
      "    Grad:   tensor([-0.0050,  0.0283])\n",
      "Epoch 2741, Loss 2.930064\n",
      "    Params: tensor([  5.3384, -17.1389])\n",
      "    Grad:   tensor([-0.0050,  0.0282])\n",
      "Epoch 2742, Loss 2.930056\n",
      "    Params: tensor([  5.3385, -17.1392])\n",
      "    Grad:   tensor([-0.0050,  0.0282])\n",
      "Epoch 2743, Loss 2.930048\n",
      "    Params: tensor([  5.3385, -17.1395])\n",
      "    Grad:   tensor([-0.0050,  0.0281])\n",
      "Epoch 2744, Loss 2.930041\n",
      "    Params: tensor([  5.3386, -17.1397])\n",
      "    Grad:   tensor([-0.0050,  0.0281])\n",
      "Epoch 2745, Loss 2.930032\n",
      "    Params: tensor([  5.3386, -17.1400])\n",
      "    Grad:   tensor([-0.0050,  0.0280])\n",
      "Epoch 2746, Loss 2.930022\n",
      "    Params: tensor([  5.3387, -17.1403])\n",
      "    Grad:   tensor([-0.0050,  0.0280])\n",
      "Epoch 2747, Loss 2.930016\n",
      "    Params: tensor([  5.3387, -17.1406])\n",
      "    Grad:   tensor([-0.0049,  0.0279])\n",
      "Epoch 2748, Loss 2.930008\n",
      "    Params: tensor([  5.3388, -17.1409])\n",
      "    Grad:   tensor([-0.0049,  0.0279])\n",
      "Epoch 2749, Loss 2.930000\n",
      "    Params: tensor([  5.3388, -17.1411])\n",
      "    Grad:   tensor([-0.0049,  0.0279])\n",
      "Epoch 2750, Loss 2.929992\n",
      "    Params: tensor([  5.3389, -17.1414])\n",
      "    Grad:   tensor([-0.0049,  0.0278])\n",
      "Epoch 2751, Loss 2.929983\n",
      "    Params: tensor([  5.3389, -17.1417])\n",
      "    Grad:   tensor([-0.0049,  0.0278])\n",
      "Epoch 2752, Loss 2.929975\n",
      "    Params: tensor([  5.3390, -17.1420])\n",
      "    Grad:   tensor([-0.0049,  0.0277])\n",
      "Epoch 2753, Loss 2.929968\n",
      "    Params: tensor([  5.3390, -17.1422])\n",
      "    Grad:   tensor([-0.0049,  0.0277])\n",
      "Epoch 2754, Loss 2.929960\n",
      "    Params: tensor([  5.3391, -17.1425])\n",
      "    Grad:   tensor([-0.0049,  0.0276])\n",
      "Epoch 2755, Loss 2.929953\n",
      "    Params: tensor([  5.3391, -17.1428])\n",
      "    Grad:   tensor([-0.0049,  0.0276])\n",
      "Epoch 2756, Loss 2.929945\n",
      "    Params: tensor([  5.3392, -17.1431])\n",
      "    Grad:   tensor([-0.0049,  0.0275])\n",
      "Epoch 2757, Loss 2.929936\n",
      "    Params: tensor([  5.3392, -17.1433])\n",
      "    Grad:   tensor([-0.0049,  0.0275])\n",
      "Epoch 2758, Loss 2.929929\n",
      "    Params: tensor([  5.3392, -17.1436])\n",
      "    Grad:   tensor([-0.0049,  0.0274])\n",
      "Epoch 2759, Loss 2.929921\n",
      "    Params: tensor([  5.3393, -17.1439])\n",
      "    Grad:   tensor([-0.0048,  0.0274])\n",
      "Epoch 2760, Loss 2.929914\n",
      "    Params: tensor([  5.3393, -17.1442])\n",
      "    Grad:   tensor([-0.0049,  0.0273])\n",
      "Epoch 2761, Loss 2.929905\n",
      "    Params: tensor([  5.3394, -17.1444])\n",
      "    Grad:   tensor([-0.0048,  0.0273])\n",
      "Epoch 2762, Loss 2.929896\n",
      "    Params: tensor([  5.3394, -17.1447])\n",
      "    Grad:   tensor([-0.0048,  0.0272])\n",
      "Epoch 2763, Loss 2.929891\n",
      "    Params: tensor([  5.3395, -17.1450])\n",
      "    Grad:   tensor([-0.0048,  0.0272])\n",
      "Epoch 2764, Loss 2.929882\n",
      "    Params: tensor([  5.3395, -17.1453])\n",
      "    Grad:   tensor([-0.0048,  0.0271])\n",
      "Epoch 2765, Loss 2.929875\n",
      "    Params: tensor([  5.3396, -17.1455])\n",
      "    Grad:   tensor([-0.0048,  0.0271])\n",
      "Epoch 2766, Loss 2.929868\n",
      "    Params: tensor([  5.3396, -17.1458])\n",
      "    Grad:   tensor([-0.0048,  0.0271])\n",
      "Epoch 2767, Loss 2.929859\n",
      "    Params: tensor([  5.3397, -17.1461])\n",
      "    Grad:   tensor([-0.0048,  0.0270])\n",
      "Epoch 2768, Loss 2.929852\n",
      "    Params: tensor([  5.3397, -17.1463])\n",
      "    Grad:   tensor([-0.0048,  0.0270])\n",
      "Epoch 2769, Loss 2.929845\n",
      "    Params: tensor([  5.3398, -17.1466])\n",
      "    Grad:   tensor([-0.0048,  0.0269])\n",
      "Epoch 2770, Loss 2.929838\n",
      "    Params: tensor([  5.3398, -17.1469])\n",
      "    Grad:   tensor([-0.0047,  0.0269])\n",
      "Epoch 2771, Loss 2.929830\n",
      "    Params: tensor([  5.3399, -17.1471])\n",
      "    Grad:   tensor([-0.0047,  0.0268])\n",
      "Epoch 2772, Loss 2.929822\n",
      "    Params: tensor([  5.3399, -17.1474])\n",
      "    Grad:   tensor([-0.0047,  0.0268])\n",
      "Epoch 2773, Loss 2.929816\n",
      "    Params: tensor([  5.3400, -17.1477])\n",
      "    Grad:   tensor([-0.0047,  0.0267])\n",
      "Epoch 2774, Loss 2.929807\n",
      "    Params: tensor([  5.3400, -17.1479])\n",
      "    Grad:   tensor([-0.0047,  0.0267])\n",
      "Epoch 2775, Loss 2.929800\n",
      "    Params: tensor([  5.3401, -17.1482])\n",
      "    Grad:   tensor([-0.0047,  0.0266])\n",
      "Epoch 2776, Loss 2.929794\n",
      "    Params: tensor([  5.3401, -17.1485])\n",
      "    Grad:   tensor([-0.0047,  0.0266])\n",
      "Epoch 2777, Loss 2.929786\n",
      "    Params: tensor([  5.3402, -17.1487])\n",
      "    Grad:   tensor([-0.0047,  0.0266])\n",
      "Epoch 2778, Loss 2.929778\n",
      "    Params: tensor([  5.3402, -17.1490])\n",
      "    Grad:   tensor([-0.0047,  0.0265])\n",
      "Epoch 2779, Loss 2.929771\n",
      "    Params: tensor([  5.3402, -17.1493])\n",
      "    Grad:   tensor([-0.0047,  0.0265])\n",
      "Epoch 2780, Loss 2.929765\n",
      "    Params: tensor([  5.3403, -17.1495])\n",
      "    Grad:   tensor([-0.0047,  0.0264])\n",
      "Epoch 2781, Loss 2.929757\n",
      "    Params: tensor([  5.3403, -17.1498])\n",
      "    Grad:   tensor([-0.0047,  0.0264])\n",
      "Epoch 2782, Loss 2.929750\n",
      "    Params: tensor([  5.3404, -17.1501])\n",
      "    Grad:   tensor([-0.0046,  0.0263])\n",
      "Epoch 2783, Loss 2.929743\n",
      "    Params: tensor([  5.3404, -17.1503])\n",
      "    Grad:   tensor([-0.0046,  0.0263])\n",
      "Epoch 2784, Loss 2.929735\n",
      "    Params: tensor([  5.3405, -17.1506])\n",
      "    Grad:   tensor([-0.0046,  0.0262])\n",
      "Epoch 2785, Loss 2.929729\n",
      "    Params: tensor([  5.3405, -17.1508])\n",
      "    Grad:   tensor([-0.0047,  0.0262])\n",
      "Epoch 2786, Loss 2.929722\n",
      "    Params: tensor([  5.3406, -17.1511])\n",
      "    Grad:   tensor([-0.0046,  0.0262])\n",
      "Epoch 2787, Loss 2.929714\n",
      "    Params: tensor([  5.3406, -17.1514])\n",
      "    Grad:   tensor([-0.0046,  0.0261])\n",
      "Epoch 2788, Loss 2.929707\n",
      "    Params: tensor([  5.3407, -17.1516])\n",
      "    Grad:   tensor([-0.0046,  0.0261])\n",
      "Epoch 2789, Loss 2.929701\n",
      "    Params: tensor([  5.3407, -17.1519])\n",
      "    Grad:   tensor([-0.0046,  0.0260])\n",
      "Epoch 2790, Loss 2.929692\n",
      "    Params: tensor([  5.3408, -17.1522])\n",
      "    Grad:   tensor([-0.0046,  0.0260])\n",
      "Epoch 2791, Loss 2.929685\n",
      "    Params: tensor([  5.3408, -17.1524])\n",
      "    Grad:   tensor([-0.0046,  0.0259])\n",
      "Epoch 2792, Loss 2.929681\n",
      "    Params: tensor([  5.3408, -17.1527])\n",
      "    Grad:   tensor([-0.0046,  0.0259])\n",
      "Epoch 2793, Loss 2.929672\n",
      "    Params: tensor([  5.3409, -17.1529])\n",
      "    Grad:   tensor([-0.0046,  0.0258])\n",
      "Epoch 2794, Loss 2.929666\n",
      "    Params: tensor([  5.3409, -17.1532])\n",
      "    Grad:   tensor([-0.0046,  0.0258])\n",
      "Epoch 2795, Loss 2.929659\n",
      "    Params: tensor([  5.3410, -17.1534])\n",
      "    Grad:   tensor([-0.0045,  0.0258])\n",
      "Epoch 2796, Loss 2.929653\n",
      "    Params: tensor([  5.3410, -17.1537])\n",
      "    Grad:   tensor([-0.0045,  0.0257])\n",
      "Epoch 2797, Loss 2.929646\n",
      "    Params: tensor([  5.3411, -17.1540])\n",
      "    Grad:   tensor([-0.0045,  0.0257])\n",
      "Epoch 2798, Loss 2.929638\n",
      "    Params: tensor([  5.3411, -17.1542])\n",
      "    Grad:   tensor([-0.0045,  0.0256])\n",
      "Epoch 2799, Loss 2.929632\n",
      "    Params: tensor([  5.3412, -17.1545])\n",
      "    Grad:   tensor([-0.0045,  0.0256])\n",
      "Epoch 2800, Loss 2.929626\n",
      "    Params: tensor([  5.3412, -17.1547])\n",
      "    Grad:   tensor([-0.0045,  0.0255])\n",
      "Epoch 2801, Loss 2.929620\n",
      "    Params: tensor([  5.3413, -17.1550])\n",
      "    Grad:   tensor([-0.0045,  0.0255])\n",
      "Epoch 2802, Loss 2.929611\n",
      "    Params: tensor([  5.3413, -17.1552])\n",
      "    Grad:   tensor([-0.0045,  0.0254])\n",
      "Epoch 2803, Loss 2.929605\n",
      "    Params: tensor([  5.3413, -17.1555])\n",
      "    Grad:   tensor([-0.0045,  0.0254])\n",
      "Epoch 2804, Loss 2.929600\n",
      "    Params: tensor([  5.3414, -17.1557])\n",
      "    Grad:   tensor([-0.0045,  0.0254])\n",
      "Epoch 2805, Loss 2.929592\n",
      "    Params: tensor([  5.3414, -17.1560])\n",
      "    Grad:   tensor([-0.0045,  0.0253])\n",
      "Epoch 2806, Loss 2.929586\n",
      "    Params: tensor([  5.3415, -17.1562])\n",
      "    Grad:   tensor([-0.0045,  0.0253])\n",
      "Epoch 2807, Loss 2.929579\n",
      "    Params: tensor([  5.3415, -17.1565])\n",
      "    Grad:   tensor([-0.0045,  0.0252])\n",
      "Epoch 2808, Loss 2.929572\n",
      "    Params: tensor([  5.3416, -17.1568])\n",
      "    Grad:   tensor([-0.0044,  0.0252])\n",
      "Epoch 2809, Loss 2.929566\n",
      "    Params: tensor([  5.3416, -17.1570])\n",
      "    Grad:   tensor([-0.0044,  0.0251])\n",
      "Epoch 2810, Loss 2.929559\n",
      "    Params: tensor([  5.3417, -17.1573])\n",
      "    Grad:   tensor([-0.0044,  0.0251])\n",
      "Epoch 2811, Loss 2.929551\n",
      "    Params: tensor([  5.3417, -17.1575])\n",
      "    Grad:   tensor([-0.0044,  0.0251])\n",
      "Epoch 2812, Loss 2.929545\n",
      "    Params: tensor([  5.3417, -17.1578])\n",
      "    Grad:   tensor([-0.0044,  0.0250])\n",
      "Epoch 2813, Loss 2.929540\n",
      "    Params: tensor([  5.3418, -17.1580])\n",
      "    Grad:   tensor([-0.0044,  0.0250])\n",
      "Epoch 2814, Loss 2.929533\n",
      "    Params: tensor([  5.3418, -17.1583])\n",
      "    Grad:   tensor([-0.0044,  0.0249])\n",
      "Epoch 2815, Loss 2.929528\n",
      "    Params: tensor([  5.3419, -17.1585])\n",
      "    Grad:   tensor([-0.0044,  0.0249])\n",
      "Epoch 2816, Loss 2.929521\n",
      "    Params: tensor([  5.3419, -17.1588])\n",
      "    Grad:   tensor([-0.0044,  0.0249])\n",
      "Epoch 2817, Loss 2.929513\n",
      "    Params: tensor([  5.3420, -17.1590])\n",
      "    Grad:   tensor([-0.0044,  0.0248])\n",
      "Epoch 2818, Loss 2.929507\n",
      "    Params: tensor([  5.3420, -17.1592])\n",
      "    Grad:   tensor([-0.0043,  0.0248])\n",
      "Epoch 2819, Loss 2.929501\n",
      "    Params: tensor([  5.3421, -17.1595])\n",
      "    Grad:   tensor([-0.0044,  0.0247])\n",
      "Epoch 2820, Loss 2.929496\n",
      "    Params: tensor([  5.3421, -17.1597])\n",
      "    Grad:   tensor([-0.0044,  0.0247])\n",
      "Epoch 2821, Loss 2.929489\n",
      "    Params: tensor([  5.3421, -17.1600])\n",
      "    Grad:   tensor([-0.0044,  0.0246])\n",
      "Epoch 2822, Loss 2.929482\n",
      "    Params: tensor([  5.3422, -17.1602])\n",
      "    Grad:   tensor([-0.0043,  0.0246])\n",
      "Epoch 2823, Loss 2.929476\n",
      "    Params: tensor([  5.3422, -17.1605])\n",
      "    Grad:   tensor([-0.0043,  0.0246])\n",
      "Epoch 2824, Loss 2.929471\n",
      "    Params: tensor([  5.3423, -17.1607])\n",
      "    Grad:   tensor([-0.0043,  0.0245])\n",
      "Epoch 2825, Loss 2.929463\n",
      "    Params: tensor([  5.3423, -17.1610])\n",
      "    Grad:   tensor([-0.0043,  0.0245])\n",
      "Epoch 2826, Loss 2.929458\n",
      "    Params: tensor([  5.3424, -17.1612])\n",
      "    Grad:   tensor([-0.0043,  0.0244])\n",
      "Epoch 2827, Loss 2.929452\n",
      "    Params: tensor([  5.3424, -17.1615])\n",
      "    Grad:   tensor([-0.0043,  0.0244])\n",
      "Epoch 2828, Loss 2.929445\n",
      "    Params: tensor([  5.3424, -17.1617])\n",
      "    Grad:   tensor([-0.0043,  0.0243])\n",
      "Epoch 2829, Loss 2.929439\n",
      "    Params: tensor([  5.3425, -17.1619])\n",
      "    Grad:   tensor([-0.0043,  0.0243])\n",
      "Epoch 2830, Loss 2.929433\n",
      "    Params: tensor([  5.3425, -17.1622])\n",
      "    Grad:   tensor([-0.0043,  0.0243])\n",
      "Epoch 2831, Loss 2.929427\n",
      "    Params: tensor([  5.3426, -17.1624])\n",
      "    Grad:   tensor([-0.0043,  0.0242])\n",
      "Epoch 2832, Loss 2.929421\n",
      "    Params: tensor([  5.3426, -17.1627])\n",
      "    Grad:   tensor([-0.0043,  0.0242])\n",
      "Epoch 2833, Loss 2.929415\n",
      "    Params: tensor([  5.3427, -17.1629])\n",
      "    Grad:   tensor([-0.0043,  0.0241])\n",
      "Epoch 2834, Loss 2.929409\n",
      "    Params: tensor([  5.3427, -17.1632])\n",
      "    Grad:   tensor([-0.0043,  0.0241])\n",
      "Epoch 2835, Loss 2.929404\n",
      "    Params: tensor([  5.3427, -17.1634])\n",
      "    Grad:   tensor([-0.0043,  0.0241])\n",
      "Epoch 2836, Loss 2.929396\n",
      "    Params: tensor([  5.3428, -17.1636])\n",
      "    Grad:   tensor([-0.0042,  0.0240])\n",
      "Epoch 2837, Loss 2.929391\n",
      "    Params: tensor([  5.3428, -17.1639])\n",
      "    Grad:   tensor([-0.0042,  0.0240])\n",
      "Epoch 2838, Loss 2.929383\n",
      "    Params: tensor([  5.3429, -17.1641])\n",
      "    Grad:   tensor([-0.0042,  0.0239])\n",
      "Epoch 2839, Loss 2.929380\n",
      "    Params: tensor([  5.3429, -17.1644])\n",
      "    Grad:   tensor([-0.0042,  0.0239])\n",
      "Epoch 2840, Loss 2.929373\n",
      "    Params: tensor([  5.3430, -17.1646])\n",
      "    Grad:   tensor([-0.0042,  0.0239])\n",
      "Epoch 2841, Loss 2.929368\n",
      "    Params: tensor([  5.3430, -17.1648])\n",
      "    Grad:   tensor([-0.0042,  0.0238])\n",
      "Epoch 2842, Loss 2.929361\n",
      "    Params: tensor([  5.3430, -17.1651])\n",
      "    Grad:   tensor([-0.0042,  0.0238])\n",
      "Epoch 2843, Loss 2.929356\n",
      "    Params: tensor([  5.3431, -17.1653])\n",
      "    Grad:   tensor([-0.0042,  0.0237])\n",
      "Epoch 2844, Loss 2.929351\n",
      "    Params: tensor([  5.3431, -17.1655])\n",
      "    Grad:   tensor([-0.0042,  0.0237])\n",
      "Epoch 2845, Loss 2.929344\n",
      "    Params: tensor([  5.3432, -17.1658])\n",
      "    Grad:   tensor([-0.0042,  0.0237])\n",
      "Epoch 2846, Loss 2.929338\n",
      "    Params: tensor([  5.3432, -17.1660])\n",
      "    Grad:   tensor([-0.0042,  0.0236])\n",
      "Epoch 2847, Loss 2.929332\n",
      "    Params: tensor([  5.3432, -17.1662])\n",
      "    Grad:   tensor([-0.0042,  0.0236])\n",
      "Epoch 2848, Loss 2.929328\n",
      "    Params: tensor([  5.3433, -17.1665])\n",
      "    Grad:   tensor([-0.0042,  0.0235])\n",
      "Epoch 2849, Loss 2.929321\n",
      "    Params: tensor([  5.3433, -17.1667])\n",
      "    Grad:   tensor([-0.0041,  0.0235])\n",
      "Epoch 2850, Loss 2.929316\n",
      "    Params: tensor([  5.3434, -17.1670])\n",
      "    Grad:   tensor([-0.0041,  0.0235])\n",
      "Epoch 2851, Loss 2.929309\n",
      "    Params: tensor([  5.3434, -17.1672])\n",
      "    Grad:   tensor([-0.0041,  0.0234])\n",
      "Epoch 2852, Loss 2.929304\n",
      "    Params: tensor([  5.3435, -17.1674])\n",
      "    Grad:   tensor([-0.0041,  0.0234])\n",
      "Epoch 2853, Loss 2.929300\n",
      "    Params: tensor([  5.3435, -17.1677])\n",
      "    Grad:   tensor([-0.0041,  0.0233])\n",
      "Epoch 2854, Loss 2.929293\n",
      "    Params: tensor([  5.3435, -17.1679])\n",
      "    Grad:   tensor([-0.0041,  0.0233])\n",
      "Epoch 2855, Loss 2.929288\n",
      "    Params: tensor([  5.3436, -17.1681])\n",
      "    Grad:   tensor([-0.0041,  0.0233])\n",
      "Epoch 2856, Loss 2.929282\n",
      "    Params: tensor([  5.3436, -17.1684])\n",
      "    Grad:   tensor([-0.0041,  0.0232])\n",
      "Epoch 2857, Loss 2.929277\n",
      "    Params: tensor([  5.3437, -17.1686])\n",
      "    Grad:   tensor([-0.0041,  0.0232])\n",
      "Epoch 2858, Loss 2.929271\n",
      "    Params: tensor([  5.3437, -17.1688])\n",
      "    Grad:   tensor([-0.0041,  0.0231])\n",
      "Epoch 2859, Loss 2.929266\n",
      "    Params: tensor([  5.3437, -17.1690])\n",
      "    Grad:   tensor([-0.0041,  0.0231])\n",
      "Epoch 2860, Loss 2.929260\n",
      "    Params: tensor([  5.3438, -17.1693])\n",
      "    Grad:   tensor([-0.0041,  0.0231])\n",
      "Epoch 2861, Loss 2.929255\n",
      "    Params: tensor([  5.3438, -17.1695])\n",
      "    Grad:   tensor([-0.0041,  0.0230])\n",
      "Epoch 2862, Loss 2.929250\n",
      "    Params: tensor([  5.3439, -17.1697])\n",
      "    Grad:   tensor([-0.0041,  0.0230])\n",
      "Epoch 2863, Loss 2.929244\n",
      "    Params: tensor([  5.3439, -17.1700])\n",
      "    Grad:   tensor([-0.0040,  0.0229])\n",
      "Epoch 2864, Loss 2.929238\n",
      "    Params: tensor([  5.3439, -17.1702])\n",
      "    Grad:   tensor([-0.0040,  0.0229])\n",
      "Epoch 2865, Loss 2.929234\n",
      "    Params: tensor([  5.3440, -17.1704])\n",
      "    Grad:   tensor([-0.0040,  0.0229])\n",
      "Epoch 2866, Loss 2.929228\n",
      "    Params: tensor([  5.3440, -17.1707])\n",
      "    Grad:   tensor([-0.0040,  0.0228])\n",
      "Epoch 2867, Loss 2.929222\n",
      "    Params: tensor([  5.3441, -17.1709])\n",
      "    Grad:   tensor([-0.0040,  0.0228])\n",
      "Epoch 2868, Loss 2.929217\n",
      "    Params: tensor([  5.3441, -17.1711])\n",
      "    Grad:   tensor([-0.0040,  0.0227])\n",
      "Epoch 2869, Loss 2.929211\n",
      "    Params: tensor([  5.3441, -17.1713])\n",
      "    Grad:   tensor([-0.0040,  0.0227])\n",
      "Epoch 2870, Loss 2.929208\n",
      "    Params: tensor([  5.3442, -17.1716])\n",
      "    Grad:   tensor([-0.0040,  0.0227])\n",
      "Epoch 2871, Loss 2.929201\n",
      "    Params: tensor([  5.3442, -17.1718])\n",
      "    Grad:   tensor([-0.0040,  0.0226])\n",
      "Epoch 2872, Loss 2.929195\n",
      "    Params: tensor([  5.3443, -17.1720])\n",
      "    Grad:   tensor([-0.0040,  0.0226])\n",
      "Epoch 2873, Loss 2.929191\n",
      "    Params: tensor([  5.3443, -17.1722])\n",
      "    Grad:   tensor([-0.0040,  0.0226])\n",
      "Epoch 2874, Loss 2.929185\n",
      "    Params: tensor([  5.3443, -17.1725])\n",
      "    Grad:   tensor([-0.0040,  0.0225])\n",
      "Epoch 2875, Loss 2.929180\n",
      "    Params: tensor([  5.3444, -17.1727])\n",
      "    Grad:   tensor([-0.0040,  0.0225])\n",
      "Epoch 2876, Loss 2.929175\n",
      "    Params: tensor([  5.3444, -17.1729])\n",
      "    Grad:   tensor([-0.0040,  0.0224])\n",
      "Epoch 2877, Loss 2.929170\n",
      "    Params: tensor([  5.3445, -17.1731])\n",
      "    Grad:   tensor([-0.0040,  0.0224])\n",
      "Epoch 2878, Loss 2.929165\n",
      "    Params: tensor([  5.3445, -17.1734])\n",
      "    Grad:   tensor([-0.0040,  0.0224])\n",
      "Epoch 2879, Loss 2.929160\n",
      "    Params: tensor([  5.3445, -17.1736])\n",
      "    Grad:   tensor([-0.0039,  0.0223])\n",
      "Epoch 2880, Loss 2.929155\n",
      "    Params: tensor([  5.3446, -17.1738])\n",
      "    Grad:   tensor([-0.0039,  0.0223])\n",
      "Epoch 2881, Loss 2.929149\n",
      "    Params: tensor([  5.3446, -17.1740])\n",
      "    Grad:   tensor([-0.0039,  0.0223])\n",
      "Epoch 2882, Loss 2.929143\n",
      "    Params: tensor([  5.3447, -17.1742])\n",
      "    Grad:   tensor([-0.0039,  0.0222])\n",
      "Epoch 2883, Loss 2.929139\n",
      "    Params: tensor([  5.3447, -17.1745])\n",
      "    Grad:   tensor([-0.0039,  0.0222])\n",
      "Epoch 2884, Loss 2.929133\n",
      "    Params: tensor([  5.3447, -17.1747])\n",
      "    Grad:   tensor([-0.0039,  0.0221])\n",
      "Epoch 2885, Loss 2.929128\n",
      "    Params: tensor([  5.3448, -17.1749])\n",
      "    Grad:   tensor([-0.0039,  0.0221])\n",
      "Epoch 2886, Loss 2.929122\n",
      "    Params: tensor([  5.3448, -17.1751])\n",
      "    Grad:   tensor([-0.0039,  0.0221])\n",
      "Epoch 2887, Loss 2.929119\n",
      "    Params: tensor([  5.3449, -17.1754])\n",
      "    Grad:   tensor([-0.0039,  0.0220])\n",
      "Epoch 2888, Loss 2.929113\n",
      "    Params: tensor([  5.3449, -17.1756])\n",
      "    Grad:   tensor([-0.0039,  0.0220])\n",
      "Epoch 2889, Loss 2.929108\n",
      "    Params: tensor([  5.3449, -17.1758])\n",
      "    Grad:   tensor([-0.0039,  0.0220])\n",
      "Epoch 2890, Loss 2.929104\n",
      "    Params: tensor([  5.3450, -17.1760])\n",
      "    Grad:   tensor([-0.0039,  0.0219])\n",
      "Epoch 2891, Loss 2.929099\n",
      "    Params: tensor([  5.3450, -17.1762])\n",
      "    Grad:   tensor([-0.0039,  0.0219])\n",
      "Epoch 2892, Loss 2.929093\n",
      "    Params: tensor([  5.3450, -17.1764])\n",
      "    Grad:   tensor([-0.0039,  0.0218])\n",
      "Epoch 2893, Loss 2.929088\n",
      "    Params: tensor([  5.3451, -17.1767])\n",
      "    Grad:   tensor([-0.0039,  0.0218])\n",
      "Epoch 2894, Loss 2.929083\n",
      "    Params: tensor([  5.3451, -17.1769])\n",
      "    Grad:   tensor([-0.0038,  0.0218])\n",
      "Epoch 2895, Loss 2.929079\n",
      "    Params: tensor([  5.3452, -17.1771])\n",
      "    Grad:   tensor([-0.0038,  0.0217])\n",
      "Epoch 2896, Loss 2.929074\n",
      "    Params: tensor([  5.3452, -17.1773])\n",
      "    Grad:   tensor([-0.0038,  0.0217])\n",
      "Epoch 2897, Loss 2.929069\n",
      "    Params: tensor([  5.3452, -17.1775])\n",
      "    Grad:   tensor([-0.0038,  0.0217])\n",
      "Epoch 2898, Loss 2.929065\n",
      "    Params: tensor([  5.3453, -17.1777])\n",
      "    Grad:   tensor([-0.0038,  0.0216])\n",
      "Epoch 2899, Loss 2.929058\n",
      "    Params: tensor([  5.3453, -17.1780])\n",
      "    Grad:   tensor([-0.0038,  0.0216])\n",
      "Epoch 2900, Loss 2.929054\n",
      "    Params: tensor([  5.3454, -17.1782])\n",
      "    Grad:   tensor([-0.0038,  0.0215])\n",
      "Epoch 2901, Loss 2.929050\n",
      "    Params: tensor([  5.3454, -17.1784])\n",
      "    Grad:   tensor([-0.0038,  0.0215])\n",
      "Epoch 2902, Loss 2.929044\n",
      "    Params: tensor([  5.3454, -17.1786])\n",
      "    Grad:   tensor([-0.0038,  0.0215])\n",
      "Epoch 2903, Loss 2.929041\n",
      "    Params: tensor([  5.3455, -17.1788])\n",
      "    Grad:   tensor([-0.0038,  0.0214])\n",
      "Epoch 2904, Loss 2.929036\n",
      "    Params: tensor([  5.3455, -17.1790])\n",
      "    Grad:   tensor([-0.0038,  0.0214])\n",
      "Epoch 2905, Loss 2.929031\n",
      "    Params: tensor([  5.3455, -17.1793])\n",
      "    Grad:   tensor([-0.0038,  0.0214])\n",
      "Epoch 2906, Loss 2.929025\n",
      "    Params: tensor([  5.3456, -17.1795])\n",
      "    Grad:   tensor([-0.0038,  0.0213])\n",
      "Epoch 2907, Loss 2.929021\n",
      "    Params: tensor([  5.3456, -17.1797])\n",
      "    Grad:   tensor([-0.0038,  0.0213])\n",
      "Epoch 2908, Loss 2.929017\n",
      "    Params: tensor([  5.3457, -17.1799])\n",
      "    Grad:   tensor([-0.0037,  0.0213])\n",
      "Epoch 2909, Loss 2.929012\n",
      "    Params: tensor([  5.3457, -17.1801])\n",
      "    Grad:   tensor([-0.0037,  0.0212])\n",
      "Epoch 2910, Loss 2.929007\n",
      "    Params: tensor([  5.3457, -17.1803])\n",
      "    Grad:   tensor([-0.0037,  0.0212])\n",
      "Epoch 2911, Loss 2.929003\n",
      "    Params: tensor([  5.3458, -17.1805])\n",
      "    Grad:   tensor([-0.0037,  0.0211])\n",
      "Epoch 2912, Loss 2.928999\n",
      "    Params: tensor([  5.3458, -17.1807])\n",
      "    Grad:   tensor([-0.0037,  0.0211])\n",
      "Epoch 2913, Loss 2.928993\n",
      "    Params: tensor([  5.3458, -17.1809])\n",
      "    Grad:   tensor([-0.0037,  0.0211])\n",
      "Epoch 2914, Loss 2.928989\n",
      "    Params: tensor([  5.3459, -17.1812])\n",
      "    Grad:   tensor([-0.0037,  0.0210])\n",
      "Epoch 2915, Loss 2.928985\n",
      "    Params: tensor([  5.3459, -17.1814])\n",
      "    Grad:   tensor([-0.0037,  0.0210])\n",
      "Epoch 2916, Loss 2.928980\n",
      "    Params: tensor([  5.3460, -17.1816])\n",
      "    Grad:   tensor([-0.0037,  0.0210])\n",
      "Epoch 2917, Loss 2.928976\n",
      "    Params: tensor([  5.3460, -17.1818])\n",
      "    Grad:   tensor([-0.0037,  0.0209])\n",
      "Epoch 2918, Loss 2.928971\n",
      "    Params: tensor([  5.3460, -17.1820])\n",
      "    Grad:   tensor([-0.0037,  0.0209])\n",
      "Epoch 2919, Loss 2.928967\n",
      "    Params: tensor([  5.3461, -17.1822])\n",
      "    Grad:   tensor([-0.0037,  0.0209])\n",
      "Epoch 2920, Loss 2.928962\n",
      "    Params: tensor([  5.3461, -17.1824])\n",
      "    Grad:   tensor([-0.0037,  0.0208])\n",
      "Epoch 2921, Loss 2.928958\n",
      "    Params: tensor([  5.3461, -17.1826])\n",
      "    Grad:   tensor([-0.0037,  0.0208])\n",
      "Epoch 2922, Loss 2.928953\n",
      "    Params: tensor([  5.3462, -17.1828])\n",
      "    Grad:   tensor([-0.0037,  0.0208])\n",
      "Epoch 2923, Loss 2.928947\n",
      "    Params: tensor([  5.3462, -17.1830])\n",
      "    Grad:   tensor([-0.0036,  0.0207])\n",
      "Epoch 2924, Loss 2.928943\n",
      "    Params: tensor([  5.3462, -17.1832])\n",
      "    Grad:   tensor([-0.0037,  0.0207])\n",
      "Epoch 2925, Loss 2.928940\n",
      "    Params: tensor([  5.3463, -17.1834])\n",
      "    Grad:   tensor([-0.0036,  0.0206])\n",
      "Epoch 2926, Loss 2.928935\n",
      "    Params: tensor([  5.3463, -17.1837])\n",
      "    Grad:   tensor([-0.0036,  0.0206])\n",
      "Epoch 2927, Loss 2.928932\n",
      "    Params: tensor([  5.3464, -17.1839])\n",
      "    Grad:   tensor([-0.0036,  0.0206])\n",
      "Epoch 2928, Loss 2.928926\n",
      "    Params: tensor([  5.3464, -17.1841])\n",
      "    Grad:   tensor([-0.0036,  0.0205])\n",
      "Epoch 2929, Loss 2.928923\n",
      "    Params: tensor([  5.3464, -17.1843])\n",
      "    Grad:   tensor([-0.0036,  0.0205])\n",
      "Epoch 2930, Loss 2.928919\n",
      "    Params: tensor([  5.3465, -17.1845])\n",
      "    Grad:   tensor([-0.0036,  0.0205])\n",
      "Epoch 2931, Loss 2.928913\n",
      "    Params: tensor([  5.3465, -17.1847])\n",
      "    Grad:   tensor([-0.0036,  0.0204])\n",
      "Epoch 2932, Loss 2.928909\n",
      "    Params: tensor([  5.3465, -17.1849])\n",
      "    Grad:   tensor([-0.0036,  0.0204])\n",
      "Epoch 2933, Loss 2.928904\n",
      "    Params: tensor([  5.3466, -17.1851])\n",
      "    Grad:   tensor([-0.0036,  0.0204])\n",
      "Epoch 2934, Loss 2.928902\n",
      "    Params: tensor([  5.3466, -17.1853])\n",
      "    Grad:   tensor([-0.0036,  0.0203])\n",
      "Epoch 2935, Loss 2.928897\n",
      "    Params: tensor([  5.3466, -17.1855])\n",
      "    Grad:   tensor([-0.0036,  0.0203])\n",
      "Epoch 2936, Loss 2.928893\n",
      "    Params: tensor([  5.3467, -17.1857])\n",
      "    Grad:   tensor([-0.0036,  0.0203])\n",
      "Epoch 2937, Loss 2.928887\n",
      "    Params: tensor([  5.3467, -17.1859])\n",
      "    Grad:   tensor([-0.0036,  0.0202])\n",
      "Epoch 2938, Loss 2.928883\n",
      "    Params: tensor([  5.3468, -17.1861])\n",
      "    Grad:   tensor([-0.0035,  0.0202])\n",
      "Epoch 2939, Loss 2.928880\n",
      "    Params: tensor([  5.3468, -17.1863])\n",
      "    Grad:   tensor([-0.0036,  0.0202])\n",
      "Epoch 2940, Loss 2.928878\n",
      "    Params: tensor([  5.3468, -17.1865])\n",
      "    Grad:   tensor([-0.0036,  0.0201])\n",
      "Epoch 2941, Loss 2.928871\n",
      "    Params: tensor([  5.3469, -17.1867])\n",
      "    Grad:   tensor([-0.0035,  0.0201])\n",
      "Epoch 2942, Loss 2.928867\n",
      "    Params: tensor([  5.3469, -17.1869])\n",
      "    Grad:   tensor([-0.0035,  0.0201])\n",
      "Epoch 2943, Loss 2.928864\n",
      "    Params: tensor([  5.3469, -17.1871])\n",
      "    Grad:   tensor([-0.0035,  0.0200])\n",
      "Epoch 2944, Loss 2.928860\n",
      "    Params: tensor([  5.3470, -17.1873])\n",
      "    Grad:   tensor([-0.0035,  0.0200])\n",
      "Epoch 2945, Loss 2.928855\n",
      "    Params: tensor([  5.3470, -17.1875])\n",
      "    Grad:   tensor([-0.0035,  0.0200])\n",
      "Epoch 2946, Loss 2.928850\n",
      "    Params: tensor([  5.3470, -17.1877])\n",
      "    Grad:   tensor([-0.0035,  0.0199])\n",
      "Epoch 2947, Loss 2.928845\n",
      "    Params: tensor([  5.3471, -17.1879])\n",
      "    Grad:   tensor([-0.0035,  0.0199])\n",
      "Epoch 2948, Loss 2.928843\n",
      "    Params: tensor([  5.3471, -17.1881])\n",
      "    Grad:   tensor([-0.0035,  0.0199])\n",
      "Epoch 2949, Loss 2.928838\n",
      "    Params: tensor([  5.3471, -17.1883])\n",
      "    Grad:   tensor([-0.0035,  0.0198])\n",
      "Epoch 2950, Loss 2.928833\n",
      "    Params: tensor([  5.3472, -17.1885])\n",
      "    Grad:   tensor([-0.0035,  0.0198])\n",
      "Epoch 2951, Loss 2.928830\n",
      "    Params: tensor([  5.3472, -17.1887])\n",
      "    Grad:   tensor([-0.0035,  0.0198])\n",
      "Epoch 2952, Loss 2.928826\n",
      "    Params: tensor([  5.3472, -17.1889])\n",
      "    Grad:   tensor([-0.0035,  0.0197])\n",
      "Epoch 2953, Loss 2.928823\n",
      "    Params: tensor([  5.3473, -17.1891])\n",
      "    Grad:   tensor([-0.0035,  0.0197])\n",
      "Epoch 2954, Loss 2.928818\n",
      "    Params: tensor([  5.3473, -17.1893])\n",
      "    Grad:   tensor([-0.0035,  0.0197])\n",
      "Epoch 2955, Loss 2.928816\n",
      "    Params: tensor([  5.3474, -17.1895])\n",
      "    Grad:   tensor([-0.0035,  0.0196])\n",
      "Epoch 2956, Loss 2.928811\n",
      "    Params: tensor([  5.3474, -17.1897])\n",
      "    Grad:   tensor([-0.0035,  0.0196])\n",
      "Epoch 2957, Loss 2.928805\n",
      "    Params: tensor([  5.3474, -17.1899])\n",
      "    Grad:   tensor([-0.0034,  0.0196])\n",
      "Epoch 2958, Loss 2.928802\n",
      "    Params: tensor([  5.3475, -17.1901])\n",
      "    Grad:   tensor([-0.0035,  0.0195])\n",
      "Epoch 2959, Loss 2.928799\n",
      "    Params: tensor([  5.3475, -17.1903])\n",
      "    Grad:   tensor([-0.0034,  0.0195])\n",
      "Epoch 2960, Loss 2.928795\n",
      "    Params: tensor([  5.3475, -17.1905])\n",
      "    Grad:   tensor([-0.0034,  0.0195])\n",
      "Epoch 2961, Loss 2.928789\n",
      "    Params: tensor([  5.3476, -17.1907])\n",
      "    Grad:   tensor([-0.0034,  0.0194])\n",
      "Epoch 2962, Loss 2.928789\n",
      "    Params: tensor([  5.3476, -17.1908])\n",
      "    Grad:   tensor([-0.0034,  0.0194])\n",
      "Epoch 2963, Loss 2.928783\n",
      "    Params: tensor([  5.3476, -17.1910])\n",
      "    Grad:   tensor([-0.0034,  0.0194])\n",
      "Epoch 2964, Loss 2.928779\n",
      "    Params: tensor([  5.3477, -17.1912])\n",
      "    Grad:   tensor([-0.0034,  0.0193])\n",
      "Epoch 2965, Loss 2.928775\n",
      "    Params: tensor([  5.3477, -17.1914])\n",
      "    Grad:   tensor([-0.0034,  0.0193])\n",
      "Epoch 2966, Loss 2.928771\n",
      "    Params: tensor([  5.3477, -17.1916])\n",
      "    Grad:   tensor([-0.0034,  0.0193])\n",
      "Epoch 2967, Loss 2.928767\n",
      "    Params: tensor([  5.3478, -17.1918])\n",
      "    Grad:   tensor([-0.0034,  0.0192])\n",
      "Epoch 2968, Loss 2.928765\n",
      "    Params: tensor([  5.3478, -17.1920])\n",
      "    Grad:   tensor([-0.0034,  0.0192])\n",
      "Epoch 2969, Loss 2.928761\n",
      "    Params: tensor([  5.3478, -17.1922])\n",
      "    Grad:   tensor([-0.0034,  0.0192])\n",
      "Epoch 2970, Loss 2.928758\n",
      "    Params: tensor([  5.3479, -17.1924])\n",
      "    Grad:   tensor([-0.0034,  0.0191])\n",
      "Epoch 2971, Loss 2.928752\n",
      "    Params: tensor([  5.3479, -17.1926])\n",
      "    Grad:   tensor([-0.0034,  0.0191])\n",
      "Epoch 2972, Loss 2.928750\n",
      "    Params: tensor([  5.3479, -17.1928])\n",
      "    Grad:   tensor([-0.0034,  0.0191])\n",
      "Epoch 2973, Loss 2.928745\n",
      "    Params: tensor([  5.3480, -17.1930])\n",
      "    Grad:   tensor([-0.0034,  0.0190])\n",
      "Epoch 2974, Loss 2.928741\n",
      "    Params: tensor([  5.3480, -17.1931])\n",
      "    Grad:   tensor([-0.0034,  0.0190])\n",
      "Epoch 2975, Loss 2.928737\n",
      "    Params: tensor([  5.3480, -17.1933])\n",
      "    Grad:   tensor([-0.0034,  0.0190])\n",
      "Epoch 2976, Loss 2.928735\n",
      "    Params: tensor([  5.3481, -17.1935])\n",
      "    Grad:   tensor([-0.0033,  0.0189])\n",
      "Epoch 2977, Loss 2.928730\n",
      "    Params: tensor([  5.3481, -17.1937])\n",
      "    Grad:   tensor([-0.0033,  0.0189])\n",
      "Epoch 2978, Loss 2.928727\n",
      "    Params: tensor([  5.3481, -17.1939])\n",
      "    Grad:   tensor([-0.0033,  0.0189])\n",
      "Epoch 2979, Loss 2.928723\n",
      "    Params: tensor([  5.3482, -17.1941])\n",
      "    Grad:   tensor([-0.0033,  0.0188])\n",
      "Epoch 2980, Loss 2.928719\n",
      "    Params: tensor([  5.3482, -17.1943])\n",
      "    Grad:   tensor([-0.0033,  0.0188])\n",
      "Epoch 2981, Loss 2.928716\n",
      "    Params: tensor([  5.3482, -17.1945])\n",
      "    Grad:   tensor([-0.0033,  0.0188])\n",
      "Epoch 2982, Loss 2.928712\n",
      "    Params: tensor([  5.3483, -17.1947])\n",
      "    Grad:   tensor([-0.0033,  0.0187])\n",
      "Epoch 2983, Loss 2.928708\n",
      "    Params: tensor([  5.3483, -17.1948])\n",
      "    Grad:   tensor([-0.0033,  0.0187])\n",
      "Epoch 2984, Loss 2.928705\n",
      "    Params: tensor([  5.3483, -17.1950])\n",
      "    Grad:   tensor([-0.0033,  0.0187])\n",
      "Epoch 2985, Loss 2.928700\n",
      "    Params: tensor([  5.3484, -17.1952])\n",
      "    Grad:   tensor([-0.0033,  0.0186])\n",
      "Epoch 2986, Loss 2.928698\n",
      "    Params: tensor([  5.3484, -17.1954])\n",
      "    Grad:   tensor([-0.0033,  0.0186])\n",
      "Epoch 2987, Loss 2.928695\n",
      "    Params: tensor([  5.3484, -17.1956])\n",
      "    Grad:   tensor([-0.0033,  0.0186])\n",
      "Epoch 2988, Loss 2.928690\n",
      "    Params: tensor([  5.3485, -17.1958])\n",
      "    Grad:   tensor([-0.0033,  0.0186])\n",
      "Epoch 2989, Loss 2.928687\n",
      "    Params: tensor([  5.3485, -17.1960])\n",
      "    Grad:   tensor([-0.0033,  0.0185])\n",
      "Epoch 2990, Loss 2.928684\n",
      "    Params: tensor([  5.3485, -17.1961])\n",
      "    Grad:   tensor([-0.0033,  0.0185])\n",
      "Epoch 2991, Loss 2.928679\n",
      "    Params: tensor([  5.3486, -17.1963])\n",
      "    Grad:   tensor([-0.0032,  0.0185])\n",
      "Epoch 2992, Loss 2.928677\n",
      "    Params: tensor([  5.3486, -17.1965])\n",
      "    Grad:   tensor([-0.0033,  0.0184])\n",
      "Epoch 2993, Loss 2.928673\n",
      "    Params: tensor([  5.3486, -17.1967])\n",
      "    Grad:   tensor([-0.0033,  0.0184])\n",
      "Epoch 2994, Loss 2.928669\n",
      "    Params: tensor([  5.3487, -17.1969])\n",
      "    Grad:   tensor([-0.0033,  0.0184])\n",
      "Epoch 2995, Loss 2.928666\n",
      "    Params: tensor([  5.3487, -17.1971])\n",
      "    Grad:   tensor([-0.0032,  0.0183])\n",
      "Epoch 2996, Loss 2.928662\n",
      "    Params: tensor([  5.3487, -17.1972])\n",
      "    Grad:   tensor([-0.0032,  0.0183])\n",
      "Epoch 2997, Loss 2.928660\n",
      "    Params: tensor([  5.3488, -17.1974])\n",
      "    Grad:   tensor([-0.0032,  0.0183])\n",
      "Epoch 2998, Loss 2.928656\n",
      "    Params: tensor([  5.3488, -17.1976])\n",
      "    Grad:   tensor([-0.0032,  0.0182])\n",
      "Epoch 2999, Loss 2.928651\n",
      "    Params: tensor([  5.3488, -17.1978])\n",
      "    Grad:   tensor([-0.0032,  0.0182])\n",
      "Epoch 3000, Loss 2.928648\n",
      "    Params: tensor([  5.3489, -17.1980])\n",
      "    Grad:   tensor([-0.0032,  0.0182])\n",
      "Epoch 3001, Loss 2.928646\n",
      "    Params: tensor([  5.3489, -17.1982])\n",
      "    Grad:   tensor([-0.0032,  0.0181])\n",
      "Epoch 3002, Loss 2.928643\n",
      "    Params: tensor([  5.3489, -17.1983])\n",
      "    Grad:   tensor([-0.0032,  0.0181])\n",
      "Epoch 3003, Loss 2.928638\n",
      "    Params: tensor([  5.3489, -17.1985])\n",
      "    Grad:   tensor([-0.0032,  0.0181])\n",
      "Epoch 3004, Loss 2.928635\n",
      "    Params: tensor([  5.3490, -17.1987])\n",
      "    Grad:   tensor([-0.0032,  0.0181])\n",
      "Epoch 3005, Loss 2.928632\n",
      "    Params: tensor([  5.3490, -17.1989])\n",
      "    Grad:   tensor([-0.0032,  0.0180])\n",
      "Epoch 3006, Loss 2.928629\n",
      "    Params: tensor([  5.3490, -17.1991])\n",
      "    Grad:   tensor([-0.0032,  0.0180])\n",
      "Epoch 3007, Loss 2.928625\n",
      "    Params: tensor([  5.3491, -17.1992])\n",
      "    Grad:   tensor([-0.0032,  0.0180])\n",
      "Epoch 3008, Loss 2.928621\n",
      "    Params: tensor([  5.3491, -17.1994])\n",
      "    Grad:   tensor([-0.0032,  0.0179])\n",
      "Epoch 3009, Loss 2.928617\n",
      "    Params: tensor([  5.3491, -17.1996])\n",
      "    Grad:   tensor([-0.0032,  0.0179])\n",
      "Epoch 3010, Loss 2.928616\n",
      "    Params: tensor([  5.3492, -17.1998])\n",
      "    Grad:   tensor([-0.0032,  0.0179])\n",
      "Epoch 3011, Loss 2.928612\n",
      "    Params: tensor([  5.3492, -17.2000])\n",
      "    Grad:   tensor([-0.0032,  0.0178])\n",
      "Epoch 3012, Loss 2.928608\n",
      "    Params: tensor([  5.3492, -17.2001])\n",
      "    Grad:   tensor([-0.0032,  0.0178])\n",
      "Epoch 3013, Loss 2.928604\n",
      "    Params: tensor([  5.3493, -17.2003])\n",
      "    Grad:   tensor([-0.0031,  0.0178])\n",
      "Epoch 3014, Loss 2.928601\n",
      "    Params: tensor([  5.3493, -17.2005])\n",
      "    Grad:   tensor([-0.0031,  0.0177])\n",
      "Epoch 3015, Loss 2.928599\n",
      "    Params: tensor([  5.3493, -17.2007])\n",
      "    Grad:   tensor([-0.0031,  0.0177])\n",
      "Epoch 3016, Loss 2.928596\n",
      "    Params: tensor([  5.3494, -17.2008])\n",
      "    Grad:   tensor([-0.0031,  0.0177])\n",
      "Epoch 3017, Loss 2.928592\n",
      "    Params: tensor([  5.3494, -17.2010])\n",
      "    Grad:   tensor([-0.0031,  0.0177])\n",
      "Epoch 3018, Loss 2.928588\n",
      "    Params: tensor([  5.3494, -17.2012])\n",
      "    Grad:   tensor([-0.0031,  0.0176])\n",
      "Epoch 3019, Loss 2.928586\n",
      "    Params: tensor([  5.3495, -17.2014])\n",
      "    Grad:   tensor([-0.0031,  0.0176])\n",
      "Epoch 3020, Loss 2.928583\n",
      "    Params: tensor([  5.3495, -17.2015])\n",
      "    Grad:   tensor([-0.0031,  0.0176])\n",
      "Epoch 3021, Loss 2.928580\n",
      "    Params: tensor([  5.3495, -17.2017])\n",
      "    Grad:   tensor([-0.0031,  0.0175])\n",
      "Epoch 3022, Loss 2.928576\n",
      "    Params: tensor([  5.3495, -17.2019])\n",
      "    Grad:   tensor([-0.0031,  0.0175])\n",
      "Epoch 3023, Loss 2.928574\n",
      "    Params: tensor([  5.3496, -17.2021])\n",
      "    Grad:   tensor([-0.0031,  0.0175])\n",
      "Epoch 3024, Loss 2.928570\n",
      "    Params: tensor([  5.3496, -17.2022])\n",
      "    Grad:   tensor([-0.0031,  0.0175])\n",
      "Epoch 3025, Loss 2.928567\n",
      "    Params: tensor([  5.3496, -17.2024])\n",
      "    Grad:   tensor([-0.0031,  0.0174])\n",
      "Epoch 3026, Loss 2.928564\n",
      "    Params: tensor([  5.3497, -17.2026])\n",
      "    Grad:   tensor([-0.0031,  0.0174])\n",
      "Epoch 3027, Loss 2.928561\n",
      "    Params: tensor([  5.3497, -17.2028])\n",
      "    Grad:   tensor([-0.0030,  0.0174])\n",
      "Epoch 3028, Loss 2.928557\n",
      "    Params: tensor([  5.3497, -17.2029])\n",
      "    Grad:   tensor([-0.0031,  0.0173])\n",
      "Epoch 3029, Loss 2.928555\n",
      "    Params: tensor([  5.3498, -17.2031])\n",
      "    Grad:   tensor([-0.0031,  0.0173])\n",
      "Epoch 3030, Loss 2.928551\n",
      "    Params: tensor([  5.3498, -17.2033])\n",
      "    Grad:   tensor([-0.0031,  0.0173])\n",
      "Epoch 3031, Loss 2.928548\n",
      "    Params: tensor([  5.3498, -17.2035])\n",
      "    Grad:   tensor([-0.0031,  0.0172])\n",
      "Epoch 3032, Loss 2.928545\n",
      "    Params: tensor([  5.3498, -17.2036])\n",
      "    Grad:   tensor([-0.0030,  0.0172])\n",
      "Epoch 3033, Loss 2.928543\n",
      "    Params: tensor([  5.3499, -17.2038])\n",
      "    Grad:   tensor([-0.0030,  0.0172])\n",
      "Epoch 3034, Loss 2.928539\n",
      "    Params: tensor([  5.3499, -17.2040])\n",
      "    Grad:   tensor([-0.0030,  0.0172])\n",
      "Epoch 3035, Loss 2.928536\n",
      "    Params: tensor([  5.3499, -17.2041])\n",
      "    Grad:   tensor([-0.0030,  0.0171])\n",
      "Epoch 3036, Loss 2.928532\n",
      "    Params: tensor([  5.3500, -17.2043])\n",
      "    Grad:   tensor([-0.0030,  0.0171])\n",
      "Epoch 3037, Loss 2.928531\n",
      "    Params: tensor([  5.3500, -17.2045])\n",
      "    Grad:   tensor([-0.0030,  0.0171])\n",
      "Epoch 3038, Loss 2.928528\n",
      "    Params: tensor([  5.3500, -17.2047])\n",
      "    Grad:   tensor([-0.0030,  0.0170])\n",
      "Epoch 3039, Loss 2.928524\n",
      "    Params: tensor([  5.3501, -17.2048])\n",
      "    Grad:   tensor([-0.0030,  0.0170])\n",
      "Epoch 3040, Loss 2.928521\n",
      "    Params: tensor([  5.3501, -17.2050])\n",
      "    Grad:   tensor([-0.0030,  0.0170])\n",
      "Epoch 3041, Loss 2.928519\n",
      "    Params: tensor([  5.3501, -17.2052])\n",
      "    Grad:   tensor([-0.0030,  0.0170])\n",
      "Epoch 3042, Loss 2.928514\n",
      "    Params: tensor([  5.3502, -17.2053])\n",
      "    Grad:   tensor([-0.0030,  0.0169])\n",
      "Epoch 3043, Loss 2.928512\n",
      "    Params: tensor([  5.3502, -17.2055])\n",
      "    Grad:   tensor([-0.0030,  0.0169])\n",
      "Epoch 3044, Loss 2.928509\n",
      "    Params: tensor([  5.3502, -17.2057])\n",
      "    Grad:   tensor([-0.0030,  0.0169])\n",
      "Epoch 3045, Loss 2.928505\n",
      "    Params: tensor([  5.3502, -17.2058])\n",
      "    Grad:   tensor([-0.0030,  0.0168])\n",
      "Epoch 3046, Loss 2.928503\n",
      "    Params: tensor([  5.3503, -17.2060])\n",
      "    Grad:   tensor([-0.0030,  0.0168])\n",
      "Epoch 3047, Loss 2.928500\n",
      "    Params: tensor([  5.3503, -17.2062])\n",
      "    Grad:   tensor([-0.0030,  0.0168])\n",
      "Epoch 3048, Loss 2.928498\n",
      "    Params: tensor([  5.3503, -17.2063])\n",
      "    Grad:   tensor([-0.0030,  0.0168])\n",
      "Epoch 3049, Loss 2.928495\n",
      "    Params: tensor([  5.3504, -17.2065])\n",
      "    Grad:   tensor([-0.0030,  0.0167])\n",
      "Epoch 3050, Loss 2.928491\n",
      "    Params: tensor([  5.3504, -17.2067])\n",
      "    Grad:   tensor([-0.0030,  0.0167])\n",
      "Epoch 3051, Loss 2.928489\n",
      "    Params: tensor([  5.3504, -17.2068])\n",
      "    Grad:   tensor([-0.0030,  0.0167])\n",
      "Epoch 3052, Loss 2.928486\n",
      "    Params: tensor([  5.3504, -17.2070])\n",
      "    Grad:   tensor([-0.0029,  0.0166])\n",
      "Epoch 3053, Loss 2.928484\n",
      "    Params: tensor([  5.3505, -17.2072])\n",
      "    Grad:   tensor([-0.0029,  0.0166])\n",
      "Epoch 3054, Loss 2.928481\n",
      "    Params: tensor([  5.3505, -17.2073])\n",
      "    Grad:   tensor([-0.0029,  0.0166])\n",
      "Epoch 3055, Loss 2.928477\n",
      "    Params: tensor([  5.3505, -17.2075])\n",
      "    Grad:   tensor([-0.0029,  0.0165])\n",
      "Epoch 3056, Loss 2.928474\n",
      "    Params: tensor([  5.3506, -17.2077])\n",
      "    Grad:   tensor([-0.0029,  0.0165])\n",
      "Epoch 3057, Loss 2.928472\n",
      "    Params: tensor([  5.3506, -17.2078])\n",
      "    Grad:   tensor([-0.0029,  0.0165])\n",
      "Epoch 3058, Loss 2.928469\n",
      "    Params: tensor([  5.3506, -17.2080])\n",
      "    Grad:   tensor([-0.0029,  0.0165])\n",
      "Epoch 3059, Loss 2.928468\n",
      "    Params: tensor([  5.3507, -17.2082])\n",
      "    Grad:   tensor([-0.0029,  0.0164])\n",
      "Epoch 3060, Loss 2.928463\n",
      "    Params: tensor([  5.3507, -17.2083])\n",
      "    Grad:   tensor([-0.0029,  0.0164])\n",
      "Epoch 3061, Loss 2.928460\n",
      "    Params: tensor([  5.3507, -17.2085])\n",
      "    Grad:   tensor([-0.0029,  0.0164])\n",
      "Epoch 3062, Loss 2.928458\n",
      "    Params: tensor([  5.3507, -17.2087])\n",
      "    Grad:   tensor([-0.0029,  0.0164])\n",
      "Epoch 3063, Loss 2.928456\n",
      "    Params: tensor([  5.3508, -17.2088])\n",
      "    Grad:   tensor([-0.0029,  0.0163])\n",
      "Epoch 3064, Loss 2.928452\n",
      "    Params: tensor([  5.3508, -17.2090])\n",
      "    Grad:   tensor([-0.0029,  0.0163])\n",
      "Epoch 3065, Loss 2.928449\n",
      "    Params: tensor([  5.3508, -17.2091])\n",
      "    Grad:   tensor([-0.0029,  0.0163])\n",
      "Epoch 3066, Loss 2.928447\n",
      "    Params: tensor([  5.3509, -17.2093])\n",
      "    Grad:   tensor([-0.0029,  0.0162])\n",
      "Epoch 3067, Loss 2.928443\n",
      "    Params: tensor([  5.3509, -17.2095])\n",
      "    Grad:   tensor([-0.0029,  0.0162])\n",
      "Epoch 3068, Loss 2.928444\n",
      "    Params: tensor([  5.3509, -17.2096])\n",
      "    Grad:   tensor([-0.0029,  0.0162])\n",
      "Epoch 3069, Loss 2.928440\n",
      "    Params: tensor([  5.3509, -17.2098])\n",
      "    Grad:   tensor([-0.0029,  0.0162])\n",
      "Epoch 3070, Loss 2.928435\n",
      "    Params: tensor([  5.3510, -17.2100])\n",
      "    Grad:   tensor([-0.0029,  0.0161])\n",
      "Epoch 3071, Loss 2.928435\n",
      "    Params: tensor([  5.3510, -17.2101])\n",
      "    Grad:   tensor([-0.0029,  0.0161])\n",
      "Epoch 3072, Loss 2.928430\n",
      "    Params: tensor([  5.3510, -17.2103])\n",
      "    Grad:   tensor([-0.0028,  0.0161])\n",
      "Epoch 3073, Loss 2.928428\n",
      "    Params: tensor([  5.3511, -17.2104])\n",
      "    Grad:   tensor([-0.0028,  0.0161])\n",
      "Epoch 3074, Loss 2.928426\n",
      "    Params: tensor([  5.3511, -17.2106])\n",
      "    Grad:   tensor([-0.0028,  0.0160])\n",
      "Epoch 3075, Loss 2.928423\n",
      "    Params: tensor([  5.3511, -17.2108])\n",
      "    Grad:   tensor([-0.0028,  0.0160])\n",
      "Epoch 3076, Loss 2.928421\n",
      "    Params: tensor([  5.3511, -17.2109])\n",
      "    Grad:   tensor([-0.0028,  0.0160])\n",
      "Epoch 3077, Loss 2.928417\n",
      "    Params: tensor([  5.3512, -17.2111])\n",
      "    Grad:   tensor([-0.0028,  0.0159])\n",
      "Epoch 3078, Loss 2.928416\n",
      "    Params: tensor([  5.3512, -17.2112])\n",
      "    Grad:   tensor([-0.0028,  0.0159])\n",
      "Epoch 3079, Loss 2.928411\n",
      "    Params: tensor([  5.3512, -17.2114])\n",
      "    Grad:   tensor([-0.0028,  0.0159])\n",
      "Epoch 3080, Loss 2.928410\n",
      "    Params: tensor([  5.3512, -17.2116])\n",
      "    Grad:   tensor([-0.0028,  0.0159])\n",
      "Epoch 3081, Loss 2.928407\n",
      "    Params: tensor([  5.3513, -17.2117])\n",
      "    Grad:   tensor([-0.0028,  0.0158])\n",
      "Epoch 3082, Loss 2.928404\n",
      "    Params: tensor([  5.3513, -17.2119])\n",
      "    Grad:   tensor([-0.0028,  0.0158])\n",
      "Epoch 3083, Loss 2.928402\n",
      "    Params: tensor([  5.3513, -17.2120])\n",
      "    Grad:   tensor([-0.0028,  0.0158])\n",
      "Epoch 3084, Loss 2.928399\n",
      "    Params: tensor([  5.3514, -17.2122])\n",
      "    Grad:   tensor([-0.0028,  0.0158])\n",
      "Epoch 3085, Loss 2.928396\n",
      "    Params: tensor([  5.3514, -17.2123])\n",
      "    Grad:   tensor([-0.0028,  0.0157])\n",
      "Epoch 3086, Loss 2.928395\n",
      "    Params: tensor([  5.3514, -17.2125])\n",
      "    Grad:   tensor([-0.0028,  0.0157])\n",
      "Epoch 3087, Loss 2.928392\n",
      "    Params: tensor([  5.3514, -17.2127])\n",
      "    Grad:   tensor([-0.0027,  0.0157])\n",
      "Epoch 3088, Loss 2.928389\n",
      "    Params: tensor([  5.3515, -17.2128])\n",
      "    Grad:   tensor([-0.0027,  0.0157])\n",
      "Epoch 3089, Loss 2.928386\n",
      "    Params: tensor([  5.3515, -17.2130])\n",
      "    Grad:   tensor([-0.0027,  0.0156])\n",
      "Epoch 3090, Loss 2.928383\n",
      "    Params: tensor([  5.3515, -17.2131])\n",
      "    Grad:   tensor([-0.0028,  0.0156])\n",
      "Epoch 3091, Loss 2.928382\n",
      "    Params: tensor([  5.3516, -17.2133])\n",
      "    Grad:   tensor([-0.0028,  0.0156])\n",
      "Epoch 3092, Loss 2.928379\n",
      "    Params: tensor([  5.3516, -17.2134])\n",
      "    Grad:   tensor([-0.0027,  0.0155])\n",
      "Epoch 3093, Loss 2.928378\n",
      "    Params: tensor([  5.3516, -17.2136])\n",
      "    Grad:   tensor([-0.0027,  0.0155])\n",
      "Epoch 3094, Loss 2.928375\n",
      "    Params: tensor([  5.3516, -17.2137])\n",
      "    Grad:   tensor([-0.0027,  0.0155])\n",
      "Epoch 3095, Loss 2.928372\n",
      "    Params: tensor([  5.3517, -17.2139])\n",
      "    Grad:   tensor([-0.0027,  0.0155])\n",
      "Epoch 3096, Loss 2.928370\n",
      "    Params: tensor([  5.3517, -17.2141])\n",
      "    Grad:   tensor([-0.0027,  0.0154])\n",
      "Epoch 3097, Loss 2.928368\n",
      "    Params: tensor([  5.3517, -17.2142])\n",
      "    Grad:   tensor([-0.0027,  0.0154])\n",
      "Epoch 3098, Loss 2.928364\n",
      "    Params: tensor([  5.3517, -17.2144])\n",
      "    Grad:   tensor([-0.0027,  0.0154])\n",
      "Epoch 3099, Loss 2.928362\n",
      "    Params: tensor([  5.3518, -17.2145])\n",
      "    Grad:   tensor([-0.0027,  0.0154])\n",
      "Epoch 3100, Loss 2.928361\n",
      "    Params: tensor([  5.3518, -17.2147])\n",
      "    Grad:   tensor([-0.0027,  0.0153])\n",
      "Epoch 3101, Loss 2.928356\n",
      "    Params: tensor([  5.3518, -17.2148])\n",
      "    Grad:   tensor([-0.0027,  0.0153])\n",
      "Epoch 3102, Loss 2.928355\n",
      "    Params: tensor([  5.3519, -17.2150])\n",
      "    Grad:   tensor([-0.0027,  0.0153])\n",
      "Epoch 3103, Loss 2.928353\n",
      "    Params: tensor([  5.3519, -17.2151])\n",
      "    Grad:   tensor([-0.0027,  0.0153])\n",
      "Epoch 3104, Loss 2.928349\n",
      "    Params: tensor([  5.3519, -17.2153])\n",
      "    Grad:   tensor([-0.0027,  0.0152])\n",
      "Epoch 3105, Loss 2.928348\n",
      "    Params: tensor([  5.3519, -17.2154])\n",
      "    Grad:   tensor([-0.0027,  0.0152])\n",
      "Epoch 3106, Loss 2.928345\n",
      "    Params: tensor([  5.3520, -17.2156])\n",
      "    Grad:   tensor([-0.0027,  0.0152])\n",
      "Epoch 3107, Loss 2.928343\n",
      "    Params: tensor([  5.3520, -17.2157])\n",
      "    Grad:   tensor([-0.0027,  0.0152])\n",
      "Epoch 3108, Loss 2.928340\n",
      "    Params: tensor([  5.3520, -17.2159])\n",
      "    Grad:   tensor([-0.0027,  0.0151])\n",
      "Epoch 3109, Loss 2.928339\n",
      "    Params: tensor([  5.3520, -17.2160])\n",
      "    Grad:   tensor([-0.0027,  0.0151])\n",
      "Epoch 3110, Loss 2.928337\n",
      "    Params: tensor([  5.3521, -17.2162])\n",
      "    Grad:   tensor([-0.0027,  0.0151])\n",
      "Epoch 3111, Loss 2.928333\n",
      "    Params: tensor([  5.3521, -17.2163])\n",
      "    Grad:   tensor([-0.0027,  0.0151])\n",
      "Epoch 3112, Loss 2.928332\n",
      "    Params: tensor([  5.3521, -17.2165])\n",
      "    Grad:   tensor([-0.0027,  0.0150])\n",
      "Epoch 3113, Loss 2.928328\n",
      "    Params: tensor([  5.3521, -17.2166])\n",
      "    Grad:   tensor([-0.0026,  0.0150])\n",
      "Epoch 3114, Loss 2.928329\n",
      "    Params: tensor([  5.3522, -17.2168])\n",
      "    Grad:   tensor([-0.0027,  0.0150])\n",
      "Epoch 3115, Loss 2.928324\n",
      "    Params: tensor([  5.3522, -17.2169])\n",
      "    Grad:   tensor([-0.0026,  0.0149])\n",
      "Epoch 3116, Loss 2.928323\n",
      "    Params: tensor([  5.3522, -17.2171])\n",
      "    Grad:   tensor([-0.0026,  0.0149])\n",
      "Epoch 3117, Loss 2.928320\n",
      "    Params: tensor([  5.3523, -17.2172])\n",
      "    Grad:   tensor([-0.0026,  0.0149])\n",
      "Epoch 3118, Loss 2.928319\n",
      "    Params: tensor([  5.3523, -17.2174])\n",
      "    Grad:   tensor([-0.0026,  0.0149])\n",
      "Epoch 3119, Loss 2.928315\n",
      "    Params: tensor([  5.3523, -17.2175])\n",
      "    Grad:   tensor([-0.0026,  0.0148])\n",
      "Epoch 3120, Loss 2.928313\n",
      "    Params: tensor([  5.3523, -17.2177])\n",
      "    Grad:   tensor([-0.0026,  0.0148])\n",
      "Epoch 3121, Loss 2.928310\n",
      "    Params: tensor([  5.3524, -17.2178])\n",
      "    Grad:   tensor([-0.0026,  0.0148])\n",
      "Epoch 3122, Loss 2.928308\n",
      "    Params: tensor([  5.3524, -17.2180])\n",
      "    Grad:   tensor([-0.0026,  0.0148])\n",
      "Epoch 3123, Loss 2.928306\n",
      "    Params: tensor([  5.3524, -17.2181])\n",
      "    Grad:   tensor([-0.0026,  0.0147])\n",
      "Epoch 3124, Loss 2.928304\n",
      "    Params: tensor([  5.3524, -17.2183])\n",
      "    Grad:   tensor([-0.0026,  0.0147])\n",
      "Epoch 3125, Loss 2.928303\n",
      "    Params: tensor([  5.3525, -17.2184])\n",
      "    Grad:   tensor([-0.0026,  0.0147])\n",
      "Epoch 3126, Loss 2.928299\n",
      "    Params: tensor([  5.3525, -17.2186])\n",
      "    Grad:   tensor([-0.0026,  0.0147])\n",
      "Epoch 3127, Loss 2.928296\n",
      "    Params: tensor([  5.3525, -17.2187])\n",
      "    Grad:   tensor([-0.0026,  0.0146])\n",
      "Epoch 3128, Loss 2.928295\n",
      "    Params: tensor([  5.3525, -17.2189])\n",
      "    Grad:   tensor([-0.0026,  0.0146])\n",
      "Epoch 3129, Loss 2.928293\n",
      "    Params: tensor([  5.3526, -17.2190])\n",
      "    Grad:   tensor([-0.0026,  0.0146])\n",
      "Epoch 3130, Loss 2.928291\n",
      "    Params: tensor([  5.3526, -17.2192])\n",
      "    Grad:   tensor([-0.0026,  0.0146])\n",
      "Epoch 3131, Loss 2.928288\n",
      "    Params: tensor([  5.3526, -17.2193])\n",
      "    Grad:   tensor([-0.0026,  0.0145])\n",
      "Epoch 3132, Loss 2.928287\n",
      "    Params: tensor([  5.3526, -17.2194])\n",
      "    Grad:   tensor([-0.0026,  0.0145])\n",
      "Epoch 3133, Loss 2.928285\n",
      "    Params: tensor([  5.3527, -17.2196])\n",
      "    Grad:   tensor([-0.0025,  0.0145])\n",
      "Epoch 3134, Loss 2.928282\n",
      "    Params: tensor([  5.3527, -17.2197])\n",
      "    Grad:   tensor([-0.0026,  0.0145])\n",
      "Epoch 3135, Loss 2.928280\n",
      "    Params: tensor([  5.3527, -17.2199])\n",
      "    Grad:   tensor([-0.0026,  0.0144])\n",
      "Epoch 3136, Loss 2.928276\n",
      "    Params: tensor([  5.3527, -17.2200])\n",
      "    Grad:   tensor([-0.0025,  0.0144])\n",
      "Epoch 3137, Loss 2.928275\n",
      "    Params: tensor([  5.3528, -17.2202])\n",
      "    Grad:   tensor([-0.0026,  0.0144])\n",
      "Epoch 3138, Loss 2.928273\n",
      "    Params: tensor([  5.3528, -17.2203])\n",
      "    Grad:   tensor([-0.0025,  0.0144])\n",
      "Epoch 3139, Loss 2.928271\n",
      "    Params: tensor([  5.3528, -17.2205])\n",
      "    Grad:   tensor([-0.0025,  0.0144])\n",
      "Epoch 3140, Loss 2.928268\n",
      "    Params: tensor([  5.3528, -17.2206])\n",
      "    Grad:   tensor([-0.0025,  0.0143])\n",
      "Epoch 3141, Loss 2.928267\n",
      "    Params: tensor([  5.3529, -17.2207])\n",
      "    Grad:   tensor([-0.0025,  0.0143])\n",
      "Epoch 3142, Loss 2.928264\n",
      "    Params: tensor([  5.3529, -17.2209])\n",
      "    Grad:   tensor([-0.0025,  0.0143])\n",
      "Epoch 3143, Loss 2.928263\n",
      "    Params: tensor([  5.3529, -17.2210])\n",
      "    Grad:   tensor([-0.0025,  0.0143])\n",
      "Epoch 3144, Loss 2.928260\n",
      "    Params: tensor([  5.3529, -17.2212])\n",
      "    Grad:   tensor([-0.0025,  0.0142])\n",
      "Epoch 3145, Loss 2.928259\n",
      "    Params: tensor([  5.3530, -17.2213])\n",
      "    Grad:   tensor([-0.0025,  0.0142])\n",
      "Epoch 3146, Loss 2.928256\n",
      "    Params: tensor([  5.3530, -17.2214])\n",
      "    Grad:   tensor([-0.0025,  0.0142])\n",
      "Epoch 3147, Loss 2.928255\n",
      "    Params: tensor([  5.3530, -17.2216])\n",
      "    Grad:   tensor([-0.0025,  0.0142])\n",
      "Epoch 3148, Loss 2.928252\n",
      "    Params: tensor([  5.3530, -17.2217])\n",
      "    Grad:   tensor([-0.0025,  0.0141])\n",
      "Epoch 3149, Loss 2.928250\n",
      "    Params: tensor([  5.3531, -17.2219])\n",
      "    Grad:   tensor([-0.0025,  0.0141])\n",
      "Epoch 3150, Loss 2.928249\n",
      "    Params: tensor([  5.3531, -17.2220])\n",
      "    Grad:   tensor([-0.0025,  0.0141])\n",
      "Epoch 3151, Loss 2.928246\n",
      "    Params: tensor([  5.3531, -17.2222])\n",
      "    Grad:   tensor([-0.0025,  0.0141])\n",
      "Epoch 3152, Loss 2.928245\n",
      "    Params: tensor([  5.3531, -17.2223])\n",
      "    Grad:   tensor([-0.0025,  0.0140])\n",
      "Epoch 3153, Loss 2.928242\n",
      "    Params: tensor([  5.3532, -17.2224])\n",
      "    Grad:   tensor([-0.0025,  0.0140])\n",
      "Epoch 3154, Loss 2.928239\n",
      "    Params: tensor([  5.3532, -17.2226])\n",
      "    Grad:   tensor([-0.0025,  0.0140])\n",
      "Epoch 3155, Loss 2.928236\n",
      "    Params: tensor([  5.3532, -17.2227])\n",
      "    Grad:   tensor([-0.0025,  0.0140])\n",
      "Epoch 3156, Loss 2.928236\n",
      "    Params: tensor([  5.3532, -17.2229])\n",
      "    Grad:   tensor([-0.0024,  0.0139])\n",
      "Epoch 3157, Loss 2.928233\n",
      "    Params: tensor([  5.3533, -17.2230])\n",
      "    Grad:   tensor([-0.0025,  0.0139])\n",
      "Epoch 3158, Loss 2.928231\n",
      "    Params: tensor([  5.3533, -17.2231])\n",
      "    Grad:   tensor([-0.0024,  0.0139])\n",
      "Epoch 3159, Loss 2.928230\n",
      "    Params: tensor([  5.3533, -17.2233])\n",
      "    Grad:   tensor([-0.0025,  0.0139])\n",
      "Epoch 3160, Loss 2.928227\n",
      "    Params: tensor([  5.3533, -17.2234])\n",
      "    Grad:   tensor([-0.0024,  0.0138])\n",
      "Epoch 3161, Loss 2.928226\n",
      "    Params: tensor([  5.3534, -17.2235])\n",
      "    Grad:   tensor([-0.0025,  0.0138])\n",
      "Epoch 3162, Loss 2.928225\n",
      "    Params: tensor([  5.3534, -17.2237])\n",
      "    Grad:   tensor([-0.0024,  0.0138])\n",
      "Epoch 3163, Loss 2.928222\n",
      "    Params: tensor([  5.3534, -17.2238])\n",
      "    Grad:   tensor([-0.0024,  0.0138])\n",
      "Epoch 3164, Loss 2.928219\n",
      "    Params: tensor([  5.3534, -17.2240])\n",
      "    Grad:   tensor([-0.0024,  0.0138])\n",
      "Epoch 3165, Loss 2.928218\n",
      "    Params: tensor([  5.3535, -17.2241])\n",
      "    Grad:   tensor([-0.0024,  0.0137])\n",
      "Epoch 3166, Loss 2.928216\n",
      "    Params: tensor([  5.3535, -17.2242])\n",
      "    Grad:   tensor([-0.0024,  0.0137])\n",
      "Epoch 3167, Loss 2.928215\n",
      "    Params: tensor([  5.3535, -17.2244])\n",
      "    Grad:   tensor([-0.0024,  0.0137])\n",
      "Epoch 3168, Loss 2.928212\n",
      "    Params: tensor([  5.3535, -17.2245])\n",
      "    Grad:   tensor([-0.0024,  0.0137])\n",
      "Epoch 3169, Loss 2.928211\n",
      "    Params: tensor([  5.3536, -17.2246])\n",
      "    Grad:   tensor([-0.0024,  0.0136])\n",
      "Epoch 3170, Loss 2.928209\n",
      "    Params: tensor([  5.3536, -17.2248])\n",
      "    Grad:   tensor([-0.0024,  0.0136])\n",
      "Epoch 3171, Loss 2.928206\n",
      "    Params: tensor([  5.3536, -17.2249])\n",
      "    Grad:   tensor([-0.0024,  0.0136])\n",
      "Epoch 3172, Loss 2.928205\n",
      "    Params: tensor([  5.3536, -17.2250])\n",
      "    Grad:   tensor([-0.0024,  0.0136])\n",
      "Epoch 3173, Loss 2.928204\n",
      "    Params: tensor([  5.3537, -17.2252])\n",
      "    Grad:   tensor([-0.0024,  0.0135])\n",
      "Epoch 3174, Loss 2.928202\n",
      "    Params: tensor([  5.3537, -17.2253])\n",
      "    Grad:   tensor([-0.0024,  0.0135])\n",
      "Epoch 3175, Loss 2.928200\n",
      "    Params: tensor([  5.3537, -17.2255])\n",
      "    Grad:   tensor([-0.0024,  0.0135])\n",
      "Epoch 3176, Loss 2.928196\n",
      "    Params: tensor([  5.3537, -17.2256])\n",
      "    Grad:   tensor([-0.0024,  0.0135])\n",
      "Epoch 3177, Loss 2.928195\n",
      "    Params: tensor([  5.3538, -17.2257])\n",
      "    Grad:   tensor([-0.0024,  0.0134])\n",
      "Epoch 3178, Loss 2.928195\n",
      "    Params: tensor([  5.3538, -17.2259])\n",
      "    Grad:   tensor([-0.0024,  0.0134])\n",
      "Epoch 3179, Loss 2.928191\n",
      "    Params: tensor([  5.3538, -17.2260])\n",
      "    Grad:   tensor([-0.0024,  0.0134])\n",
      "Epoch 3180, Loss 2.928190\n",
      "    Params: tensor([  5.3538, -17.2261])\n",
      "    Grad:   tensor([-0.0024,  0.0134])\n",
      "Epoch 3181, Loss 2.928188\n",
      "    Params: tensor([  5.3538, -17.2263])\n",
      "    Grad:   tensor([-0.0023,  0.0134])\n",
      "Epoch 3182, Loss 2.928186\n",
      "    Params: tensor([  5.3539, -17.2264])\n",
      "    Grad:   tensor([-0.0023,  0.0133])\n",
      "Epoch 3183, Loss 2.928185\n",
      "    Params: tensor([  5.3539, -17.2265])\n",
      "    Grad:   tensor([-0.0024,  0.0133])\n",
      "Epoch 3184, Loss 2.928184\n",
      "    Params: tensor([  5.3539, -17.2267])\n",
      "    Grad:   tensor([-0.0023,  0.0133])\n",
      "Epoch 3185, Loss 2.928182\n",
      "    Params: tensor([  5.3539, -17.2268])\n",
      "    Grad:   tensor([-0.0024,  0.0133])\n",
      "Epoch 3186, Loss 2.928180\n",
      "    Params: tensor([  5.3540, -17.2269])\n",
      "    Grad:   tensor([-0.0024,  0.0132])\n",
      "Epoch 3187, Loss 2.928178\n",
      "    Params: tensor([  5.3540, -17.2271])\n",
      "    Grad:   tensor([-0.0023,  0.0132])\n",
      "Epoch 3188, Loss 2.928175\n",
      "    Params: tensor([  5.3540, -17.2272])\n",
      "    Grad:   tensor([-0.0023,  0.0132])\n",
      "Epoch 3189, Loss 2.928172\n",
      "    Params: tensor([  5.3540, -17.2273])\n",
      "    Grad:   tensor([-0.0023,  0.0132])\n",
      "Epoch 3190, Loss 2.928171\n",
      "    Params: tensor([  5.3541, -17.2275])\n",
      "    Grad:   tensor([-0.0023,  0.0132])\n",
      "Epoch 3191, Loss 2.928170\n",
      "    Params: tensor([  5.3541, -17.2276])\n",
      "    Grad:   tensor([-0.0023,  0.0131])\n",
      "Epoch 3192, Loss 2.928169\n",
      "    Params: tensor([  5.3541, -17.2277])\n",
      "    Grad:   tensor([-0.0023,  0.0131])\n",
      "Epoch 3193, Loss 2.928167\n",
      "    Params: tensor([  5.3541, -17.2278])\n",
      "    Grad:   tensor([-0.0023,  0.0131])\n",
      "Epoch 3194, Loss 2.928164\n",
      "    Params: tensor([  5.3542, -17.2280])\n",
      "    Grad:   tensor([-0.0023,  0.0131])\n",
      "Epoch 3195, Loss 2.928163\n",
      "    Params: tensor([  5.3542, -17.2281])\n",
      "    Grad:   tensor([-0.0023,  0.0130])\n",
      "Epoch 3196, Loss 2.928162\n",
      "    Params: tensor([  5.3542, -17.2282])\n",
      "    Grad:   tensor([-0.0023,  0.0130])\n",
      "Epoch 3197, Loss 2.928160\n",
      "    Params: tensor([  5.3542, -17.2284])\n",
      "    Grad:   tensor([-0.0023,  0.0130])\n",
      "Epoch 3198, Loss 2.928158\n",
      "    Params: tensor([  5.3542, -17.2285])\n",
      "    Grad:   tensor([-0.0023,  0.0130])\n",
      "Epoch 3199, Loss 2.928157\n",
      "    Params: tensor([  5.3543, -17.2286])\n",
      "    Grad:   tensor([-0.0023,  0.0130])\n",
      "Epoch 3200, Loss 2.928154\n",
      "    Params: tensor([  5.3543, -17.2288])\n",
      "    Grad:   tensor([-0.0023,  0.0129])\n",
      "Epoch 3201, Loss 2.928152\n",
      "    Params: tensor([  5.3543, -17.2289])\n",
      "    Grad:   tensor([-0.0023,  0.0129])\n",
      "Epoch 3202, Loss 2.928149\n",
      "    Params: tensor([  5.3543, -17.2290])\n",
      "    Grad:   tensor([-0.0023,  0.0129])\n",
      "Epoch 3203, Loss 2.928150\n",
      "    Params: tensor([  5.3544, -17.2291])\n",
      "    Grad:   tensor([-0.0023,  0.0129])\n",
      "Epoch 3204, Loss 2.928147\n",
      "    Params: tensor([  5.3544, -17.2293])\n",
      "    Grad:   tensor([-0.0022,  0.0129])\n",
      "Epoch 3205, Loss 2.928146\n",
      "    Params: tensor([  5.3544, -17.2294])\n",
      "    Grad:   tensor([-0.0023,  0.0128])\n",
      "Epoch 3206, Loss 2.928144\n",
      "    Params: tensor([  5.3544, -17.2295])\n",
      "    Grad:   tensor([-0.0023,  0.0128])\n",
      "Epoch 3207, Loss 2.928142\n",
      "    Params: tensor([  5.3544, -17.2297])\n",
      "    Grad:   tensor([-0.0023,  0.0128])\n",
      "Epoch 3208, Loss 2.928140\n",
      "    Params: tensor([  5.3545, -17.2298])\n",
      "    Grad:   tensor([-0.0022,  0.0128])\n",
      "Epoch 3209, Loss 2.928138\n",
      "    Params: tensor([  5.3545, -17.2299])\n",
      "    Grad:   tensor([-0.0022,  0.0127])\n",
      "Epoch 3210, Loss 2.928137\n",
      "    Params: tensor([  5.3545, -17.2300])\n",
      "    Grad:   tensor([-0.0023,  0.0127])\n",
      "Epoch 3211, Loss 2.928135\n",
      "    Params: tensor([  5.3545, -17.2302])\n",
      "    Grad:   tensor([-0.0023,  0.0127])\n",
      "Epoch 3212, Loss 2.928135\n",
      "    Params: tensor([  5.3546, -17.2303])\n",
      "    Grad:   tensor([-0.0023,  0.0127])\n",
      "Epoch 3213, Loss 2.928133\n",
      "    Params: tensor([  5.3546, -17.2304])\n",
      "    Grad:   tensor([-0.0022,  0.0127])\n",
      "Epoch 3214, Loss 2.928131\n",
      "    Params: tensor([  5.3546, -17.2305])\n",
      "    Grad:   tensor([-0.0022,  0.0126])\n",
      "Epoch 3215, Loss 2.928130\n",
      "    Params: tensor([  5.3546, -17.2307])\n",
      "    Grad:   tensor([-0.0022,  0.0126])\n",
      "Epoch 3216, Loss 2.928126\n",
      "    Params: tensor([  5.3546, -17.2308])\n",
      "    Grad:   tensor([-0.0022,  0.0126])\n",
      "Epoch 3217, Loss 2.928125\n",
      "    Params: tensor([  5.3547, -17.2309])\n",
      "    Grad:   tensor([-0.0022,  0.0126])\n",
      "Epoch 3218, Loss 2.928124\n",
      "    Params: tensor([  5.3547, -17.2310])\n",
      "    Grad:   tensor([-0.0022,  0.0125])\n",
      "Epoch 3219, Loss 2.928121\n",
      "    Params: tensor([  5.3547, -17.2312])\n",
      "    Grad:   tensor([-0.0022,  0.0125])\n",
      "Epoch 3220, Loss 2.928121\n",
      "    Params: tensor([  5.3547, -17.2313])\n",
      "    Grad:   tensor([-0.0022,  0.0125])\n",
      "Epoch 3221, Loss 2.928120\n",
      "    Params: tensor([  5.3548, -17.2314])\n",
      "    Grad:   tensor([-0.0022,  0.0125])\n",
      "Epoch 3222, Loss 2.928118\n",
      "    Params: tensor([  5.3548, -17.2315])\n",
      "    Grad:   tensor([-0.0022,  0.0125])\n",
      "Epoch 3223, Loss 2.928117\n",
      "    Params: tensor([  5.3548, -17.2317])\n",
      "    Grad:   tensor([-0.0022,  0.0124])\n",
      "Epoch 3224, Loss 2.928115\n",
      "    Params: tensor([  5.3548, -17.2318])\n",
      "    Grad:   tensor([-0.0022,  0.0124])\n",
      "Epoch 3225, Loss 2.928113\n",
      "    Params: tensor([  5.3548, -17.2319])\n",
      "    Grad:   tensor([-0.0022,  0.0124])\n",
      "Epoch 3226, Loss 2.928110\n",
      "    Params: tensor([  5.3549, -17.2320])\n",
      "    Grad:   tensor([-0.0022,  0.0124])\n",
      "Epoch 3227, Loss 2.928109\n",
      "    Params: tensor([  5.3549, -17.2322])\n",
      "    Grad:   tensor([-0.0022,  0.0124])\n",
      "Epoch 3228, Loss 2.928108\n",
      "    Params: tensor([  5.3549, -17.2323])\n",
      "    Grad:   tensor([-0.0022,  0.0123])\n",
      "Epoch 3229, Loss 2.928105\n",
      "    Params: tensor([  5.3549, -17.2324])\n",
      "    Grad:   tensor([-0.0022,  0.0123])\n",
      "Epoch 3230, Loss 2.928105\n",
      "    Params: tensor([  5.3550, -17.2325])\n",
      "    Grad:   tensor([-0.0022,  0.0123])\n",
      "Epoch 3231, Loss 2.928104\n",
      "    Params: tensor([  5.3550, -17.2327])\n",
      "    Grad:   tensor([-0.0022,  0.0123])\n",
      "Epoch 3232, Loss 2.928102\n",
      "    Params: tensor([  5.3550, -17.2328])\n",
      "    Grad:   tensor([-0.0021,  0.0123])\n",
      "Epoch 3233, Loss 2.928101\n",
      "    Params: tensor([  5.3550, -17.2329])\n",
      "    Grad:   tensor([-0.0022,  0.0122])\n",
      "Epoch 3234, Loss 2.928098\n",
      "    Params: tensor([  5.3550, -17.2330])\n",
      "    Grad:   tensor([-0.0022,  0.0122])\n",
      "Epoch 3235, Loss 2.928097\n",
      "    Params: tensor([  5.3551, -17.2331])\n",
      "    Grad:   tensor([-0.0022,  0.0122])\n",
      "Epoch 3236, Loss 2.928095\n",
      "    Params: tensor([  5.3551, -17.2333])\n",
      "    Grad:   tensor([-0.0022,  0.0122])\n",
      "Epoch 3237, Loss 2.928094\n",
      "    Params: tensor([  5.3551, -17.2334])\n",
      "    Grad:   tensor([-0.0022,  0.0121])\n",
      "Epoch 3238, Loss 2.928093\n",
      "    Params: tensor([  5.3551, -17.2335])\n",
      "    Grad:   tensor([-0.0022,  0.0121])\n",
      "Epoch 3239, Loss 2.928091\n",
      "    Params: tensor([  5.3551, -17.2336])\n",
      "    Grad:   tensor([-0.0022,  0.0121])\n",
      "Epoch 3240, Loss 2.928090\n",
      "    Params: tensor([  5.3552, -17.2338])\n",
      "    Grad:   tensor([-0.0021,  0.0121])\n",
      "Epoch 3241, Loss 2.928088\n",
      "    Params: tensor([  5.3552, -17.2339])\n",
      "    Grad:   tensor([-0.0021,  0.0121])\n",
      "Epoch 3242, Loss 2.928086\n",
      "    Params: tensor([  5.3552, -17.2340])\n",
      "    Grad:   tensor([-0.0021,  0.0120])\n",
      "Epoch 3243, Loss 2.928085\n",
      "    Params: tensor([  5.3552, -17.2341])\n",
      "    Grad:   tensor([-0.0021,  0.0120])\n",
      "Epoch 3244, Loss 2.928084\n",
      "    Params: tensor([  5.3553, -17.2342])\n",
      "    Grad:   tensor([-0.0021,  0.0120])\n",
      "Epoch 3245, Loss 2.928082\n",
      "    Params: tensor([  5.3553, -17.2344])\n",
      "    Grad:   tensor([-0.0021,  0.0120])\n",
      "Epoch 3246, Loss 2.928080\n",
      "    Params: tensor([  5.3553, -17.2345])\n",
      "    Grad:   tensor([-0.0021,  0.0120])\n",
      "Epoch 3247, Loss 2.928079\n",
      "    Params: tensor([  5.3553, -17.2346])\n",
      "    Grad:   tensor([-0.0021,  0.0119])\n",
      "Epoch 3248, Loss 2.928076\n",
      "    Params: tensor([  5.3553, -17.2347])\n",
      "    Grad:   tensor([-0.0021,  0.0119])\n",
      "Epoch 3249, Loss 2.928077\n",
      "    Params: tensor([  5.3554, -17.2348])\n",
      "    Grad:   tensor([-0.0021,  0.0119])\n",
      "Epoch 3250, Loss 2.928075\n",
      "    Params: tensor([  5.3554, -17.2350])\n",
      "    Grad:   tensor([-0.0021,  0.0119])\n",
      "Epoch 3251, Loss 2.928072\n",
      "    Params: tensor([  5.3554, -17.2351])\n",
      "    Grad:   tensor([-0.0021,  0.0119])\n",
      "Epoch 3252, Loss 2.928072\n",
      "    Params: tensor([  5.3554, -17.2352])\n",
      "    Grad:   tensor([-0.0021,  0.0118])\n",
      "Epoch 3253, Loss 2.928071\n",
      "    Params: tensor([  5.3554, -17.2353])\n",
      "    Grad:   tensor([-0.0021,  0.0118])\n",
      "Epoch 3254, Loss 2.928068\n",
      "    Params: tensor([  5.3555, -17.2354])\n",
      "    Grad:   tensor([-0.0021,  0.0118])\n",
      "Epoch 3255, Loss 2.928069\n",
      "    Params: tensor([  5.3555, -17.2355])\n",
      "    Grad:   tensor([-0.0021,  0.0118])\n",
      "Epoch 3256, Loss 2.928066\n",
      "    Params: tensor([  5.3555, -17.2357])\n",
      "    Grad:   tensor([-0.0021,  0.0118])\n",
      "Epoch 3257, Loss 2.928065\n",
      "    Params: tensor([  5.3555, -17.2358])\n",
      "    Grad:   tensor([-0.0021,  0.0117])\n",
      "Epoch 3258, Loss 2.928064\n",
      "    Params: tensor([  5.3555, -17.2359])\n",
      "    Grad:   tensor([-0.0021,  0.0117])\n",
      "Epoch 3259, Loss 2.928061\n",
      "    Params: tensor([  5.3556, -17.2360])\n",
      "    Grad:   tensor([-0.0021,  0.0117])\n",
      "Epoch 3260, Loss 2.928060\n",
      "    Params: tensor([  5.3556, -17.2361])\n",
      "    Grad:   tensor([-0.0021,  0.0117])\n",
      "Epoch 3261, Loss 2.928057\n",
      "    Params: tensor([  5.3556, -17.2362])\n",
      "    Grad:   tensor([-0.0021,  0.0117])\n",
      "Epoch 3262, Loss 2.928058\n",
      "    Params: tensor([  5.3556, -17.2364])\n",
      "    Grad:   tensor([-0.0021,  0.0116])\n",
      "Epoch 3263, Loss 2.928056\n",
      "    Params: tensor([  5.3557, -17.2365])\n",
      "    Grad:   tensor([-0.0021,  0.0116])\n",
      "Epoch 3264, Loss 2.928055\n",
      "    Params: tensor([  5.3557, -17.2366])\n",
      "    Grad:   tensor([-0.0021,  0.0116])\n",
      "Epoch 3265, Loss 2.928052\n",
      "    Params: tensor([  5.3557, -17.2367])\n",
      "    Grad:   tensor([-0.0021,  0.0116])\n",
      "Epoch 3266, Loss 2.928053\n",
      "    Params: tensor([  5.3557, -17.2368])\n",
      "    Grad:   tensor([-0.0021,  0.0116])\n",
      "Epoch 3267, Loss 2.928051\n",
      "    Params: tensor([  5.3557, -17.2369])\n",
      "    Grad:   tensor([-0.0021,  0.0115])\n",
      "Epoch 3268, Loss 2.928050\n",
      "    Params: tensor([  5.3558, -17.2371])\n",
      "    Grad:   tensor([-0.0021,  0.0115])\n",
      "Epoch 3269, Loss 2.928047\n",
      "    Params: tensor([  5.3558, -17.2372])\n",
      "    Grad:   tensor([-0.0020,  0.0115])\n",
      "Epoch 3270, Loss 2.928046\n",
      "    Params: tensor([  5.3558, -17.2373])\n",
      "    Grad:   tensor([-0.0020,  0.0115])\n",
      "Epoch 3271, Loss 2.928046\n",
      "    Params: tensor([  5.3558, -17.2374])\n",
      "    Grad:   tensor([-0.0020,  0.0115])\n",
      "Epoch 3272, Loss 2.928044\n",
      "    Params: tensor([  5.3558, -17.2375])\n",
      "    Grad:   tensor([-0.0020,  0.0115])\n",
      "Epoch 3273, Loss 2.928042\n",
      "    Params: tensor([  5.3559, -17.2376])\n",
      "    Grad:   tensor([-0.0020,  0.0114])\n",
      "Epoch 3274, Loss 2.928040\n",
      "    Params: tensor([  5.3559, -17.2377])\n",
      "    Grad:   tensor([-0.0020,  0.0114])\n",
      "Epoch 3275, Loss 2.928040\n",
      "    Params: tensor([  5.3559, -17.2379])\n",
      "    Grad:   tensor([-0.0020,  0.0114])\n",
      "Epoch 3276, Loss 2.928036\n",
      "    Params: tensor([  5.3559, -17.2380])\n",
      "    Grad:   tensor([-0.0020,  0.0114])\n",
      "Epoch 3277, Loss 2.928036\n",
      "    Params: tensor([  5.3559, -17.2381])\n",
      "    Grad:   tensor([-0.0020,  0.0113])\n",
      "Epoch 3278, Loss 2.928037\n",
      "    Params: tensor([  5.3560, -17.2382])\n",
      "    Grad:   tensor([-0.0020,  0.0113])\n",
      "Epoch 3279, Loss 2.928034\n",
      "    Params: tensor([  5.3560, -17.2383])\n",
      "    Grad:   tensor([-0.0020,  0.0113])\n",
      "Epoch 3280, Loss 2.928034\n",
      "    Params: tensor([  5.3560, -17.2384])\n",
      "    Grad:   tensor([-0.0020,  0.0113])\n",
      "Epoch 3281, Loss 2.928031\n",
      "    Params: tensor([  5.3560, -17.2385])\n",
      "    Grad:   tensor([-0.0020,  0.0113])\n",
      "Epoch 3282, Loss 2.928032\n",
      "    Params: tensor([  5.3560, -17.2386])\n",
      "    Grad:   tensor([-0.0020,  0.0113])\n",
      "Epoch 3283, Loss 2.928028\n",
      "    Params: tensor([  5.3561, -17.2388])\n",
      "    Grad:   tensor([-0.0020,  0.0112])\n",
      "Epoch 3284, Loss 2.928027\n",
      "    Params: tensor([  5.3561, -17.2389])\n",
      "    Grad:   tensor([-0.0020,  0.0112])\n",
      "Epoch 3285, Loss 2.928026\n",
      "    Params: tensor([  5.3561, -17.2390])\n",
      "    Grad:   tensor([-0.0020,  0.0112])\n",
      "Epoch 3286, Loss 2.928025\n",
      "    Params: tensor([  5.3561, -17.2391])\n",
      "    Grad:   tensor([-0.0020,  0.0112])\n",
      "Epoch 3287, Loss 2.928024\n",
      "    Params: tensor([  5.3561, -17.2392])\n",
      "    Grad:   tensor([-0.0020,  0.0112])\n",
      "Epoch 3288, Loss 2.928022\n",
      "    Params: tensor([  5.3562, -17.2393])\n",
      "    Grad:   tensor([-0.0020,  0.0111])\n",
      "Epoch 3289, Loss 2.928023\n",
      "    Params: tensor([  5.3562, -17.2394])\n",
      "    Grad:   tensor([-0.0020,  0.0111])\n",
      "Epoch 3290, Loss 2.928021\n",
      "    Params: tensor([  5.3562, -17.2395])\n",
      "    Grad:   tensor([-0.0020,  0.0111])\n",
      "Epoch 3291, Loss 2.928019\n",
      "    Params: tensor([  5.3562, -17.2397])\n",
      "    Grad:   tensor([-0.0020,  0.0111])\n",
      "Epoch 3292, Loss 2.928018\n",
      "    Params: tensor([  5.3562, -17.2398])\n",
      "    Grad:   tensor([-0.0020,  0.0111])\n",
      "Epoch 3293, Loss 2.928017\n",
      "    Params: tensor([  5.3563, -17.2399])\n",
      "    Grad:   tensor([-0.0020,  0.0110])\n",
      "Epoch 3294, Loss 2.928015\n",
      "    Params: tensor([  5.3563, -17.2400])\n",
      "    Grad:   tensor([-0.0020,  0.0110])\n",
      "Epoch 3295, Loss 2.928013\n",
      "    Params: tensor([  5.3563, -17.2401])\n",
      "    Grad:   tensor([-0.0020,  0.0110])\n",
      "Epoch 3296, Loss 2.928013\n",
      "    Params: tensor([  5.3563, -17.2402])\n",
      "    Grad:   tensor([-0.0019,  0.0110])\n",
      "Epoch 3297, Loss 2.928011\n",
      "    Params: tensor([  5.3563, -17.2403])\n",
      "    Grad:   tensor([-0.0019,  0.0110])\n",
      "Epoch 3298, Loss 2.928009\n",
      "    Params: tensor([  5.3563, -17.2404])\n",
      "    Grad:   tensor([-0.0019,  0.0110])\n",
      "Epoch 3299, Loss 2.928008\n",
      "    Params: tensor([  5.3564, -17.2405])\n",
      "    Grad:   tensor([-0.0019,  0.0109])\n",
      "Epoch 3300, Loss 2.928006\n",
      "    Params: tensor([  5.3564, -17.2406])\n",
      "    Grad:   tensor([-0.0019,  0.0109])\n",
      "Epoch 3301, Loss 2.928007\n",
      "    Params: tensor([  5.3564, -17.2407])\n",
      "    Grad:   tensor([-0.0019,  0.0109])\n",
      "Epoch 3302, Loss 2.928007\n",
      "    Params: tensor([  5.3564, -17.2409])\n",
      "    Grad:   tensor([-0.0019,  0.0109])\n",
      "Epoch 3303, Loss 2.928004\n",
      "    Params: tensor([  5.3564, -17.2410])\n",
      "    Grad:   tensor([-0.0019,  0.0109])\n",
      "Epoch 3304, Loss 2.928002\n",
      "    Params: tensor([  5.3565, -17.2411])\n",
      "    Grad:   tensor([-0.0019,  0.0108])\n",
      "Epoch 3305, Loss 2.928002\n",
      "    Params: tensor([  5.3565, -17.2412])\n",
      "    Grad:   tensor([-0.0019,  0.0108])\n",
      "Epoch 3306, Loss 2.928000\n",
      "    Params: tensor([  5.3565, -17.2413])\n",
      "    Grad:   tensor([-0.0019,  0.0108])\n",
      "Epoch 3307, Loss 2.928000\n",
      "    Params: tensor([  5.3565, -17.2414])\n",
      "    Grad:   tensor([-0.0019,  0.0108])\n",
      "Epoch 3308, Loss 2.927998\n",
      "    Params: tensor([  5.3565, -17.2415])\n",
      "    Grad:   tensor([-0.0019,  0.0108])\n",
      "Epoch 3309, Loss 2.927995\n",
      "    Params: tensor([  5.3566, -17.2416])\n",
      "    Grad:   tensor([-0.0019,  0.0107])\n",
      "Epoch 3310, Loss 2.927995\n",
      "    Params: tensor([  5.3566, -17.2417])\n",
      "    Grad:   tensor([-0.0019,  0.0107])\n",
      "Epoch 3311, Loss 2.927994\n",
      "    Params: tensor([  5.3566, -17.2418])\n",
      "    Grad:   tensor([-0.0019,  0.0107])\n",
      "Epoch 3312, Loss 2.927994\n",
      "    Params: tensor([  5.3566, -17.2419])\n",
      "    Grad:   tensor([-0.0019,  0.0107])\n",
      "Epoch 3313, Loss 2.927991\n",
      "    Params: tensor([  5.3566, -17.2420])\n",
      "    Grad:   tensor([-0.0019,  0.0107])\n",
      "Epoch 3314, Loss 2.927991\n",
      "    Params: tensor([  5.3567, -17.2421])\n",
      "    Grad:   tensor([-0.0019,  0.0107])\n",
      "Epoch 3315, Loss 2.927990\n",
      "    Params: tensor([  5.3567, -17.2423])\n",
      "    Grad:   tensor([-0.0019,  0.0106])\n",
      "Epoch 3316, Loss 2.927989\n",
      "    Params: tensor([  5.3567, -17.2424])\n",
      "    Grad:   tensor([-0.0019,  0.0106])\n",
      "Epoch 3317, Loss 2.927988\n",
      "    Params: tensor([  5.3567, -17.2425])\n",
      "    Grad:   tensor([-0.0019,  0.0106])\n",
      "Epoch 3318, Loss 2.927986\n",
      "    Params: tensor([  5.3567, -17.2426])\n",
      "    Grad:   tensor([-0.0019,  0.0106])\n",
      "Epoch 3319, Loss 2.927985\n",
      "    Params: tensor([  5.3567, -17.2427])\n",
      "    Grad:   tensor([-0.0019,  0.0106])\n",
      "Epoch 3320, Loss 2.927983\n",
      "    Params: tensor([  5.3568, -17.2428])\n",
      "    Grad:   tensor([-0.0018,  0.0106])\n",
      "Epoch 3321, Loss 2.927983\n",
      "    Params: tensor([  5.3568, -17.2429])\n",
      "    Grad:   tensor([-0.0018,  0.0105])\n",
      "Epoch 3322, Loss 2.927981\n",
      "    Params: tensor([  5.3568, -17.2430])\n",
      "    Grad:   tensor([-0.0018,  0.0105])\n",
      "Epoch 3323, Loss 2.927980\n",
      "    Params: tensor([  5.3568, -17.2431])\n",
      "    Grad:   tensor([-0.0018,  0.0105])\n",
      "Epoch 3324, Loss 2.927979\n",
      "    Params: tensor([  5.3568, -17.2432])\n",
      "    Grad:   tensor([-0.0018,  0.0105])\n",
      "Epoch 3325, Loss 2.927979\n",
      "    Params: tensor([  5.3569, -17.2433])\n",
      "    Grad:   tensor([-0.0018,  0.0105])\n",
      "Epoch 3326, Loss 2.927977\n",
      "    Params: tensor([  5.3569, -17.2434])\n",
      "    Grad:   tensor([-0.0018,  0.0104])\n",
      "Epoch 3327, Loss 2.927975\n",
      "    Params: tensor([  5.3569, -17.2435])\n",
      "    Grad:   tensor([-0.0019,  0.0104])\n",
      "Epoch 3328, Loss 2.927973\n",
      "    Params: tensor([  5.3569, -17.2436])\n",
      "    Grad:   tensor([-0.0018,  0.0104])\n",
      "Epoch 3329, Loss 2.927974\n",
      "    Params: tensor([  5.3569, -17.2437])\n",
      "    Grad:   tensor([-0.0018,  0.0104])\n",
      "Epoch 3330, Loss 2.927974\n",
      "    Params: tensor([  5.3570, -17.2438])\n",
      "    Grad:   tensor([-0.0018,  0.0104])\n",
      "Epoch 3331, Loss 2.927972\n",
      "    Params: tensor([  5.3570, -17.2439])\n",
      "    Grad:   tensor([-0.0018,  0.0104])\n",
      "Epoch 3332, Loss 2.927972\n",
      "    Params: tensor([  5.3570, -17.2440])\n",
      "    Grad:   tensor([-0.0018,  0.0103])\n",
      "Epoch 3333, Loss 2.927969\n",
      "    Params: tensor([  5.3570, -17.2441])\n",
      "    Grad:   tensor([-0.0018,  0.0103])\n",
      "Epoch 3334, Loss 2.927969\n",
      "    Params: tensor([  5.3570, -17.2442])\n",
      "    Grad:   tensor([-0.0018,  0.0103])\n",
      "Epoch 3335, Loss 2.927967\n",
      "    Params: tensor([  5.3570, -17.2443])\n",
      "    Grad:   tensor([-0.0018,  0.0103])\n",
      "Epoch 3336, Loss 2.927967\n",
      "    Params: tensor([  5.3571, -17.2444])\n",
      "    Grad:   tensor([-0.0018,  0.0103])\n",
      "Epoch 3337, Loss 2.927963\n",
      "    Params: tensor([  5.3571, -17.2446])\n",
      "    Grad:   tensor([-0.0018,  0.0102])\n",
      "Epoch 3338, Loss 2.927963\n",
      "    Params: tensor([  5.3571, -17.2447])\n",
      "    Grad:   tensor([-0.0018,  0.0102])\n",
      "Epoch 3339, Loss 2.927962\n",
      "    Params: tensor([  5.3571, -17.2448])\n",
      "    Grad:   tensor([-0.0018,  0.0102])\n",
      "Epoch 3340, Loss 2.927962\n",
      "    Params: tensor([  5.3571, -17.2449])\n",
      "    Grad:   tensor([-0.0018,  0.0102])\n",
      "Epoch 3341, Loss 2.927960\n",
      "    Params: tensor([  5.3572, -17.2450])\n",
      "    Grad:   tensor([-0.0018,  0.0102])\n",
      "Epoch 3342, Loss 2.927960\n",
      "    Params: tensor([  5.3572, -17.2451])\n",
      "    Grad:   tensor([-0.0018,  0.0102])\n",
      "Epoch 3343, Loss 2.927959\n",
      "    Params: tensor([  5.3572, -17.2452])\n",
      "    Grad:   tensor([-0.0018,  0.0101])\n",
      "Epoch 3344, Loss 2.927958\n",
      "    Params: tensor([  5.3572, -17.2453])\n",
      "    Grad:   tensor([-0.0018,  0.0101])\n",
      "Epoch 3345, Loss 2.927956\n",
      "    Params: tensor([  5.3572, -17.2454])\n",
      "    Grad:   tensor([-0.0018,  0.0101])\n",
      "Epoch 3346, Loss 2.927956\n",
      "    Params: tensor([  5.3572, -17.2455])\n",
      "    Grad:   tensor([-0.0018,  0.0101])\n",
      "Epoch 3347, Loss 2.927955\n",
      "    Params: tensor([  5.3573, -17.2456])\n",
      "    Grad:   tensor([-0.0018,  0.0101])\n",
      "Epoch 3348, Loss 2.927953\n",
      "    Params: tensor([  5.3573, -17.2457])\n",
      "    Grad:   tensor([-0.0018,  0.0101])\n",
      "Epoch 3349, Loss 2.927953\n",
      "    Params: tensor([  5.3573, -17.2458])\n",
      "    Grad:   tensor([-0.0018,  0.0100])\n",
      "Epoch 3350, Loss 2.927951\n",
      "    Params: tensor([  5.3573, -17.2459])\n",
      "    Grad:   tensor([-0.0018,  0.0100])\n",
      "Epoch 3351, Loss 2.927950\n",
      "    Params: tensor([  5.3573, -17.2460])\n",
      "    Grad:   tensor([-0.0018,  0.0100])\n",
      "Epoch 3352, Loss 2.927948\n",
      "    Params: tensor([  5.3573, -17.2461])\n",
      "    Grad:   tensor([-0.0018,  0.0100])\n",
      "Epoch 3353, Loss 2.927947\n",
      "    Params: tensor([  5.3574, -17.2462])\n",
      "    Grad:   tensor([-0.0017,  0.0100])\n",
      "Epoch 3354, Loss 2.927948\n",
      "    Params: tensor([  5.3574, -17.2463])\n",
      "    Grad:   tensor([-0.0018,  0.0100])\n",
      "Epoch 3355, Loss 2.927945\n",
      "    Params: tensor([  5.3574, -17.2464])\n",
      "    Grad:   tensor([-0.0017,  0.0099])\n",
      "Epoch 3356, Loss 2.927944\n",
      "    Params: tensor([  5.3574, -17.2465])\n",
      "    Grad:   tensor([-0.0017,  0.0099])\n",
      "Epoch 3357, Loss 2.927943\n",
      "    Params: tensor([  5.3574, -17.2466])\n",
      "    Grad:   tensor([-0.0018,  0.0099])\n",
      "Epoch 3358, Loss 2.927944\n",
      "    Params: tensor([  5.3575, -17.2467])\n",
      "    Grad:   tensor([-0.0017,  0.0099])\n",
      "Epoch 3359, Loss 2.927942\n",
      "    Params: tensor([  5.3575, -17.2468])\n",
      "    Grad:   tensor([-0.0017,  0.0099])\n",
      "Epoch 3360, Loss 2.927941\n",
      "    Params: tensor([  5.3575, -17.2469])\n",
      "    Grad:   tensor([-0.0018,  0.0099])\n",
      "Epoch 3361, Loss 2.927940\n",
      "    Params: tensor([  5.3575, -17.2470])\n",
      "    Grad:   tensor([-0.0017,  0.0098])\n",
      "Epoch 3362, Loss 2.927938\n",
      "    Params: tensor([  5.3575, -17.2471])\n",
      "    Grad:   tensor([-0.0017,  0.0098])\n",
      "Epoch 3363, Loss 2.927938\n",
      "    Params: tensor([  5.3575, -17.2472])\n",
      "    Grad:   tensor([-0.0018,  0.0098])\n",
      "Epoch 3364, Loss 2.927936\n",
      "    Params: tensor([  5.3576, -17.2473])\n",
      "    Grad:   tensor([-0.0017,  0.0098])\n",
      "Epoch 3365, Loss 2.927936\n",
      "    Params: tensor([  5.3576, -17.2474])\n",
      "    Grad:   tensor([-0.0017,  0.0098])\n",
      "Epoch 3366, Loss 2.927937\n",
      "    Params: tensor([  5.3576, -17.2474])\n",
      "    Grad:   tensor([-0.0017,  0.0098])\n",
      "Epoch 3367, Loss 2.927934\n",
      "    Params: tensor([  5.3576, -17.2475])\n",
      "    Grad:   tensor([-0.0017,  0.0097])\n",
      "Epoch 3368, Loss 2.927933\n",
      "    Params: tensor([  5.3576, -17.2476])\n",
      "    Grad:   tensor([-0.0017,  0.0097])\n",
      "Epoch 3369, Loss 2.927932\n",
      "    Params: tensor([  5.3576, -17.2477])\n",
      "    Grad:   tensor([-0.0017,  0.0097])\n",
      "Epoch 3370, Loss 2.927930\n",
      "    Params: tensor([  5.3577, -17.2478])\n",
      "    Grad:   tensor([-0.0017,  0.0097])\n",
      "Epoch 3371, Loss 2.927928\n",
      "    Params: tensor([  5.3577, -17.2479])\n",
      "    Grad:   tensor([-0.0017,  0.0097])\n",
      "Epoch 3372, Loss 2.927931\n",
      "    Params: tensor([  5.3577, -17.2480])\n",
      "    Grad:   tensor([-0.0017,  0.0097])\n",
      "Epoch 3373, Loss 2.927929\n",
      "    Params: tensor([  5.3577, -17.2481])\n",
      "    Grad:   tensor([-0.0017,  0.0096])\n",
      "Epoch 3374, Loss 2.927927\n",
      "    Params: tensor([  5.3577, -17.2482])\n",
      "    Grad:   tensor([-0.0017,  0.0096])\n",
      "Epoch 3375, Loss 2.927926\n",
      "    Params: tensor([  5.3577, -17.2483])\n",
      "    Grad:   tensor([-0.0017,  0.0096])\n",
      "Epoch 3376, Loss 2.927925\n",
      "    Params: tensor([  5.3578, -17.2484])\n",
      "    Grad:   tensor([-0.0017,  0.0096])\n",
      "Epoch 3377, Loss 2.927924\n",
      "    Params: tensor([  5.3578, -17.2485])\n",
      "    Grad:   tensor([-0.0017,  0.0096])\n",
      "Epoch 3378, Loss 2.927923\n",
      "    Params: tensor([  5.3578, -17.2486])\n",
      "    Grad:   tensor([-0.0017,  0.0096])\n",
      "Epoch 3379, Loss 2.927924\n",
      "    Params: tensor([  5.3578, -17.2487])\n",
      "    Grad:   tensor([-0.0017,  0.0095])\n",
      "Epoch 3380, Loss 2.927922\n",
      "    Params: tensor([  5.3578, -17.2488])\n",
      "    Grad:   tensor([-0.0017,  0.0095])\n",
      "Epoch 3381, Loss 2.927922\n",
      "    Params: tensor([  5.3578, -17.2489])\n",
      "    Grad:   tensor([-0.0017,  0.0095])\n",
      "Epoch 3382, Loss 2.927920\n",
      "    Params: tensor([  5.3579, -17.2490])\n",
      "    Grad:   tensor([-0.0017,  0.0095])\n",
      "Epoch 3383, Loss 2.927918\n",
      "    Params: tensor([  5.3579, -17.2491])\n",
      "    Grad:   tensor([-0.0017,  0.0095])\n",
      "Epoch 3384, Loss 2.927917\n",
      "    Params: tensor([  5.3579, -17.2492])\n",
      "    Grad:   tensor([-0.0017,  0.0095])\n",
      "Epoch 3385, Loss 2.927917\n",
      "    Params: tensor([  5.3579, -17.2493])\n",
      "    Grad:   tensor([-0.0017,  0.0094])\n",
      "Epoch 3386, Loss 2.927915\n",
      "    Params: tensor([  5.3579, -17.2494])\n",
      "    Grad:   tensor([-0.0017,  0.0094])\n",
      "Epoch 3387, Loss 2.927915\n",
      "    Params: tensor([  5.3579, -17.2495])\n",
      "    Grad:   tensor([-0.0017,  0.0094])\n",
      "Epoch 3388, Loss 2.927914\n",
      "    Params: tensor([  5.3580, -17.2496])\n",
      "    Grad:   tensor([-0.0016,  0.0094])\n",
      "Epoch 3389, Loss 2.927913\n",
      "    Params: tensor([  5.3580, -17.2496])\n",
      "    Grad:   tensor([-0.0017,  0.0094])\n",
      "Epoch 3390, Loss 2.927911\n",
      "    Params: tensor([  5.3580, -17.2497])\n",
      "    Grad:   tensor([-0.0016,  0.0094])\n",
      "Epoch 3391, Loss 2.927913\n",
      "    Params: tensor([  5.3580, -17.2498])\n",
      "    Grad:   tensor([-0.0017,  0.0093])\n",
      "Epoch 3392, Loss 2.927911\n",
      "    Params: tensor([  5.3580, -17.2499])\n",
      "    Grad:   tensor([-0.0016,  0.0093])\n",
      "Epoch 3393, Loss 2.927910\n",
      "    Params: tensor([  5.3580, -17.2500])\n",
      "    Grad:   tensor([-0.0016,  0.0093])\n",
      "Epoch 3394, Loss 2.927909\n",
      "    Params: tensor([  5.3581, -17.2501])\n",
      "    Grad:   tensor([-0.0016,  0.0093])\n",
      "Epoch 3395, Loss 2.927908\n",
      "    Params: tensor([  5.3581, -17.2502])\n",
      "    Grad:   tensor([-0.0016,  0.0093])\n",
      "Epoch 3396, Loss 2.927907\n",
      "    Params: tensor([  5.3581, -17.2503])\n",
      "    Grad:   tensor([-0.0017,  0.0093])\n",
      "Epoch 3397, Loss 2.927906\n",
      "    Params: tensor([  5.3581, -17.2504])\n",
      "    Grad:   tensor([-0.0016,  0.0093])\n",
      "Epoch 3398, Loss 2.927905\n",
      "    Params: tensor([  5.3581, -17.2505])\n",
      "    Grad:   tensor([-0.0017,  0.0092])\n",
      "Epoch 3399, Loss 2.927905\n",
      "    Params: tensor([  5.3581, -17.2506])\n",
      "    Grad:   tensor([-0.0016,  0.0092])\n",
      "Epoch 3400, Loss 2.927904\n",
      "    Params: tensor([  5.3582, -17.2507])\n",
      "    Grad:   tensor([-0.0016,  0.0092])\n",
      "Epoch 3401, Loss 2.927902\n",
      "    Params: tensor([  5.3582, -17.2508])\n",
      "    Grad:   tensor([-0.0016,  0.0092])\n",
      "Epoch 3402, Loss 2.927902\n",
      "    Params: tensor([  5.3582, -17.2509])\n",
      "    Grad:   tensor([-0.0016,  0.0092])\n",
      "Epoch 3403, Loss 2.927902\n",
      "    Params: tensor([  5.3582, -17.2509])\n",
      "    Grad:   tensor([-0.0016,  0.0092])\n",
      "Epoch 3404, Loss 2.927899\n",
      "    Params: tensor([  5.3582, -17.2510])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3405, Loss 2.927899\n",
      "    Params: tensor([  5.3582, -17.2511])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3406, Loss 2.927898\n",
      "    Params: tensor([  5.3583, -17.2512])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3407, Loss 2.927899\n",
      "    Params: tensor([  5.3583, -17.2513])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3408, Loss 2.927896\n",
      "    Params: tensor([  5.3583, -17.2514])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3409, Loss 2.927895\n",
      "    Params: tensor([  5.3583, -17.2515])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3410, Loss 2.927896\n",
      "    Params: tensor([  5.3583, -17.2516])\n",
      "    Grad:   tensor([-0.0016,  0.0091])\n",
      "Epoch 3411, Loss 2.927894\n",
      "    Params: tensor([  5.3583, -17.2517])\n",
      "    Grad:   tensor([-0.0016,  0.0090])\n",
      "Epoch 3412, Loss 2.927892\n",
      "    Params: tensor([  5.3584, -17.2518])\n",
      "    Grad:   tensor([-0.0016,  0.0090])\n",
      "Epoch 3413, Loss 2.927892\n",
      "    Params: tensor([  5.3584, -17.2519])\n",
      "    Grad:   tensor([-0.0016,  0.0090])\n",
      "Epoch 3414, Loss 2.927891\n",
      "    Params: tensor([  5.3584, -17.2519])\n",
      "    Grad:   tensor([-0.0016,  0.0090])\n",
      "Epoch 3415, Loss 2.927891\n",
      "    Params: tensor([  5.3584, -17.2520])\n",
      "    Grad:   tensor([-0.0016,  0.0090])\n",
      "Epoch 3416, Loss 2.927890\n",
      "    Params: tensor([  5.3584, -17.2521])\n",
      "    Grad:   tensor([-0.0016,  0.0090])\n",
      "Epoch 3417, Loss 2.927891\n",
      "    Params: tensor([  5.3584, -17.2522])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3418, Loss 2.927888\n",
      "    Params: tensor([  5.3584, -17.2523])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3419, Loss 2.927888\n",
      "    Params: tensor([  5.3585, -17.2524])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3420, Loss 2.927886\n",
      "    Params: tensor([  5.3585, -17.2525])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3421, Loss 2.927887\n",
      "    Params: tensor([  5.3585, -17.2526])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3422, Loss 2.927886\n",
      "    Params: tensor([  5.3585, -17.2527])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3423, Loss 2.927884\n",
      "    Params: tensor([  5.3585, -17.2527])\n",
      "    Grad:   tensor([-0.0016,  0.0089])\n",
      "Epoch 3424, Loss 2.927883\n",
      "    Params: tensor([  5.3585, -17.2528])\n",
      "    Grad:   tensor([-0.0015,  0.0088])\n",
      "Epoch 3425, Loss 2.927881\n",
      "    Params: tensor([  5.3586, -17.2529])\n",
      "    Grad:   tensor([-0.0015,  0.0088])\n",
      "Epoch 3426, Loss 2.927881\n",
      "    Params: tensor([  5.3586, -17.2530])\n",
      "    Grad:   tensor([-0.0016,  0.0088])\n",
      "Epoch 3427, Loss 2.927880\n",
      "    Params: tensor([  5.3586, -17.2531])\n",
      "    Grad:   tensor([-0.0015,  0.0088])\n",
      "Epoch 3428, Loss 2.927880\n",
      "    Params: tensor([  5.3586, -17.2532])\n",
      "    Grad:   tensor([-0.0016,  0.0088])\n",
      "Epoch 3429, Loss 2.927879\n",
      "    Params: tensor([  5.3586, -17.2533])\n",
      "    Grad:   tensor([-0.0015,  0.0088])\n",
      "Epoch 3430, Loss 2.927877\n",
      "    Params: tensor([  5.3586, -17.2534])\n",
      "    Grad:   tensor([-0.0016,  0.0087])\n",
      "Epoch 3431, Loss 2.927876\n",
      "    Params: tensor([  5.3586, -17.2534])\n",
      "    Grad:   tensor([-0.0015,  0.0087])\n",
      "Epoch 3432, Loss 2.927876\n",
      "    Params: tensor([  5.3587, -17.2535])\n",
      "    Grad:   tensor([-0.0015,  0.0087])\n",
      "Epoch 3433, Loss 2.927876\n",
      "    Params: tensor([  5.3587, -17.2536])\n",
      "    Grad:   tensor([-0.0015,  0.0087])\n",
      "Epoch 3434, Loss 2.927876\n",
      "    Params: tensor([  5.3587, -17.2537])\n",
      "    Grad:   tensor([-0.0016,  0.0087])\n",
      "Epoch 3435, Loss 2.927876\n",
      "    Params: tensor([  5.3587, -17.2538])\n",
      "    Grad:   tensor([-0.0016,  0.0087])\n",
      "Epoch 3436, Loss 2.927875\n",
      "    Params: tensor([  5.3587, -17.2539])\n",
      "    Grad:   tensor([-0.0015,  0.0087])\n",
      "Epoch 3437, Loss 2.927873\n",
      "    Params: tensor([  5.3587, -17.2540])\n",
      "    Grad:   tensor([-0.0015,  0.0087])\n",
      "Epoch 3438, Loss 2.927872\n",
      "    Params: tensor([  5.3588, -17.2541])\n",
      "    Grad:   tensor([-0.0015,  0.0086])\n",
      "Epoch 3439, Loss 2.927871\n",
      "    Params: tensor([  5.3588, -17.2541])\n",
      "    Grad:   tensor([-0.0015,  0.0086])\n",
      "Epoch 3440, Loss 2.927870\n",
      "    Params: tensor([  5.3588, -17.2542])\n",
      "    Grad:   tensor([-0.0015,  0.0086])\n",
      "Epoch 3441, Loss 2.927871\n",
      "    Params: tensor([  5.3588, -17.2543])\n",
      "    Grad:   tensor([-0.0015,  0.0086])\n",
      "Epoch 3442, Loss 2.927869\n",
      "    Params: tensor([  5.3588, -17.2544])\n",
      "    Grad:   tensor([-0.0015,  0.0086])\n",
      "Epoch 3443, Loss 2.927869\n",
      "    Params: tensor([  5.3588, -17.2545])\n",
      "    Grad:   tensor([-0.0015,  0.0086])\n",
      "Epoch 3444, Loss 2.927867\n",
      "    Params: tensor([  5.3588, -17.2546])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3445, Loss 2.927866\n",
      "    Params: tensor([  5.3589, -17.2547])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3446, Loss 2.927866\n",
      "    Params: tensor([  5.3589, -17.2547])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3447, Loss 2.927866\n",
      "    Params: tensor([  5.3589, -17.2548])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3448, Loss 2.927864\n",
      "    Params: tensor([  5.3589, -17.2549])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3449, Loss 2.927863\n",
      "    Params: tensor([  5.3589, -17.2550])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3450, Loss 2.927863\n",
      "    Params: tensor([  5.3589, -17.2551])\n",
      "    Grad:   tensor([-0.0015,  0.0085])\n",
      "Epoch 3451, Loss 2.927862\n",
      "    Params: tensor([  5.3590, -17.2552])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3452, Loss 2.927863\n",
      "    Params: tensor([  5.3590, -17.2552])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3453, Loss 2.927860\n",
      "    Params: tensor([  5.3590, -17.2553])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3454, Loss 2.927860\n",
      "    Params: tensor([  5.3590, -17.2554])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3455, Loss 2.927860\n",
      "    Params: tensor([  5.3590, -17.2555])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3456, Loss 2.927859\n",
      "    Params: tensor([  5.3590, -17.2556])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3457, Loss 2.927858\n",
      "    Params: tensor([  5.3590, -17.2557])\n",
      "    Grad:   tensor([-0.0015,  0.0084])\n",
      "Epoch 3458, Loss 2.927858\n",
      "    Params: tensor([  5.3591, -17.2557])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3459, Loss 2.927856\n",
      "    Params: tensor([  5.3591, -17.2558])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3460, Loss 2.927857\n",
      "    Params: tensor([  5.3591, -17.2559])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3461, Loss 2.927854\n",
      "    Params: tensor([  5.3591, -17.2560])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3462, Loss 2.927855\n",
      "    Params: tensor([  5.3591, -17.2561])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3463, Loss 2.927854\n",
      "    Params: tensor([  5.3591, -17.2562])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3464, Loss 2.927854\n",
      "    Params: tensor([  5.3591, -17.2562])\n",
      "    Grad:   tensor([-0.0015,  0.0083])\n",
      "Epoch 3465, Loss 2.927851\n",
      "    Params: tensor([  5.3592, -17.2563])\n",
      "    Grad:   tensor([-0.0014,  0.0082])\n",
      "Epoch 3466, Loss 2.927853\n",
      "    Params: tensor([  5.3592, -17.2564])\n",
      "    Grad:   tensor([-0.0015,  0.0082])\n",
      "Epoch 3467, Loss 2.927852\n",
      "    Params: tensor([  5.3592, -17.2565])\n",
      "    Grad:   tensor([-0.0014,  0.0082])\n",
      "Epoch 3468, Loss 2.927850\n",
      "    Params: tensor([  5.3592, -17.2566])\n",
      "    Grad:   tensor([-0.0014,  0.0082])\n",
      "Epoch 3469, Loss 2.927849\n",
      "    Params: tensor([  5.3592, -17.2567])\n",
      "    Grad:   tensor([-0.0015,  0.0082])\n",
      "Epoch 3470, Loss 2.927849\n",
      "    Params: tensor([  5.3592, -17.2567])\n",
      "    Grad:   tensor([-0.0015,  0.0082])\n",
      "Epoch 3471, Loss 2.927848\n",
      "    Params: tensor([  5.3592, -17.2568])\n",
      "    Grad:   tensor([-0.0014,  0.0082])\n",
      "Epoch 3472, Loss 2.927848\n",
      "    Params: tensor([  5.3593, -17.2569])\n",
      "    Grad:   tensor([-0.0014,  0.0081])\n",
      "Epoch 3473, Loss 2.927846\n",
      "    Params: tensor([  5.3593, -17.2570])\n",
      "    Grad:   tensor([-0.0015,  0.0081])\n",
      "Epoch 3474, Loss 2.927846\n",
      "    Params: tensor([  5.3593, -17.2571])\n",
      "    Grad:   tensor([-0.0015,  0.0081])\n",
      "Epoch 3475, Loss 2.927845\n",
      "    Params: tensor([  5.3593, -17.2571])\n",
      "    Grad:   tensor([-0.0014,  0.0081])\n",
      "Epoch 3476, Loss 2.927844\n",
      "    Params: tensor([  5.3593, -17.2572])\n",
      "    Grad:   tensor([-0.0014,  0.0081])\n",
      "Epoch 3477, Loss 2.927844\n",
      "    Params: tensor([  5.3593, -17.2573])\n",
      "    Grad:   tensor([-0.0014,  0.0081])\n",
      "Epoch 3478, Loss 2.927844\n",
      "    Params: tensor([  5.3593, -17.2574])\n",
      "    Grad:   tensor([-0.0014,  0.0081])\n",
      "Epoch 3479, Loss 2.927843\n",
      "    Params: tensor([  5.3594, -17.2575])\n",
      "    Grad:   tensor([-0.0014,  0.0081])\n",
      "Epoch 3480, Loss 2.927843\n",
      "    Params: tensor([  5.3594, -17.2575])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3481, Loss 2.927842\n",
      "    Params: tensor([  5.3594, -17.2576])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3482, Loss 2.927840\n",
      "    Params: tensor([  5.3594, -17.2577])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3483, Loss 2.927842\n",
      "    Params: tensor([  5.3594, -17.2578])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3484, Loss 2.927839\n",
      "    Params: tensor([  5.3594, -17.2579])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3485, Loss 2.927838\n",
      "    Params: tensor([  5.3594, -17.2579])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3486, Loss 2.927839\n",
      "    Params: tensor([  5.3595, -17.2580])\n",
      "    Grad:   tensor([-0.0014,  0.0080])\n",
      "Epoch 3487, Loss 2.927838\n",
      "    Params: tensor([  5.3595, -17.2581])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3488, Loss 2.927837\n",
      "    Params: tensor([  5.3595, -17.2582])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3489, Loss 2.927835\n",
      "    Params: tensor([  5.3595, -17.2583])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3490, Loss 2.927837\n",
      "    Params: tensor([  5.3595, -17.2583])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3491, Loss 2.927836\n",
      "    Params: tensor([  5.3595, -17.2584])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3492, Loss 2.927835\n",
      "    Params: tensor([  5.3595, -17.2585])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3493, Loss 2.927833\n",
      "    Params: tensor([  5.3596, -17.2586])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3494, Loss 2.927833\n",
      "    Params: tensor([  5.3596, -17.2587])\n",
      "    Grad:   tensor([-0.0014,  0.0079])\n",
      "Epoch 3495, Loss 2.927833\n",
      "    Params: tensor([  5.3596, -17.2587])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3496, Loss 2.927832\n",
      "    Params: tensor([  5.3596, -17.2588])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3497, Loss 2.927831\n",
      "    Params: tensor([  5.3596, -17.2589])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3498, Loss 2.927830\n",
      "    Params: tensor([  5.3596, -17.2590])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3499, Loss 2.927830\n",
      "    Params: tensor([  5.3596, -17.2590])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3500, Loss 2.927830\n",
      "    Params: tensor([  5.3597, -17.2591])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3501, Loss 2.927829\n",
      "    Params: tensor([  5.3597, -17.2592])\n",
      "    Grad:   tensor([-0.0014,  0.0078])\n",
      "Epoch 3502, Loss 2.927828\n",
      "    Params: tensor([  5.3597, -17.2593])\n",
      "    Grad:   tensor([-0.0014,  0.0077])\n",
      "Epoch 3503, Loss 2.927828\n",
      "    Params: tensor([  5.3597, -17.2594])\n",
      "    Grad:   tensor([-0.0014,  0.0077])\n",
      "Epoch 3504, Loss 2.927827\n",
      "    Params: tensor([  5.3597, -17.2594])\n",
      "    Grad:   tensor([-0.0014,  0.0077])\n",
      "Epoch 3505, Loss 2.927825\n",
      "    Params: tensor([  5.3597, -17.2595])\n",
      "    Grad:   tensor([-0.0014,  0.0077])\n",
      "Epoch 3506, Loss 2.927827\n",
      "    Params: tensor([  5.3597, -17.2596])\n",
      "    Grad:   tensor([-0.0014,  0.0077])\n",
      "Epoch 3507, Loss 2.927825\n",
      "    Params: tensor([  5.3597, -17.2597])\n",
      "    Grad:   tensor([-0.0014,  0.0077])\n",
      "Epoch 3508, Loss 2.927824\n",
      "    Params: tensor([  5.3598, -17.2597])\n",
      "    Grad:   tensor([-0.0013,  0.0077])\n",
      "Epoch 3509, Loss 2.927824\n",
      "    Params: tensor([  5.3598, -17.2598])\n",
      "    Grad:   tensor([-0.0013,  0.0077])\n",
      "Epoch 3510, Loss 2.927824\n",
      "    Params: tensor([  5.3598, -17.2599])\n",
      "    Grad:   tensor([-0.0013,  0.0076])\n",
      "Epoch 3511, Loss 2.927822\n",
      "    Params: tensor([  5.3598, -17.2600])\n",
      "    Grad:   tensor([-0.0014,  0.0076])\n",
      "Epoch 3512, Loss 2.927822\n",
      "    Params: tensor([  5.3598, -17.2600])\n",
      "    Grad:   tensor([-0.0014,  0.0076])\n",
      "Epoch 3513, Loss 2.927821\n",
      "    Params: tensor([  5.3598, -17.2601])\n",
      "    Grad:   tensor([-0.0013,  0.0076])\n",
      "Epoch 3514, Loss 2.927820\n",
      "    Params: tensor([  5.3598, -17.2602])\n",
      "    Grad:   tensor([-0.0013,  0.0076])\n",
      "Epoch 3515, Loss 2.927820\n",
      "    Params: tensor([  5.3599, -17.2603])\n",
      "    Grad:   tensor([-0.0014,  0.0076])\n",
      "Epoch 3516, Loss 2.927821\n",
      "    Params: tensor([  5.3599, -17.2604])\n",
      "    Grad:   tensor([-0.0014,  0.0076])\n",
      "Epoch 3517, Loss 2.927819\n",
      "    Params: tensor([  5.3599, -17.2604])\n",
      "    Grad:   tensor([-0.0014,  0.0075])\n",
      "Epoch 3518, Loss 2.927819\n",
      "    Params: tensor([  5.3599, -17.2605])\n",
      "    Grad:   tensor([-0.0014,  0.0075])\n",
      "Epoch 3519, Loss 2.927819\n",
      "    Params: tensor([  5.3599, -17.2606])\n",
      "    Grad:   tensor([-0.0013,  0.0075])\n",
      "Epoch 3520, Loss 2.927817\n",
      "    Params: tensor([  5.3599, -17.2607])\n",
      "    Grad:   tensor([-0.0013,  0.0075])\n",
      "Epoch 3521, Loss 2.927817\n",
      "    Params: tensor([  5.3599, -17.2607])\n",
      "    Grad:   tensor([-0.0013,  0.0075])\n",
      "Epoch 3522, Loss 2.927816\n",
      "    Params: tensor([  5.3599, -17.2608])\n",
      "    Grad:   tensor([-0.0013,  0.0075])\n",
      "Epoch 3523, Loss 2.927815\n",
      "    Params: tensor([  5.3600, -17.2609])\n",
      "    Grad:   tensor([-0.0013,  0.0075])\n",
      "Epoch 3524, Loss 2.927816\n",
      "    Params: tensor([  5.3600, -17.2610])\n",
      "    Grad:   tensor([-0.0013,  0.0075])\n",
      "Epoch 3525, Loss 2.927815\n",
      "    Params: tensor([  5.3600, -17.2610])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3526, Loss 2.927814\n",
      "    Params: tensor([  5.3600, -17.2611])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3527, Loss 2.927813\n",
      "    Params: tensor([  5.3600, -17.2612])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3528, Loss 2.927812\n",
      "    Params: tensor([  5.3600, -17.2612])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3529, Loss 2.927811\n",
      "    Params: tensor([  5.3600, -17.2613])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3530, Loss 2.927812\n",
      "    Params: tensor([  5.3601, -17.2614])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3531, Loss 2.927812\n",
      "    Params: tensor([  5.3601, -17.2615])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3532, Loss 2.927810\n",
      "    Params: tensor([  5.3601, -17.2615])\n",
      "    Grad:   tensor([-0.0013,  0.0074])\n",
      "Epoch 3533, Loss 2.927809\n",
      "    Params: tensor([  5.3601, -17.2616])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3534, Loss 2.927810\n",
      "    Params: tensor([  5.3601, -17.2617])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3535, Loss 2.927809\n",
      "    Params: tensor([  5.3601, -17.2618])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3536, Loss 2.927808\n",
      "    Params: tensor([  5.3601, -17.2618])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3537, Loss 2.927808\n",
      "    Params: tensor([  5.3601, -17.2619])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3538, Loss 2.927806\n",
      "    Params: tensor([  5.3602, -17.2620])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3539, Loss 2.927806\n",
      "    Params: tensor([  5.3602, -17.2621])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3540, Loss 2.927805\n",
      "    Params: tensor([  5.3602, -17.2621])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3541, Loss 2.927804\n",
      "    Params: tensor([  5.3602, -17.2622])\n",
      "    Grad:   tensor([-0.0013,  0.0073])\n",
      "Epoch 3542, Loss 2.927805\n",
      "    Params: tensor([  5.3602, -17.2623])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3543, Loss 2.927804\n",
      "    Params: tensor([  5.3602, -17.2623])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3544, Loss 2.927805\n",
      "    Params: tensor([  5.3602, -17.2624])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3545, Loss 2.927804\n",
      "    Params: tensor([  5.3602, -17.2625])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3546, Loss 2.927804\n",
      "    Params: tensor([  5.3603, -17.2626])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3547, Loss 2.927803\n",
      "    Params: tensor([  5.3603, -17.2626])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3548, Loss 2.927802\n",
      "    Params: tensor([  5.3603, -17.2627])\n",
      "    Grad:   tensor([-0.0013,  0.0072])\n",
      "Epoch 3549, Loss 2.927801\n",
      "    Params: tensor([  5.3603, -17.2628])\n",
      "    Grad:   tensor([-0.0013,  0.0071])\n",
      "Epoch 3550, Loss 2.927801\n",
      "    Params: tensor([  5.3603, -17.2628])\n",
      "    Grad:   tensor([-0.0013,  0.0071])\n",
      "Epoch 3551, Loss 2.927799\n",
      "    Params: tensor([  5.3603, -17.2629])\n",
      "    Grad:   tensor([-0.0012,  0.0071])\n",
      "Epoch 3552, Loss 2.927801\n",
      "    Params: tensor([  5.3603, -17.2630])\n",
      "    Grad:   tensor([-0.0012,  0.0071])\n",
      "Epoch 3553, Loss 2.927798\n",
      "    Params: tensor([  5.3603, -17.2631])\n",
      "    Grad:   tensor([-0.0012,  0.0071])\n",
      "Epoch 3554, Loss 2.927798\n",
      "    Params: tensor([  5.3604, -17.2631])\n",
      "    Grad:   tensor([-0.0012,  0.0071])\n",
      "Epoch 3555, Loss 2.927798\n",
      "    Params: tensor([  5.3604, -17.2632])\n",
      "    Grad:   tensor([-0.0013,  0.0071])\n",
      "Epoch 3556, Loss 2.927798\n",
      "    Params: tensor([  5.3604, -17.2633])\n",
      "    Grad:   tensor([-0.0013,  0.0071])\n",
      "Epoch 3557, Loss 2.927798\n",
      "    Params: tensor([  5.3604, -17.2633])\n",
      "    Grad:   tensor([-0.0013,  0.0071])\n",
      "Epoch 3558, Loss 2.927796\n",
      "    Params: tensor([  5.3604, -17.2634])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3559, Loss 2.927795\n",
      "    Params: tensor([  5.3604, -17.2635])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3560, Loss 2.927796\n",
      "    Params: tensor([  5.3604, -17.2636])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3561, Loss 2.927794\n",
      "    Params: tensor([  5.3604, -17.2636])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3562, Loss 2.927795\n",
      "    Params: tensor([  5.3605, -17.2637])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3563, Loss 2.927795\n",
      "    Params: tensor([  5.3605, -17.2638])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3564, Loss 2.927793\n",
      "    Params: tensor([  5.3605, -17.2638])\n",
      "    Grad:   tensor([-0.0013,  0.0070])\n",
      "Epoch 3565, Loss 2.927795\n",
      "    Params: tensor([  5.3605, -17.2639])\n",
      "    Grad:   tensor([-0.0012,  0.0070])\n",
      "Epoch 3566, Loss 2.927791\n",
      "    Params: tensor([  5.3605, -17.2640])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3567, Loss 2.927791\n",
      "    Params: tensor([  5.3605, -17.2640])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3568, Loss 2.927791\n",
      "    Params: tensor([  5.3605, -17.2641])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3569, Loss 2.927790\n",
      "    Params: tensor([  5.3605, -17.2642])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3570, Loss 2.927790\n",
      "    Params: tensor([  5.3606, -17.2642])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3571, Loss 2.927789\n",
      "    Params: tensor([  5.3606, -17.2643])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3572, Loss 2.927790\n",
      "    Params: tensor([  5.3606, -17.2644])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3573, Loss 2.927789\n",
      "    Params: tensor([  5.3606, -17.2645])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3574, Loss 2.927789\n",
      "    Params: tensor([  5.3606, -17.2645])\n",
      "    Grad:   tensor([-0.0012,  0.0069])\n",
      "Epoch 3575, Loss 2.927789\n",
      "    Params: tensor([  5.3606, -17.2646])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3576, Loss 2.927787\n",
      "    Params: tensor([  5.3606, -17.2647])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3577, Loss 2.927786\n",
      "    Params: tensor([  5.3606, -17.2647])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3578, Loss 2.927788\n",
      "    Params: tensor([  5.3607, -17.2648])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3579, Loss 2.927785\n",
      "    Params: tensor([  5.3607, -17.2649])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3580, Loss 2.927785\n",
      "    Params: tensor([  5.3607, -17.2649])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3581, Loss 2.927786\n",
      "    Params: tensor([  5.3607, -17.2650])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3582, Loss 2.927785\n",
      "    Params: tensor([  5.3607, -17.2651])\n",
      "    Grad:   tensor([-0.0012,  0.0068])\n",
      "Epoch 3583, Loss 2.927784\n",
      "    Params: tensor([  5.3607, -17.2651])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3584, Loss 2.927784\n",
      "    Params: tensor([  5.3607, -17.2652])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3585, Loss 2.927783\n",
      "    Params: tensor([  5.3607, -17.2653])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3586, Loss 2.927783\n",
      "    Params: tensor([  5.3607, -17.2653])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3587, Loss 2.927781\n",
      "    Params: tensor([  5.3608, -17.2654])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3588, Loss 2.927782\n",
      "    Params: tensor([  5.3608, -17.2655])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3589, Loss 2.927781\n",
      "    Params: tensor([  5.3608, -17.2655])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3590, Loss 2.927781\n",
      "    Params: tensor([  5.3608, -17.2656])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3591, Loss 2.927781\n",
      "    Params: tensor([  5.3608, -17.2657])\n",
      "    Grad:   tensor([-0.0012,  0.0067])\n",
      "Epoch 3592, Loss 2.927780\n",
      "    Params: tensor([  5.3608, -17.2657])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3593, Loss 2.927780\n",
      "    Params: tensor([  5.3608, -17.2658])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3594, Loss 2.927778\n",
      "    Params: tensor([  5.3608, -17.2659])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3595, Loss 2.927779\n",
      "    Params: tensor([  5.3609, -17.2659])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3596, Loss 2.927778\n",
      "    Params: tensor([  5.3609, -17.2660])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3597, Loss 2.927778\n",
      "    Params: tensor([  5.3609, -17.2661])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3598, Loss 2.927779\n",
      "    Params: tensor([  5.3609, -17.2661])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3599, Loss 2.927777\n",
      "    Params: tensor([  5.3609, -17.2662])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3600, Loss 2.927776\n",
      "    Params: tensor([  5.3609, -17.2663])\n",
      "    Grad:   tensor([-0.0012,  0.0066])\n",
      "Epoch 3601, Loss 2.927775\n",
      "    Params: tensor([  5.3609, -17.2663])\n",
      "    Grad:   tensor([-0.0012,  0.0065])\n",
      "Epoch 3602, Loss 2.927776\n",
      "    Params: tensor([  5.3609, -17.2664])\n",
      "    Grad:   tensor([-0.0012,  0.0065])\n",
      "Epoch 3603, Loss 2.927773\n",
      "    Params: tensor([  5.3609, -17.2665])\n",
      "    Grad:   tensor([-0.0012,  0.0065])\n",
      "Epoch 3604, Loss 2.927775\n",
      "    Params: tensor([  5.3610, -17.2665])\n",
      "    Grad:   tensor([-0.0012,  0.0065])\n",
      "Epoch 3605, Loss 2.927775\n",
      "    Params: tensor([  5.3610, -17.2666])\n",
      "    Grad:   tensor([-0.0011,  0.0065])\n",
      "Epoch 3606, Loss 2.927775\n",
      "    Params: tensor([  5.3610, -17.2667])\n",
      "    Grad:   tensor([-0.0011,  0.0065])\n",
      "Epoch 3607, Loss 2.927773\n",
      "    Params: tensor([  5.3610, -17.2667])\n",
      "    Grad:   tensor([-0.0011,  0.0065])\n",
      "Epoch 3608, Loss 2.927773\n",
      "    Params: tensor([  5.3610, -17.2668])\n",
      "    Grad:   tensor([-0.0011,  0.0065])\n",
      "Epoch 3609, Loss 2.927773\n",
      "    Params: tensor([  5.3610, -17.2668])\n",
      "    Grad:   tensor([-0.0011,  0.0065])\n",
      "Epoch 3610, Loss 2.927772\n",
      "    Params: tensor([  5.3610, -17.2669])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3611, Loss 2.927772\n",
      "    Params: tensor([  5.3610, -17.2670])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3612, Loss 2.927770\n",
      "    Params: tensor([  5.3611, -17.2670])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3613, Loss 2.927772\n",
      "    Params: tensor([  5.3611, -17.2671])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3614, Loss 2.927771\n",
      "    Params: tensor([  5.3611, -17.2672])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3615, Loss 2.927770\n",
      "    Params: tensor([  5.3611, -17.2672])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3616, Loss 2.927770\n",
      "    Params: tensor([  5.3611, -17.2673])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3617, Loss 2.927769\n",
      "    Params: tensor([  5.3611, -17.2674])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3618, Loss 2.927768\n",
      "    Params: tensor([  5.3611, -17.2674])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3619, Loss 2.927769\n",
      "    Params: tensor([  5.3611, -17.2675])\n",
      "    Grad:   tensor([-0.0011,  0.0064])\n",
      "Epoch 3620, Loss 2.927768\n",
      "    Params: tensor([  5.3611, -17.2675])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3621, Loss 2.927767\n",
      "    Params: tensor([  5.3612, -17.2676])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3622, Loss 2.927767\n",
      "    Params: tensor([  5.3612, -17.2677])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3623, Loss 2.927767\n",
      "    Params: tensor([  5.3612, -17.2677])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3624, Loss 2.927765\n",
      "    Params: tensor([  5.3612, -17.2678])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3625, Loss 2.927766\n",
      "    Params: tensor([  5.3612, -17.2679])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3626, Loss 2.927765\n",
      "    Params: tensor([  5.3612, -17.2679])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3627, Loss 2.927765\n",
      "    Params: tensor([  5.3612, -17.2680])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3628, Loss 2.927764\n",
      "    Params: tensor([  5.3612, -17.2681])\n",
      "    Grad:   tensor([-0.0011,  0.0063])\n",
      "Epoch 3629, Loss 2.927764\n",
      "    Params: tensor([  5.3612, -17.2681])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3630, Loss 2.927764\n",
      "    Params: tensor([  5.3613, -17.2682])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3631, Loss 2.927762\n",
      "    Params: tensor([  5.3613, -17.2682])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3632, Loss 2.927763\n",
      "    Params: tensor([  5.3613, -17.2683])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3633, Loss 2.927763\n",
      "    Params: tensor([  5.3613, -17.2684])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3634, Loss 2.927762\n",
      "    Params: tensor([  5.3613, -17.2684])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3635, Loss 2.927761\n",
      "    Params: tensor([  5.3613, -17.2685])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3636, Loss 2.927762\n",
      "    Params: tensor([  5.3613, -17.2685])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3637, Loss 2.927759\n",
      "    Params: tensor([  5.3613, -17.2686])\n",
      "    Grad:   tensor([-0.0011,  0.0062])\n",
      "Epoch 3638, Loss 2.927761\n",
      "    Params: tensor([  5.3613, -17.2687])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3639, Loss 2.927761\n",
      "    Params: tensor([  5.3614, -17.2687])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3640, Loss 2.927760\n",
      "    Params: tensor([  5.3614, -17.2688])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3641, Loss 2.927759\n",
      "    Params: tensor([  5.3614, -17.2689])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3642, Loss 2.927758\n",
      "    Params: tensor([  5.3614, -17.2689])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3643, Loss 2.927759\n",
      "    Params: tensor([  5.3614, -17.2690])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3644, Loss 2.927757\n",
      "    Params: tensor([  5.3614, -17.2690])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3645, Loss 2.927758\n",
      "    Params: tensor([  5.3614, -17.2691])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3646, Loss 2.927757\n",
      "    Params: tensor([  5.3614, -17.2692])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3647, Loss 2.927757\n",
      "    Params: tensor([  5.3614, -17.2692])\n",
      "    Grad:   tensor([-0.0011,  0.0061])\n",
      "Epoch 3648, Loss 2.927757\n",
      "    Params: tensor([  5.3614, -17.2693])\n",
      "    Grad:   tensor([-0.0011,  0.0060])\n",
      "Epoch 3649, Loss 2.927756\n",
      "    Params: tensor([  5.3615, -17.2693])\n",
      "    Grad:   tensor([-0.0011,  0.0060])\n",
      "Epoch 3650, Loss 2.927757\n",
      "    Params: tensor([  5.3615, -17.2694])\n",
      "    Grad:   tensor([-0.0011,  0.0060])\n",
      "Epoch 3651, Loss 2.927756\n",
      "    Params: tensor([  5.3615, -17.2695])\n",
      "    Grad:   tensor([-0.0011,  0.0060])\n",
      "Epoch 3652, Loss 2.927756\n",
      "    Params: tensor([  5.3615, -17.2695])\n",
      "    Grad:   tensor([-0.0010,  0.0060])\n",
      "Epoch 3653, Loss 2.927755\n",
      "    Params: tensor([  5.3615, -17.2696])\n",
      "    Grad:   tensor([-0.0010,  0.0060])\n",
      "Epoch 3654, Loss 2.927755\n",
      "    Params: tensor([  5.3615, -17.2696])\n",
      "    Grad:   tensor([-0.0010,  0.0060])\n",
      "Epoch 3655, Loss 2.927754\n",
      "    Params: tensor([  5.3615, -17.2697])\n",
      "    Grad:   tensor([-0.0010,  0.0060])\n",
      "Epoch 3656, Loss 2.927754\n",
      "    Params: tensor([  5.3615, -17.2698])\n",
      "    Grad:   tensor([-0.0010,  0.0060])\n",
      "Epoch 3657, Loss 2.927755\n",
      "    Params: tensor([  5.3615, -17.2698])\n",
      "    Grad:   tensor([-0.0010,  0.0060])\n",
      "Epoch 3658, Loss 2.927753\n",
      "    Params: tensor([  5.3616, -17.2699])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3659, Loss 2.927752\n",
      "    Params: tensor([  5.3616, -17.2699])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3660, Loss 2.927754\n",
      "    Params: tensor([  5.3616, -17.2700])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3661, Loss 2.927752\n",
      "    Params: tensor([  5.3616, -17.2701])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3662, Loss 2.927751\n",
      "    Params: tensor([  5.3616, -17.2701])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3663, Loss 2.927752\n",
      "    Params: tensor([  5.3616, -17.2702])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3664, Loss 2.927750\n",
      "    Params: tensor([  5.3616, -17.2702])\n",
      "    Grad:   tensor([-0.0011,  0.0059])\n",
      "Epoch 3665, Loss 2.927749\n",
      "    Params: tensor([  5.3616, -17.2703])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3666, Loss 2.927751\n",
      "    Params: tensor([  5.3616, -17.2703])\n",
      "    Grad:   tensor([-0.0010,  0.0059])\n",
      "Epoch 3667, Loss 2.927750\n",
      "    Params: tensor([  5.3616, -17.2704])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3668, Loss 2.927750\n",
      "    Params: tensor([  5.3617, -17.2705])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3669, Loss 2.927747\n",
      "    Params: tensor([  5.3617, -17.2705])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3670, Loss 2.927749\n",
      "    Params: tensor([  5.3617, -17.2706])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3671, Loss 2.927747\n",
      "    Params: tensor([  5.3617, -17.2706])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3672, Loss 2.927748\n",
      "    Params: tensor([  5.3617, -17.2707])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3673, Loss 2.927748\n",
      "    Params: tensor([  5.3617, -17.2708])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3674, Loss 2.927747\n",
      "    Params: tensor([  5.3617, -17.2708])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3675, Loss 2.927747\n",
      "    Params: tensor([  5.3617, -17.2709])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3676, Loss 2.927748\n",
      "    Params: tensor([  5.3617, -17.2709])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3677, Loss 2.927747\n",
      "    Params: tensor([  5.3617, -17.2710])\n",
      "    Grad:   tensor([-0.0010,  0.0058])\n",
      "Epoch 3678, Loss 2.927747\n",
      "    Params: tensor([  5.3618, -17.2710])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3679, Loss 2.927745\n",
      "    Params: tensor([  5.3618, -17.2711])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3680, Loss 2.927745\n",
      "    Params: tensor([  5.3618, -17.2712])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3681, Loss 2.927746\n",
      "    Params: tensor([  5.3618, -17.2712])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3682, Loss 2.927744\n",
      "    Params: tensor([  5.3618, -17.2713])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3683, Loss 2.927743\n",
      "    Params: tensor([  5.3618, -17.2713])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3684, Loss 2.927743\n",
      "    Params: tensor([  5.3618, -17.2714])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3685, Loss 2.927743\n",
      "    Params: tensor([  5.3618, -17.2714])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3686, Loss 2.927743\n",
      "    Params: tensor([  5.3618, -17.2715])\n",
      "    Grad:   tensor([-0.0010,  0.0057])\n",
      "Epoch 3687, Loss 2.927743\n",
      "    Params: tensor([  5.3618, -17.2716])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3688, Loss 2.927744\n",
      "    Params: tensor([  5.3619, -17.2716])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3689, Loss 2.927742\n",
      "    Params: tensor([  5.3619, -17.2717])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3690, Loss 2.927742\n",
      "    Params: tensor([  5.3619, -17.2717])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3691, Loss 2.927742\n",
      "    Params: tensor([  5.3619, -17.2718])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3692, Loss 2.927742\n",
      "    Params: tensor([  5.3619, -17.2718])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3693, Loss 2.927741\n",
      "    Params: tensor([  5.3619, -17.2719])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3694, Loss 2.927741\n",
      "    Params: tensor([  5.3619, -17.2719])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3695, Loss 2.927741\n",
      "    Params: tensor([  5.3619, -17.2720])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3696, Loss 2.927742\n",
      "    Params: tensor([  5.3619, -17.2721])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3697, Loss 2.927741\n",
      "    Params: tensor([  5.3619, -17.2721])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3698, Loss 2.927741\n",
      "    Params: tensor([  5.3620, -17.2722])\n",
      "    Grad:   tensor([-0.0010,  0.0056])\n",
      "Epoch 3699, Loss 2.927740\n",
      "    Params: tensor([  5.3620, -17.2722])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3700, Loss 2.927739\n",
      "    Params: tensor([  5.3620, -17.2723])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3701, Loss 2.927738\n",
      "    Params: tensor([  5.3620, -17.2723])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3702, Loss 2.927738\n",
      "    Params: tensor([  5.3620, -17.2724])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3703, Loss 2.927737\n",
      "    Params: tensor([  5.3620, -17.2724])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3704, Loss 2.927737\n",
      "    Params: tensor([  5.3620, -17.2725])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3705, Loss 2.927738\n",
      "    Params: tensor([  5.3620, -17.2726])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3706, Loss 2.927737\n",
      "    Params: tensor([  5.3620, -17.2726])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3707, Loss 2.927736\n",
      "    Params: tensor([  5.3620, -17.2727])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3708, Loss 2.927737\n",
      "    Params: tensor([  5.3621, -17.2727])\n",
      "    Grad:   tensor([-0.0010,  0.0055])\n",
      "Epoch 3709, Loss 2.927737\n",
      "    Params: tensor([  5.3621, -17.2728])\n",
      "    Grad:   tensor([-0.0010,  0.0054])\n",
      "Epoch 3710, Loss 2.927736\n",
      "    Params: tensor([  5.3621, -17.2728])\n",
      "    Grad:   tensor([-0.0010,  0.0054])\n",
      "Epoch 3711, Loss 2.927734\n",
      "    Params: tensor([  5.3621, -17.2729])\n",
      "    Grad:   tensor([-0.0010,  0.0054])\n",
      "Epoch 3712, Loss 2.927735\n",
      "    Params: tensor([  5.3621, -17.2729])\n",
      "    Grad:   tensor([-0.0010,  0.0054])\n",
      "Epoch 3713, Loss 2.927735\n",
      "    Params: tensor([  5.3621, -17.2730])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3714, Loss 2.927734\n",
      "    Params: tensor([  5.3621, -17.2730])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3715, Loss 2.927734\n",
      "    Params: tensor([  5.3621, -17.2731])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3716, Loss 2.927733\n",
      "    Params: tensor([  5.3621, -17.2732])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3717, Loss 2.927734\n",
      "    Params: tensor([  5.3621, -17.2732])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3718, Loss 2.927733\n",
      "    Params: tensor([  5.3622, -17.2733])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3719, Loss 2.927733\n",
      "    Params: tensor([  5.3622, -17.2733])\n",
      "    Grad:   tensor([-0.0009,  0.0054])\n",
      "Epoch 3720, Loss 2.927733\n",
      "    Params: tensor([  5.3622, -17.2734])\n",
      "    Grad:   tensor([-0.0010,  0.0053])\n",
      "Epoch 3721, Loss 2.927732\n",
      "    Params: tensor([  5.3622, -17.2734])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3722, Loss 2.927731\n",
      "    Params: tensor([  5.3622, -17.2735])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3723, Loss 2.927731\n",
      "    Params: tensor([  5.3622, -17.2735])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3724, Loss 2.927733\n",
      "    Params: tensor([  5.3622, -17.2736])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3725, Loss 2.927730\n",
      "    Params: tensor([  5.3622, -17.2736])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3726, Loss 2.927730\n",
      "    Params: tensor([  5.3622, -17.2737])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3727, Loss 2.927732\n",
      "    Params: tensor([  5.3622, -17.2737])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3728, Loss 2.927730\n",
      "    Params: tensor([  5.3622, -17.2738])\n",
      "    Grad:   tensor([-0.0010,  0.0053])\n",
      "Epoch 3729, Loss 2.927732\n",
      "    Params: tensor([  5.3623, -17.2738])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3730, Loss 2.927731\n",
      "    Params: tensor([  5.3623, -17.2739])\n",
      "    Grad:   tensor([-0.0009,  0.0053])\n",
      "Epoch 3731, Loss 2.927730\n",
      "    Params: tensor([  5.3623, -17.2740])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3732, Loss 2.927728\n",
      "    Params: tensor([  5.3623, -17.2740])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3733, Loss 2.927729\n",
      "    Params: tensor([  5.3623, -17.2741])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3734, Loss 2.927729\n",
      "    Params: tensor([  5.3623, -17.2741])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3735, Loss 2.927728\n",
      "    Params: tensor([  5.3623, -17.2742])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3736, Loss 2.927728\n",
      "    Params: tensor([  5.3623, -17.2742])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3737, Loss 2.927728\n",
      "    Params: tensor([  5.3623, -17.2743])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3738, Loss 2.927728\n",
      "    Params: tensor([  5.3623, -17.2743])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3739, Loss 2.927727\n",
      "    Params: tensor([  5.3623, -17.2744])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3740, Loss 2.927728\n",
      "    Params: tensor([  5.3624, -17.2744])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3741, Loss 2.927728\n",
      "    Params: tensor([  5.3624, -17.2745])\n",
      "    Grad:   tensor([-0.0009,  0.0052])\n",
      "Epoch 3742, Loss 2.927727\n",
      "    Params: tensor([  5.3624, -17.2745])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3743, Loss 2.927727\n",
      "    Params: tensor([  5.3624, -17.2746])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3744, Loss 2.927726\n",
      "    Params: tensor([  5.3624, -17.2746])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3745, Loss 2.927726\n",
      "    Params: tensor([  5.3624, -17.2747])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3746, Loss 2.927725\n",
      "    Params: tensor([  5.3624, -17.2747])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3747, Loss 2.927725\n",
      "    Params: tensor([  5.3624, -17.2748])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3748, Loss 2.927725\n",
      "    Params: tensor([  5.3624, -17.2748])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3749, Loss 2.927723\n",
      "    Params: tensor([  5.3624, -17.2749])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3750, Loss 2.927724\n",
      "    Params: tensor([  5.3624, -17.2749])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3751, Loss 2.927724\n",
      "    Params: tensor([  5.3625, -17.2750])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3752, Loss 2.927725\n",
      "    Params: tensor([  5.3625, -17.2750])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3753, Loss 2.927724\n",
      "    Params: tensor([  5.3625, -17.2751])\n",
      "    Grad:   tensor([-0.0009,  0.0051])\n",
      "Epoch 3754, Loss 2.927724\n",
      "    Params: tensor([  5.3625, -17.2751])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3755, Loss 2.927723\n",
      "    Params: tensor([  5.3625, -17.2752])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3756, Loss 2.927723\n",
      "    Params: tensor([  5.3625, -17.2752])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3757, Loss 2.927723\n",
      "    Params: tensor([  5.3625, -17.2753])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3758, Loss 2.927722\n",
      "    Params: tensor([  5.3625, -17.2753])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3759, Loss 2.927723\n",
      "    Params: tensor([  5.3625, -17.2754])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3760, Loss 2.927722\n",
      "    Params: tensor([  5.3625, -17.2754])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3761, Loss 2.927723\n",
      "    Params: tensor([  5.3625, -17.2755])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3762, Loss 2.927721\n",
      "    Params: tensor([  5.3626, -17.2755])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3763, Loss 2.927722\n",
      "    Params: tensor([  5.3626, -17.2756])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3764, Loss 2.927720\n",
      "    Params: tensor([  5.3626, -17.2756])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3765, Loss 2.927720\n",
      "    Params: tensor([  5.3626, -17.2757])\n",
      "    Grad:   tensor([-0.0009,  0.0050])\n",
      "Epoch 3766, Loss 2.927719\n",
      "    Params: tensor([  5.3626, -17.2757])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3767, Loss 2.927721\n",
      "    Params: tensor([  5.3626, -17.2758])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3768, Loss 2.927719\n",
      "    Params: tensor([  5.3626, -17.2758])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3769, Loss 2.927719\n",
      "    Params: tensor([  5.3626, -17.2759])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3770, Loss 2.927719\n",
      "    Params: tensor([  5.3626, -17.2759])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3771, Loss 2.927719\n",
      "    Params: tensor([  5.3626, -17.2760])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3772, Loss 2.927719\n",
      "    Params: tensor([  5.3626, -17.2760])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3773, Loss 2.927720\n",
      "    Params: tensor([  5.3626, -17.2761])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3774, Loss 2.927718\n",
      "    Params: tensor([  5.3627, -17.2761])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3775, Loss 2.927718\n",
      "    Params: tensor([  5.3627, -17.2762])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3776, Loss 2.927717\n",
      "    Params: tensor([  5.3627, -17.2762])\n",
      "    Grad:   tensor([-0.0009,  0.0049])\n",
      "Epoch 3777, Loss 2.927718\n",
      "    Params: tensor([  5.3627, -17.2763])\n",
      "    Grad:   tensor([-0.0008,  0.0049])\n",
      "Epoch 3778, Loss 2.927717\n",
      "    Params: tensor([  5.3627, -17.2763])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3779, Loss 2.927717\n",
      "    Params: tensor([  5.3627, -17.2764])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3780, Loss 2.927716\n",
      "    Params: tensor([  5.3627, -17.2764])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3781, Loss 2.927716\n",
      "    Params: tensor([  5.3627, -17.2765])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3782, Loss 2.927717\n",
      "    Params: tensor([  5.3627, -17.2765])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3783, Loss 2.927717\n",
      "    Params: tensor([  5.3627, -17.2766])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3784, Loss 2.927716\n",
      "    Params: tensor([  5.3627, -17.2766])\n",
      "    Grad:   tensor([-0.0009,  0.0048])\n",
      "Epoch 3785, Loss 2.927715\n",
      "    Params: tensor([  5.3627, -17.2767])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3786, Loss 2.927715\n",
      "    Params: tensor([  5.3628, -17.2767])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3787, Loss 2.927715\n",
      "    Params: tensor([  5.3628, -17.2767])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3788, Loss 2.927715\n",
      "    Params: tensor([  5.3628, -17.2768])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3789, Loss 2.927715\n",
      "    Params: tensor([  5.3628, -17.2768])\n",
      "    Grad:   tensor([-0.0008,  0.0048])\n",
      "Epoch 3790, Loss 2.927715\n",
      "    Params: tensor([  5.3628, -17.2769])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3791, Loss 2.927714\n",
      "    Params: tensor([  5.3628, -17.2769])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3792, Loss 2.927714\n",
      "    Params: tensor([  5.3628, -17.2770])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3793, Loss 2.927714\n",
      "    Params: tensor([  5.3628, -17.2770])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3794, Loss 2.927714\n",
      "    Params: tensor([  5.3628, -17.2771])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3795, Loss 2.927713\n",
      "    Params: tensor([  5.3628, -17.2771])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3796, Loss 2.927714\n",
      "    Params: tensor([  5.3628, -17.2772])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3797, Loss 2.927713\n",
      "    Params: tensor([  5.3629, -17.2772])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3798, Loss 2.927712\n",
      "    Params: tensor([  5.3629, -17.2773])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3799, Loss 2.927712\n",
      "    Params: tensor([  5.3629, -17.2773])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3800, Loss 2.927713\n",
      "    Params: tensor([  5.3629, -17.2774])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3801, Loss 2.927711\n",
      "    Params: tensor([  5.3629, -17.2774])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3802, Loss 2.927712\n",
      "    Params: tensor([  5.3629, -17.2775])\n",
      "    Grad:   tensor([-0.0008,  0.0047])\n",
      "Epoch 3803, Loss 2.927712\n",
      "    Params: tensor([  5.3629, -17.2775])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3804, Loss 2.927711\n",
      "    Params: tensor([  5.3629, -17.2775])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3805, Loss 2.927712\n",
      "    Params: tensor([  5.3629, -17.2776])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3806, Loss 2.927711\n",
      "    Params: tensor([  5.3629, -17.2776])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3807, Loss 2.927711\n",
      "    Params: tensor([  5.3629, -17.2777])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3808, Loss 2.927711\n",
      "    Params: tensor([  5.3629, -17.2777])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3809, Loss 2.927709\n",
      "    Params: tensor([  5.3629, -17.2778])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3810, Loss 2.927710\n",
      "    Params: tensor([  5.3630, -17.2778])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3811, Loss 2.927710\n",
      "    Params: tensor([  5.3630, -17.2779])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3812, Loss 2.927708\n",
      "    Params: tensor([  5.3630, -17.2779])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3813, Loss 2.927708\n",
      "    Params: tensor([  5.3630, -17.2780])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3814, Loss 2.927709\n",
      "    Params: tensor([  5.3630, -17.2780])\n",
      "    Grad:   tensor([-0.0008,  0.0046])\n",
      "Epoch 3815, Loss 2.927709\n",
      "    Params: tensor([  5.3630, -17.2781])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3816, Loss 2.927710\n",
      "    Params: tensor([  5.3630, -17.2781])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3817, Loss 2.927708\n",
      "    Params: tensor([  5.3630, -17.2781])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3818, Loss 2.927708\n",
      "    Params: tensor([  5.3630, -17.2782])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3819, Loss 2.927706\n",
      "    Params: tensor([  5.3630, -17.2782])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3820, Loss 2.927707\n",
      "    Params: tensor([  5.3630, -17.2783])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3821, Loss 2.927708\n",
      "    Params: tensor([  5.3630, -17.2783])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3822, Loss 2.927707\n",
      "    Params: tensor([  5.3631, -17.2784])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3823, Loss 2.927707\n",
      "    Params: tensor([  5.3631, -17.2784])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3824, Loss 2.927707\n",
      "    Params: tensor([  5.3631, -17.2785])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3825, Loss 2.927708\n",
      "    Params: tensor([  5.3631, -17.2785])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3826, Loss 2.927707\n",
      "    Params: tensor([  5.3631, -17.2786])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3827, Loss 2.927706\n",
      "    Params: tensor([  5.3631, -17.2786])\n",
      "    Grad:   tensor([-0.0008,  0.0045])\n",
      "Epoch 3828, Loss 2.927707\n",
      "    Params: tensor([  5.3631, -17.2786])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3829, Loss 2.927705\n",
      "    Params: tensor([  5.3631, -17.2787])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3830, Loss 2.927706\n",
      "    Params: tensor([  5.3631, -17.2787])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3831, Loss 2.927706\n",
      "    Params: tensor([  5.3631, -17.2788])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3832, Loss 2.927705\n",
      "    Params: tensor([  5.3631, -17.2788])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3833, Loss 2.927705\n",
      "    Params: tensor([  5.3631, -17.2789])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3834, Loss 2.927705\n",
      "    Params: tensor([  5.3631, -17.2789])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3835, Loss 2.927705\n",
      "    Params: tensor([  5.3632, -17.2789])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3836, Loss 2.927705\n",
      "    Params: tensor([  5.3632, -17.2790])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3837, Loss 2.927705\n",
      "    Params: tensor([  5.3632, -17.2790])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3838, Loss 2.927704\n",
      "    Params: tensor([  5.3632, -17.2791])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3839, Loss 2.927704\n",
      "    Params: tensor([  5.3632, -17.2791])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3840, Loss 2.927704\n",
      "    Params: tensor([  5.3632, -17.2792])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3841, Loss 2.927703\n",
      "    Params: tensor([  5.3632, -17.2792])\n",
      "    Grad:   tensor([-0.0008,  0.0044])\n",
      "Epoch 3842, Loss 2.927702\n",
      "    Params: tensor([  5.3632, -17.2793])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3843, Loss 2.927703\n",
      "    Params: tensor([  5.3632, -17.2793])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3844, Loss 2.927703\n",
      "    Params: tensor([  5.3632, -17.2793])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3845, Loss 2.927704\n",
      "    Params: tensor([  5.3632, -17.2794])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3846, Loss 2.927702\n",
      "    Params: tensor([  5.3632, -17.2794])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3847, Loss 2.927701\n",
      "    Params: tensor([  5.3632, -17.2795])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3848, Loss 2.927703\n",
      "    Params: tensor([  5.3633, -17.2795])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3849, Loss 2.927702\n",
      "    Params: tensor([  5.3633, -17.2796])\n",
      "    Grad:   tensor([-0.0008,  0.0043])\n",
      "Epoch 3850, Loss 2.927701\n",
      "    Params: tensor([  5.3633, -17.2796])\n",
      "    Grad:   tensor([-0.0007,  0.0043])\n",
      "Epoch 3851, Loss 2.927701\n",
      "    Params: tensor([  5.3633, -17.2796])\n",
      "    Grad:   tensor([-0.0007,  0.0043])\n",
      "Epoch 3852, Loss 2.927703\n",
      "    Params: tensor([  5.3633, -17.2797])\n",
      "    Grad:   tensor([-0.0007,  0.0043])\n",
      "Epoch 3853, Loss 2.927700\n",
      "    Params: tensor([  5.3633, -17.2797])\n",
      "    Grad:   tensor([-0.0007,  0.0043])\n",
      "Epoch 3854, Loss 2.927701\n",
      "    Params: tensor([  5.3633, -17.2798])\n",
      "    Grad:   tensor([-0.0007,  0.0043])\n",
      "Epoch 3855, Loss 2.927701\n",
      "    Params: tensor([  5.3633, -17.2798])\n",
      "    Grad:   tensor([-0.0007,  0.0043])\n",
      "Epoch 3856, Loss 2.927700\n",
      "    Params: tensor([  5.3633, -17.2799])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3857, Loss 2.927700\n",
      "    Params: tensor([  5.3633, -17.2799])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3858, Loss 2.927700\n",
      "    Params: tensor([  5.3633, -17.2799])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3859, Loss 2.927701\n",
      "    Params: tensor([  5.3633, -17.2800])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3860, Loss 2.927699\n",
      "    Params: tensor([  5.3633, -17.2800])\n",
      "    Grad:   tensor([-0.0008,  0.0042])\n",
      "Epoch 3861, Loss 2.927699\n",
      "    Params: tensor([  5.3634, -17.2801])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3862, Loss 2.927700\n",
      "    Params: tensor([  5.3634, -17.2801])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3863, Loss 2.927699\n",
      "    Params: tensor([  5.3634, -17.2801])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3864, Loss 2.927698\n",
      "    Params: tensor([  5.3634, -17.2802])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3865, Loss 2.927699\n",
      "    Params: tensor([  5.3634, -17.2802])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3866, Loss 2.927697\n",
      "    Params: tensor([  5.3634, -17.2803])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3867, Loss 2.927700\n",
      "    Params: tensor([  5.3634, -17.2803])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3868, Loss 2.927699\n",
      "    Params: tensor([  5.3634, -17.2804])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3869, Loss 2.927698\n",
      "    Params: tensor([  5.3634, -17.2804])\n",
      "    Grad:   tensor([-0.0007,  0.0042])\n",
      "Epoch 3870, Loss 2.927697\n",
      "    Params: tensor([  5.3634, -17.2804])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3871, Loss 2.927698\n",
      "    Params: tensor([  5.3634, -17.2805])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3872, Loss 2.927696\n",
      "    Params: tensor([  5.3634, -17.2805])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3873, Loss 2.927699\n",
      "    Params: tensor([  5.3634, -17.2806])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3874, Loss 2.927698\n",
      "    Params: tensor([  5.3634, -17.2806])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3875, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2806])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3876, Loss 2.927698\n",
      "    Params: tensor([  5.3635, -17.2807])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3877, Loss 2.927697\n",
      "    Params: tensor([  5.3635, -17.2807])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3878, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2808])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3879, Loss 2.927697\n",
      "    Params: tensor([  5.3635, -17.2808])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3880, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2808])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3881, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2809])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3882, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2809])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3883, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2810])\n",
      "    Grad:   tensor([-0.0007,  0.0041])\n",
      "Epoch 3884, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2810])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3885, Loss 2.927695\n",
      "    Params: tensor([  5.3635, -17.2810])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3886, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2811])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3887, Loss 2.927696\n",
      "    Params: tensor([  5.3635, -17.2811])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3888, Loss 2.927695\n",
      "    Params: tensor([  5.3635, -17.2812])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3889, Loss 2.927695\n",
      "    Params: tensor([  5.3636, -17.2812])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3890, Loss 2.927694\n",
      "    Params: tensor([  5.3636, -17.2812])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3891, Loss 2.927693\n",
      "    Params: tensor([  5.3636, -17.2813])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3892, Loss 2.927693\n",
      "    Params: tensor([  5.3636, -17.2813])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3893, Loss 2.927695\n",
      "    Params: tensor([  5.3636, -17.2814])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3894, Loss 2.927695\n",
      "    Params: tensor([  5.3636, -17.2814])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3895, Loss 2.927694\n",
      "    Params: tensor([  5.3636, -17.2815])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3896, Loss 2.927696\n",
      "    Params: tensor([  5.3636, -17.2815])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3897, Loss 2.927693\n",
      "    Params: tensor([  5.3636, -17.2815])\n",
      "    Grad:   tensor([-0.0007,  0.0040])\n",
      "Epoch 3898, Loss 2.927693\n",
      "    Params: tensor([  5.3636, -17.2816])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3899, Loss 2.927694\n",
      "    Params: tensor([  5.3636, -17.2816])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3900, Loss 2.927693\n",
      "    Params: tensor([  5.3636, -17.2817])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3901, Loss 2.927692\n",
      "    Params: tensor([  5.3636, -17.2817])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3902, Loss 2.927694\n",
      "    Params: tensor([  5.3636, -17.2817])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3903, Loss 2.927692\n",
      "    Params: tensor([  5.3637, -17.2818])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3904, Loss 2.927693\n",
      "    Params: tensor([  5.3637, -17.2818])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3905, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2818])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3906, Loss 2.927692\n",
      "    Params: tensor([  5.3637, -17.2819])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3907, Loss 2.927692\n",
      "    Params: tensor([  5.3637, -17.2819])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3908, Loss 2.927692\n",
      "    Params: tensor([  5.3637, -17.2820])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3909, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2820])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3910, Loss 2.927692\n",
      "    Params: tensor([  5.3637, -17.2820])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3911, Loss 2.927690\n",
      "    Params: tensor([  5.3637, -17.2821])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3912, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2821])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3913, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2822])\n",
      "    Grad:   tensor([-0.0007,  0.0039])\n",
      "Epoch 3914, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2822])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3915, Loss 2.927690\n",
      "    Params: tensor([  5.3637, -17.2822])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3916, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2823])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3917, Loss 2.927691\n",
      "    Params: tensor([  5.3637, -17.2823])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3918, Loss 2.927689\n",
      "    Params: tensor([  5.3638, -17.2823])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3919, Loss 2.927690\n",
      "    Params: tensor([  5.3638, -17.2824])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3920, Loss 2.927690\n",
      "    Params: tensor([  5.3638, -17.2824])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3921, Loss 2.927690\n",
      "    Params: tensor([  5.3638, -17.2825])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3922, Loss 2.927690\n",
      "    Params: tensor([  5.3638, -17.2825])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3923, Loss 2.927689\n",
      "    Params: tensor([  5.3638, -17.2825])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3924, Loss 2.927689\n",
      "    Params: tensor([  5.3638, -17.2826])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3925, Loss 2.927689\n",
      "    Params: tensor([  5.3638, -17.2826])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3926, Loss 2.927688\n",
      "    Params: tensor([  5.3638, -17.2826])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3927, Loss 2.927689\n",
      "    Params: tensor([  5.3638, -17.2827])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3928, Loss 2.927689\n",
      "    Params: tensor([  5.3638, -17.2827])\n",
      "    Grad:   tensor([-0.0007,  0.0038])\n",
      "Epoch 3929, Loss 2.927688\n",
      "    Params: tensor([  5.3638, -17.2828])\n",
      "    Grad:   tensor([-0.0007,  0.0037])\n",
      "Epoch 3930, Loss 2.927688\n",
      "    Params: tensor([  5.3638, -17.2828])\n",
      "    Grad:   tensor([-0.0007,  0.0037])\n",
      "Epoch 3931, Loss 2.927688\n",
      "    Params: tensor([  5.3638, -17.2828])\n",
      "    Grad:   tensor([-0.0007,  0.0037])\n",
      "Epoch 3932, Loss 2.927688\n",
      "    Params: tensor([  5.3638, -17.2829])\n",
      "    Grad:   tensor([-0.0007,  0.0037])\n",
      "Epoch 3933, Loss 2.927687\n",
      "    Params: tensor([  5.3639, -17.2829])\n",
      "    Grad:   tensor([-0.0007,  0.0037])\n",
      "Epoch 3934, Loss 2.927689\n",
      "    Params: tensor([  5.3639, -17.2829])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3935, Loss 2.927688\n",
      "    Params: tensor([  5.3639, -17.2830])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3936, Loss 2.927687\n",
      "    Params: tensor([  5.3639, -17.2830])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3937, Loss 2.927687\n",
      "    Params: tensor([  5.3639, -17.2831])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3938, Loss 2.927687\n",
      "    Params: tensor([  5.3639, -17.2831])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3939, Loss 2.927686\n",
      "    Params: tensor([  5.3639, -17.2831])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3940, Loss 2.927686\n",
      "    Params: tensor([  5.3639, -17.2832])\n",
      "    Grad:   tensor([-0.0007,  0.0037])\n",
      "Epoch 3941, Loss 2.927687\n",
      "    Params: tensor([  5.3639, -17.2832])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3942, Loss 2.927686\n",
      "    Params: tensor([  5.3639, -17.2832])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3943, Loss 2.927686\n",
      "    Params: tensor([  5.3639, -17.2833])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3944, Loss 2.927686\n",
      "    Params: tensor([  5.3639, -17.2833])\n",
      "    Grad:   tensor([-0.0006,  0.0037])\n",
      "Epoch 3945, Loss 2.927686\n",
      "    Params: tensor([  5.3639, -17.2833])\n",
      "    Grad:   tensor([-0.0007,  0.0036])\n",
      "Epoch 3946, Loss 2.927685\n",
      "    Params: tensor([  5.3639, -17.2834])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3947, Loss 2.927685\n",
      "    Params: tensor([  5.3639, -17.2834])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3948, Loss 2.927686\n",
      "    Params: tensor([  5.3640, -17.2835])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3949, Loss 2.927685\n",
      "    Params: tensor([  5.3640, -17.2835])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3950, Loss 2.927686\n",
      "    Params: tensor([  5.3640, -17.2835])\n",
      "    Grad:   tensor([-0.0007,  0.0036])\n",
      "Epoch 3951, Loss 2.927686\n",
      "    Params: tensor([  5.3640, -17.2836])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3952, Loss 2.927687\n",
      "    Params: tensor([  5.3640, -17.2836])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3953, Loss 2.927685\n",
      "    Params: tensor([  5.3640, -17.2836])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3954, Loss 2.927686\n",
      "    Params: tensor([  5.3640, -17.2837])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3955, Loss 2.927686\n",
      "    Params: tensor([  5.3640, -17.2837])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3956, Loss 2.927685\n",
      "    Params: tensor([  5.3640, -17.2837])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3957, Loss 2.927683\n",
      "    Params: tensor([  5.3640, -17.2838])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3958, Loss 2.927684\n",
      "    Params: tensor([  5.3640, -17.2838])\n",
      "    Grad:   tensor([-0.0007,  0.0036])\n",
      "Epoch 3959, Loss 2.927685\n",
      "    Params: tensor([  5.3640, -17.2839])\n",
      "    Grad:   tensor([-0.0006,  0.0036])\n",
      "Epoch 3960, Loss 2.927684\n",
      "    Params: tensor([  5.3640, -17.2839])\n",
      "    Grad:   tensor([-0.0007,  0.0036])\n",
      "Epoch 3961, Loss 2.927684\n",
      "    Params: tensor([  5.3640, -17.2839])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3962, Loss 2.927684\n",
      "    Params: tensor([  5.3640, -17.2840])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3963, Loss 2.927685\n",
      "    Params: tensor([  5.3640, -17.2840])\n",
      "    Grad:   tensor([-0.0007,  0.0035])\n",
      "Epoch 3964, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2840])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3965, Loss 2.927685\n",
      "    Params: tensor([  5.3641, -17.2841])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3966, Loss 2.927685\n",
      "    Params: tensor([  5.3641, -17.2841])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3967, Loss 2.927684\n",
      "    Params: tensor([  5.3641, -17.2841])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3968, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2842])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3969, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2842])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3970, Loss 2.927682\n",
      "    Params: tensor([  5.3641, -17.2842])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3971, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2843])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3972, Loss 2.927684\n",
      "    Params: tensor([  5.3641, -17.2843])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3973, Loss 2.927682\n",
      "    Params: tensor([  5.3641, -17.2843])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3974, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2844])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3975, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2844])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3976, Loss 2.927683\n",
      "    Params: tensor([  5.3641, -17.2844])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3977, Loss 2.927682\n",
      "    Params: tensor([  5.3641, -17.2845])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3978, Loss 2.927682\n",
      "    Params: tensor([  5.3641, -17.2845])\n",
      "    Grad:   tensor([-0.0006,  0.0035])\n",
      "Epoch 3979, Loss 2.927682\n",
      "    Params: tensor([  5.3641, -17.2845])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3980, Loss 2.927681\n",
      "    Params: tensor([  5.3642, -17.2846])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3981, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2846])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3982, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2847])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3983, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2847])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3984, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2847])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3985, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2848])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3986, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2848])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3987, Loss 2.927680\n",
      "    Params: tensor([  5.3642, -17.2848])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3988, Loss 2.927682\n",
      "    Params: tensor([  5.3642, -17.2849])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3989, Loss 2.927681\n",
      "    Params: tensor([  5.3642, -17.2849])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3990, Loss 2.927681\n",
      "    Params: tensor([  5.3642, -17.2849])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3991, Loss 2.927680\n",
      "    Params: tensor([  5.3642, -17.2850])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3992, Loss 2.927681\n",
      "    Params: tensor([  5.3642, -17.2850])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3993, Loss 2.927680\n",
      "    Params: tensor([  5.3642, -17.2850])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3994, Loss 2.927680\n",
      "    Params: tensor([  5.3642, -17.2851])\n",
      "    Grad:   tensor([-0.0006,  0.0034])\n",
      "Epoch 3995, Loss 2.927681\n",
      "    Params: tensor([  5.3642, -17.2851])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 3996, Loss 2.927681\n",
      "    Params: tensor([  5.3642, -17.2851])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 3997, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2852])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 3998, Loss 2.927680\n",
      "    Params: tensor([  5.3643, -17.2852])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 3999, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2852])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4000, Loss 2.927680\n",
      "    Params: tensor([  5.3643, -17.2853])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4001, Loss 2.927680\n",
      "    Params: tensor([  5.3643, -17.2853])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4002, Loss 2.927681\n",
      "    Params: tensor([  5.3643, -17.2853])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4003, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2854])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4004, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2854])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4005, Loss 2.927680\n",
      "    Params: tensor([  5.3643, -17.2854])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4006, Loss 2.927680\n",
      "    Params: tensor([  5.3643, -17.2855])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4007, Loss 2.927677\n",
      "    Params: tensor([  5.3643, -17.2855])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4008, Loss 2.927678\n",
      "    Params: tensor([  5.3643, -17.2855])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4009, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2856])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4010, Loss 2.927678\n",
      "    Params: tensor([  5.3643, -17.2856])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4011, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2856])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4012, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2857])\n",
      "    Grad:   tensor([-0.0006,  0.0033])\n",
      "Epoch 4013, Loss 2.927679\n",
      "    Params: tensor([  5.3643, -17.2857])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4014, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2857])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4015, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2857])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4016, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2858])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4017, Loss 2.927679\n",
      "    Params: tensor([  5.3644, -17.2858])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4018, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2858])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4019, Loss 2.927678\n",
      "    Params: tensor([  5.3644, -17.2859])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4020, Loss 2.927678\n",
      "    Params: tensor([  5.3644, -17.2859])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4021, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2859])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4022, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2860])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4023, Loss 2.927678\n",
      "    Params: tensor([  5.3644, -17.2860])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4024, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2860])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4025, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2861])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4026, Loss 2.927676\n",
      "    Params: tensor([  5.3644, -17.2861])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4027, Loss 2.927676\n",
      "    Params: tensor([  5.3644, -17.2861])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4028, Loss 2.927675\n",
      "    Params: tensor([  5.3644, -17.2862])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4029, Loss 2.927677\n",
      "    Params: tensor([  5.3644, -17.2862])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4030, Loss 2.927674\n",
      "    Params: tensor([  5.3644, -17.2862])\n",
      "    Grad:   tensor([-0.0006,  0.0032])\n",
      "Epoch 4031, Loss 2.927676\n",
      "    Params: tensor([  5.3644, -17.2863])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4032, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2863])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4033, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2863])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4034, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2864])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4035, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2864])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4036, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2864])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4037, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2865])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4038, Loss 2.927677\n",
      "    Params: tensor([  5.3645, -17.2865])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4039, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2865])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4040, Loss 2.927676\n",
      "    Params: tensor([  5.3645, -17.2865])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4041, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2866])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4042, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2866])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4043, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2866])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4044, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2867])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4045, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2867])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4046, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2867])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4047, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2868])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4048, Loss 2.927674\n",
      "    Params: tensor([  5.3645, -17.2868])\n",
      "    Grad:   tensor([-0.0006,  0.0031])\n",
      "Epoch 4049, Loss 2.927675\n",
      "    Params: tensor([  5.3645, -17.2868])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4050, Loss 2.927672\n",
      "    Params: tensor([  5.3646, -17.2868])\n",
      "    Grad:   tensor([-0.0005,  0.0031])\n",
      "Epoch 4051, Loss 2.927675\n",
      "    Params: tensor([  5.3646, -17.2869])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4052, Loss 2.927675\n",
      "    Params: tensor([  5.3646, -17.2869])\n",
      "    Grad:   tensor([-0.0006,  0.0030])\n",
      "Epoch 4053, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2869])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4054, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2870])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4055, Loss 2.927674\n",
      "    Params: tensor([  5.3646, -17.2870])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4056, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2870])\n",
      "    Grad:   tensor([-0.0006,  0.0030])\n",
      "Epoch 4057, Loss 2.927674\n",
      "    Params: tensor([  5.3646, -17.2871])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4058, Loss 2.927672\n",
      "    Params: tensor([  5.3646, -17.2871])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4059, Loss 2.927674\n",
      "    Params: tensor([  5.3646, -17.2871])\n",
      "    Grad:   tensor([-0.0006,  0.0030])\n",
      "Epoch 4060, Loss 2.927675\n",
      "    Params: tensor([  5.3646, -17.2872])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4061, Loss 2.927672\n",
      "    Params: tensor([  5.3646, -17.2872])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4062, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2872])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4063, Loss 2.927675\n",
      "    Params: tensor([  5.3646, -17.2872])\n",
      "    Grad:   tensor([-0.0006,  0.0030])\n",
      "Epoch 4064, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2873])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4065, Loss 2.927674\n",
      "    Params: tensor([  5.3646, -17.2873])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4066, Loss 2.927672\n",
      "    Params: tensor([  5.3646, -17.2873])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4067, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2874])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4068, Loss 2.927673\n",
      "    Params: tensor([  5.3646, -17.2874])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4069, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2874])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4070, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2875])\n",
      "    Grad:   tensor([-0.0005,  0.0030])\n",
      "Epoch 4071, Loss 2.927673\n",
      "    Params: tensor([  5.3647, -17.2875])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4072, Loss 2.927673\n",
      "    Params: tensor([  5.3647, -17.2875])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4073, Loss 2.927671\n",
      "    Params: tensor([  5.3647, -17.2875])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4074, Loss 2.927673\n",
      "    Params: tensor([  5.3647, -17.2876])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4075, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2876])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4076, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2876])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4077, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2877])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4078, Loss 2.927670\n",
      "    Params: tensor([  5.3647, -17.2877])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4079, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2877])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4080, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2877])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4081, Loss 2.927671\n",
      "    Params: tensor([  5.3647, -17.2878])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4082, Loss 2.927670\n",
      "    Params: tensor([  5.3647, -17.2878])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4083, Loss 2.927673\n",
      "    Params: tensor([  5.3647, -17.2878])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4084, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2879])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4085, Loss 2.927670\n",
      "    Params: tensor([  5.3647, -17.2879])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4086, Loss 2.927670\n",
      "    Params: tensor([  5.3647, -17.2879])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4087, Loss 2.927670\n",
      "    Params: tensor([  5.3647, -17.2879])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4088, Loss 2.927672\n",
      "    Params: tensor([  5.3647, -17.2880])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4089, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2880])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4090, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2880])\n",
      "    Grad:   tensor([-0.0005,  0.0029])\n",
      "Epoch 4091, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2881])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4092, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2881])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4093, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2881])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4094, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2881])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4095, Loss 2.927669\n",
      "    Params: tensor([  5.3648, -17.2882])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4096, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2882])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4097, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2882])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4098, Loss 2.927671\n",
      "    Params: tensor([  5.3648, -17.2883])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4099, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2883])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4100, Loss 2.927671\n",
      "    Params: tensor([  5.3648, -17.2883])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4101, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2883])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4102, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2884])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4103, Loss 2.927671\n",
      "    Params: tensor([  5.3648, -17.2884])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4104, Loss 2.927669\n",
      "    Params: tensor([  5.3648, -17.2884])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4105, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2885])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4106, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2885])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4107, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2885])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4108, Loss 2.927670\n",
      "    Params: tensor([  5.3648, -17.2885])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4109, Loss 2.927668\n",
      "    Params: tensor([  5.3649, -17.2886])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4110, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2886])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4111, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2886])\n",
      "    Grad:   tensor([-0.0005,  0.0028])\n",
      "Epoch 4112, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2886])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4113, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2887])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4114, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2887])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4115, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2887])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4116, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2887])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4117, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2888])\n",
      "    Grad:   tensor([-0.0004,  0.0027])\n",
      "Epoch 4118, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2888])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4119, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2888])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4120, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2889])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4121, Loss 2.927668\n",
      "    Params: tensor([  5.3649, -17.2889])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4122, Loss 2.927668\n",
      "    Params: tensor([  5.3649, -17.2889])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4123, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2889])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4124, Loss 2.927668\n",
      "    Params: tensor([  5.3649, -17.2890])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4125, Loss 2.927670\n",
      "    Params: tensor([  5.3649, -17.2890])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4126, Loss 2.927666\n",
      "    Params: tensor([  5.3649, -17.2890])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4127, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2890])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4128, Loss 2.927668\n",
      "    Params: tensor([  5.3649, -17.2891])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4129, Loss 2.927669\n",
      "    Params: tensor([  5.3649, -17.2891])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4130, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2891])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4131, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2892])\n",
      "    Grad:   tensor([-0.0004,  0.0027])\n",
      "Epoch 4132, Loss 2.927668\n",
      "    Params: tensor([  5.3650, -17.2892])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4133, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2892])\n",
      "    Grad:   tensor([-0.0005,  0.0027])\n",
      "Epoch 4134, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2892])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4135, Loss 2.927666\n",
      "    Params: tensor([  5.3650, -17.2893])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4136, Loss 2.927666\n",
      "    Params: tensor([  5.3650, -17.2893])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4137, Loss 2.927669\n",
      "    Params: tensor([  5.3650, -17.2893])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4138, Loss 2.927666\n",
      "    Params: tensor([  5.3650, -17.2893])\n",
      "    Grad:   tensor([-0.0004,  0.0026])\n",
      "Epoch 4139, Loss 2.927668\n",
      "    Params: tensor([  5.3650, -17.2894])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4140, Loss 2.927666\n",
      "    Params: tensor([  5.3650, -17.2894])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4141, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2894])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4142, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2894])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4143, Loss 2.927666\n",
      "    Params: tensor([  5.3650, -17.2895])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4144, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2895])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4145, Loss 2.927666\n",
      "    Params: tensor([  5.3650, -17.2895])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4146, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2896])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4147, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2896])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4148, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2896])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4149, Loss 2.927667\n",
      "    Params: tensor([  5.3650, -17.2896])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4150, Loss 2.927665\n",
      "    Params: tensor([  5.3650, -17.2897])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4151, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2897])\n",
      "    Grad:   tensor([-0.0004,  0.0026])\n",
      "Epoch 4152, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2897])\n",
      "    Grad:   tensor([-0.0004,  0.0026])\n",
      "Epoch 4153, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2897])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4154, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2898])\n",
      "    Grad:   tensor([-0.0005,  0.0026])\n",
      "Epoch 4155, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2898])\n",
      "    Grad:   tensor([-0.0004,  0.0026])\n",
      "Epoch 4156, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2898])\n",
      "    Grad:   tensor([-0.0004,  0.0026])\n",
      "Epoch 4157, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2898])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4158, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2899])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4159, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2899])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4160, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2899])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4161, Loss 2.927664\n",
      "    Params: tensor([  5.3651, -17.2899])\n",
      "    Grad:   tensor([-0.0005,  0.0025])\n",
      "Epoch 4162, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2900])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4163, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2900])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4164, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2900])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4165, Loss 2.927664\n",
      "    Params: tensor([  5.3651, -17.2900])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4166, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2901])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4167, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2901])\n",
      "    Grad:   tensor([-0.0005,  0.0025])\n",
      "Epoch 4168, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2901])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4169, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2901])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4170, Loss 2.927664\n",
      "    Params: tensor([  5.3651, -17.2902])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4171, Loss 2.927665\n",
      "    Params: tensor([  5.3651, -17.2902])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4172, Loss 2.927666\n",
      "    Params: tensor([  5.3651, -17.2902])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4173, Loss 2.927663\n",
      "    Params: tensor([  5.3651, -17.2902])\n",
      "    Grad:   tensor([-0.0005,  0.0025])\n",
      "Epoch 4174, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2903])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4175, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2903])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4176, Loss 2.927665\n",
      "    Params: tensor([  5.3652, -17.2903])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4177, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2903])\n",
      "    Grad:   tensor([-0.0004,  0.0025])\n",
      "Epoch 4178, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2903])\n",
      "    Grad:   tensor([-0.0005,  0.0025])\n",
      "Epoch 4179, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2904])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4180, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2904])\n",
      "    Grad:   tensor([-0.0005,  0.0024])\n",
      "Epoch 4181, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2904])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4182, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2904])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4183, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2905])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4184, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2905])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4185, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2905])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4186, Loss 2.927662\n",
      "    Params: tensor([  5.3652, -17.2905])\n",
      "    Grad:   tensor([-0.0005,  0.0024])\n",
      "Epoch 4187, Loss 2.927665\n",
      "    Params: tensor([  5.3652, -17.2906])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4188, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2906])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4189, Loss 2.927662\n",
      "    Params: tensor([  5.3652, -17.2906])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4190, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2906])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4191, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2907])\n",
      "    Grad:   tensor([-0.0005,  0.0024])\n",
      "Epoch 4192, Loss 2.927664\n",
      "    Params: tensor([  5.3652, -17.2907])\n",
      "    Grad:   tensor([-0.0005,  0.0024])\n",
      "Epoch 4193, Loss 2.927662\n",
      "    Params: tensor([  5.3652, -17.2907])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4194, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2907])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4195, Loss 2.927663\n",
      "    Params: tensor([  5.3652, -17.2908])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4196, Loss 2.927665\n",
      "    Params: tensor([  5.3652, -17.2908])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4197, Loss 2.927664\n",
      "    Params: tensor([  5.3653, -17.2908])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4198, Loss 2.927663\n",
      "    Params: tensor([  5.3653, -17.2908])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4199, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2909])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4200, Loss 2.927664\n",
      "    Params: tensor([  5.3653, -17.2909])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4201, Loss 2.927663\n",
      "    Params: tensor([  5.3653, -17.2909])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4202, Loss 2.927661\n",
      "    Params: tensor([  5.3653, -17.2909])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4203, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2910])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4204, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2910])\n",
      "    Grad:   tensor([-0.0004,  0.0024])\n",
      "Epoch 4205, Loss 2.927663\n",
      "    Params: tensor([  5.3653, -17.2910])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4206, Loss 2.927663\n",
      "    Params: tensor([  5.3653, -17.2910])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4207, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2910])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4208, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2911])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4209, Loss 2.927663\n",
      "    Params: tensor([  5.3653, -17.2911])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4210, Loss 2.927664\n",
      "    Params: tensor([  5.3653, -17.2911])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4211, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2911])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4212, Loss 2.927660\n",
      "    Params: tensor([  5.3653, -17.2912])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4213, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2912])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4214, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2912])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4215, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2912])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4216, Loss 2.927661\n",
      "    Params: tensor([  5.3653, -17.2913])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4217, Loss 2.927660\n",
      "    Params: tensor([  5.3653, -17.2913])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4218, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2913])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4219, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2913])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4220, Loss 2.927663\n",
      "    Params: tensor([  5.3653, -17.2913])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4221, Loss 2.927662\n",
      "    Params: tensor([  5.3653, -17.2914])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4222, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2914])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4223, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2914])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4224, Loss 2.927663\n",
      "    Params: tensor([  5.3654, -17.2914])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4225, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2915])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4226, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2915])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4227, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2915])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4228, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2915])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4229, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2915])\n",
      "    Grad:   tensor([-0.0004,  0.0023])\n",
      "Epoch 4230, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2916])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4231, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2916])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4232, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2916])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4233, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2916])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4234, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2917])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4235, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2917])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4236, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2917])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4237, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2917])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4238, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2918])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4239, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2918])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4240, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2918])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4241, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2918])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4242, Loss 2.927660\n",
      "    Params: tensor([  5.3654, -17.2918])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4243, Loss 2.927659\n",
      "    Params: tensor([  5.3654, -17.2919])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4244, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2919])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4245, Loss 2.927661\n",
      "    Params: tensor([  5.3654, -17.2919])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4246, Loss 2.927662\n",
      "    Params: tensor([  5.3654, -17.2919])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4247, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2920])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4248, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2920])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4249, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2920])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4250, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2920])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4251, Loss 2.927662\n",
      "    Params: tensor([  5.3655, -17.2920])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4252, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2921])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4253, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2921])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4254, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2921])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4255, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2921])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4256, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2921])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4257, Loss 2.927661\n",
      "    Params: tensor([  5.3655, -17.2922])\n",
      "    Grad:   tensor([-0.0004,  0.0022])\n",
      "Epoch 4258, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2922])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4259, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2922])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4260, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2922])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4261, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2922])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4262, Loss 2.927662\n",
      "    Params: tensor([  5.3655, -17.2923])\n",
      "    Grad:   tensor([-0.0003,  0.0021])\n",
      "Epoch 4263, Loss 2.927658\n",
      "    Params: tensor([  5.3655, -17.2923])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4264, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2923])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4265, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2923])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4266, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2924])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4267, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2924])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4268, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2924])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4269, Loss 2.927659\n",
      "    Params: tensor([  5.3655, -17.2924])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4270, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2924])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4271, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2925])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4272, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2925])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4273, Loss 2.927660\n",
      "    Params: tensor([  5.3655, -17.2925])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4274, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2925])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4275, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2925])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4276, Loss 2.927660\n",
      "    Params: tensor([  5.3656, -17.2926])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4277, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2926])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4278, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2926])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4279, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2926])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4280, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2926])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4281, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2927])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4282, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2927])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4283, Loss 2.927660\n",
      "    Params: tensor([  5.3656, -17.2927])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4284, Loss 2.927660\n",
      "    Params: tensor([  5.3656, -17.2927])\n",
      "    Grad:   tensor([-0.0004,  0.0021])\n",
      "Epoch 4285, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2927])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4286, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2928])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4287, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2928])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4288, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2928])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4289, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2928])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4290, Loss 2.927657\n",
      "    Params: tensor([  5.3656, -17.2929])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4291, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2929])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4292, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2929])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4293, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2929])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4294, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2929])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4295, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2930])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4296, Loss 2.927657\n",
      "    Params: tensor([  5.3656, -17.2930])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4297, Loss 2.927657\n",
      "    Params: tensor([  5.3656, -17.2930])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4298, Loss 2.927657\n",
      "    Params: tensor([  5.3656, -17.2930])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4299, Loss 2.927658\n",
      "    Params: tensor([  5.3656, -17.2930])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4300, Loss 2.927659\n",
      "    Params: tensor([  5.3656, -17.2931])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4301, Loss 2.927660\n",
      "    Params: tensor([  5.3657, -17.2931])\n",
      "    Grad:   tensor([-0.0004,  0.0020])\n",
      "Epoch 4302, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2931])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4303, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2931])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4304, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2931])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4305, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2932])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4306, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2932])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4307, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2932])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4308, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2932])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4309, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2932])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4310, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2932])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4311, Loss 2.927660\n",
      "    Params: tensor([  5.3657, -17.2933])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4312, Loss 2.927659\n",
      "    Params: tensor([  5.3657, -17.2933])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4313, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2933])\n",
      "    Grad:   tensor([-0.0003,  0.0020])\n",
      "Epoch 4314, Loss 2.927656\n",
      "    Params: tensor([  5.3657, -17.2933])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4315, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2933])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4316, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2934])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4317, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2934])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4318, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2934])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4319, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2934])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4320, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2934])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4321, Loss 2.927656\n",
      "    Params: tensor([  5.3657, -17.2935])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4322, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2935])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4323, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2935])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4324, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2935])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4325, Loss 2.927658\n",
      "    Params: tensor([  5.3657, -17.2935])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4326, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2936])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4327, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2936])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4328, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2936])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4329, Loss 2.927656\n",
      "    Params: tensor([  5.3657, -17.2936])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4330, Loss 2.927657\n",
      "    Params: tensor([  5.3657, -17.2936])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4331, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2936])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4332, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2937])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4333, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2937])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4334, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2937])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4335, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2937])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4336, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2937])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4337, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2938])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4338, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2938])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4339, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2938])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4340, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2938])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4341, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2938])\n",
      "    Grad:   tensor([-0.0003,  0.0019])\n",
      "Epoch 4342, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2939])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4343, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2939])\n",
      "    Grad:   tensor([-0.0004,  0.0019])\n",
      "Epoch 4344, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2939])\n",
      "    Grad:   tensor([-0.0004,  0.0018])\n",
      "Epoch 4345, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2939])\n",
      "    Grad:   tensor([-0.0004,  0.0018])\n",
      "Epoch 4346, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2939])\n",
      "    Grad:   tensor([-0.0004,  0.0018])\n",
      "Epoch 4347, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2940])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4348, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2940])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4349, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2940])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4350, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2940])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4351, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2940])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4352, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2941])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4353, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2941])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4354, Loss 2.927655\n",
      "    Params: tensor([  5.3658, -17.2941])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4355, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2941])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4356, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2941])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4357, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2941])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4358, Loss 2.927657\n",
      "    Params: tensor([  5.3658, -17.2942])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4359, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2942])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4360, Loss 2.927656\n",
      "    Params: tensor([  5.3658, -17.2942])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4361, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2942])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4362, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2942])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4363, Loss 2.927657\n",
      "    Params: tensor([  5.3659, -17.2942])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4364, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2943])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4365, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2943])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4366, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2943])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4367, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2943])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4368, Loss 2.927654\n",
      "    Params: tensor([  5.3659, -17.2943])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4369, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2943])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4370, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2944])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4371, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2944])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4372, Loss 2.927657\n",
      "    Params: tensor([  5.3659, -17.2944])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4373, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2944])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4374, Loss 2.927657\n",
      "    Params: tensor([  5.3659, -17.2944])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4375, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2945])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4376, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2945])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4377, Loss 2.927654\n",
      "    Params: tensor([  5.3659, -17.2945])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4378, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2945])\n",
      "    Grad:   tensor([-0.0003,  0.0018])\n",
      "Epoch 4379, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2945])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4380, Loss 2.927654\n",
      "    Params: tensor([  5.3659, -17.2945])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4381, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2946])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4382, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2946])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4383, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2946])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4384, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2946])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4385, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2946])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4386, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2946])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4387, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2947])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4388, Loss 2.927653\n",
      "    Params: tensor([  5.3659, -17.2947])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4389, Loss 2.927654\n",
      "    Params: tensor([  5.3659, -17.2947])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4390, Loss 2.927654\n",
      "    Params: tensor([  5.3659, -17.2947])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4391, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2947])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4392, Loss 2.927656\n",
      "    Params: tensor([  5.3659, -17.2947])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4393, Loss 2.927655\n",
      "    Params: tensor([  5.3659, -17.2948])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4394, Loss 2.927656\n",
      "    Params: tensor([  5.3660, -17.2948])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4395, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2948])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4396, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2948])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4397, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2948])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4398, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2948])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4399, Loss 2.927656\n",
      "    Params: tensor([  5.3660, -17.2949])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4400, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2949])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4401, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2949])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4402, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2949])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4403, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2949])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4404, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2949])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4405, Loss 2.927656\n",
      "    Params: tensor([  5.3660, -17.2950])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4406, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2950])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4407, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2950])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4408, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2950])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4409, Loss 2.927653\n",
      "    Params: tensor([  5.3660, -17.2950])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4410, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2951])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4411, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2951])\n",
      "    Grad:   tensor([-0.0003,  0.0017])\n",
      "Epoch 4412, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2951])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4413, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2951])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4414, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2951])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4415, Loss 2.927653\n",
      "    Params: tensor([  5.3660, -17.2951])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4416, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2952])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4417, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2952])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4418, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2952])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4419, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2952])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4420, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2952])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4421, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2952])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4422, Loss 2.927653\n",
      "    Params: tensor([  5.3660, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4423, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4424, Loss 2.927653\n",
      "    Params: tensor([  5.3660, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4425, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4426, Loss 2.927655\n",
      "    Params: tensor([  5.3660, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4427, Loss 2.927654\n",
      "    Params: tensor([  5.3660, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4428, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2953])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4429, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2954])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4430, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2954])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4431, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2954])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4432, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2954])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4433, Loss 2.927655\n",
      "    Params: tensor([  5.3661, -17.2954])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4434, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2954])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4435, Loss 2.927655\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4436, Loss 2.927652\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4437, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4438, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4439, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4440, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4441, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2955])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4442, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2956])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4443, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2956])\n",
      "    Grad:   tensor([-0.0002,  0.0016])\n",
      "Epoch 4444, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2956])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4445, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2956])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4446, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2956])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4447, Loss 2.927652\n",
      "    Params: tensor([  5.3661, -17.2956])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4448, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4449, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0016])\n",
      "Epoch 4450, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4451, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4452, Loss 2.927651\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4453, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4454, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2957])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4455, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2958])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4456, Loss 2.927655\n",
      "    Params: tensor([  5.3661, -17.2958])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4457, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2958])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4458, Loss 2.927652\n",
      "    Params: tensor([  5.3661, -17.2958])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4459, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2958])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4460, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2958])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4461, Loss 2.927654\n",
      "    Params: tensor([  5.3661, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4462, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4463, Loss 2.927652\n",
      "    Params: tensor([  5.3661, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4464, Loss 2.927653\n",
      "    Params: tensor([  5.3661, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4465, Loss 2.927654\n",
      "    Params: tensor([  5.3662, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4466, Loss 2.927654\n",
      "    Params: tensor([  5.3662, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4467, Loss 2.927654\n",
      "    Params: tensor([  5.3662, -17.2959])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4468, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4469, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4470, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4471, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4472, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4473, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4474, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2960])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4475, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2961])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4476, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2961])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4477, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2961])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4478, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2961])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4479, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2961])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4480, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2961])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4481, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4482, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4483, Loss 2.927654\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4484, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4485, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4486, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4487, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2962])\n",
      "    Grad:   tensor([-0.0003,  0.0015])\n",
      "Epoch 4488, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2963])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4489, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2963])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4490, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2963])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4491, Loss 2.927651\n",
      "    Params: tensor([  5.3662, -17.2963])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4492, Loss 2.927651\n",
      "    Params: tensor([  5.3662, -17.2963])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4493, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2963])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4494, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4495, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4496, Loss 2.927651\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0003,  0.0014])\n",
      "Epoch 4497, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4498, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4499, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4500, Loss 2.927651\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4501, Loss 2.927652\n",
      "    Params: tensor([  5.3662, -17.2964])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4502, Loss 2.927653\n",
      "    Params: tensor([  5.3662, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4503, Loss 2.927653\n",
      "    Params: tensor([  5.3663, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4504, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4505, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4506, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4507, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4508, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2965])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4509, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4510, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4511, Loss 2.927650\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4512, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4513, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4514, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4515, Loss 2.927650\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4516, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2966])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4517, Loss 2.927653\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4518, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4519, Loss 2.927650\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4520, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4521, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4522, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4523, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2967])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4524, Loss 2.927653\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4525, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4526, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4527, Loss 2.927653\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4528, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4529, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4530, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2968])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4531, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0014])\n",
      "Epoch 4532, Loss 2.927650\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4533, Loss 2.927652\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4534, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4535, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4536, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4537, Loss 2.927653\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4538, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2969])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4539, Loss 2.927650\n",
      "    Params: tensor([  5.3663, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4540, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4541, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4542, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4543, Loss 2.927651\n",
      "    Params: tensor([  5.3663, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4544, Loss 2.927650\n",
      "    Params: tensor([  5.3663, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4545, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2970])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4546, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4547, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4548, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4549, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4550, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4551, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4552, Loss 2.927653\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4553, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2971])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4554, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4555, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4556, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4557, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4558, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4559, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4560, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2972])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4561, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4562, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4563, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4564, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4565, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4566, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4567, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4568, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2973])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4569, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4570, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4571, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4572, Loss 2.927649\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4573, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0013])\n",
      "Epoch 4574, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4575, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2974])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4576, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4577, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4578, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4579, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4580, Loss 2.927652\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4581, Loss 2.927649\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4582, Loss 2.927653\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4583, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2975])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4584, Loss 2.927649\n",
      "    Params: tensor([  5.3664, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4585, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4586, Loss 2.927650\n",
      "    Params: tensor([  5.3664, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4587, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4588, Loss 2.927651\n",
      "    Params: tensor([  5.3664, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4589, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4590, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4591, Loss 2.927652\n",
      "    Params: tensor([  5.3665, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4592, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2976])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4593, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4594, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4595, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4596, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4597, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4598, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4599, Loss 2.927652\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4600, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4601, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2977])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4602, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4603, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4604, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4605, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4606, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4607, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4608, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4609, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4610, Loss 2.927649\n",
      "    Params: tensor([  5.3665, -17.2978])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4611, Loss 2.927649\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4612, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4613, Loss 2.927649\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4614, Loss 2.927649\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4615, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4616, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4617, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4618, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2979])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4619, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4620, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4621, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4622, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4623, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4624, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4625, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4626, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0012])\n",
      "Epoch 4627, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2980])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4628, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4629, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4630, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4631, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4632, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4633, Loss 2.927651\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4634, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4635, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4636, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2981])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4637, Loss 2.927649\n",
      "    Params: tensor([  5.3665, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4638, Loss 2.927650\n",
      "    Params: tensor([  5.3665, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4639, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4640, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4641, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4642, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4643, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4644, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4645, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2982])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4646, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4647, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4648, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4649, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4650, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4651, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4652, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4653, Loss 2.927651\n",
      "    Params: tensor([  5.3666, -17.2983])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4654, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4655, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4656, Loss 2.927651\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4657, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4658, Loss 2.927651\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4659, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4660, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4661, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4662, Loss 2.927648\n",
      "    Params: tensor([  5.3666, -17.2984])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4663, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4664, Loss 2.927648\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4665, Loss 2.927648\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4666, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4667, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4668, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4669, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4670, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4671, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2985])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4672, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4673, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4674, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4675, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0011])\n",
      "Epoch 4676, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4677, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4678, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4679, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4680, Loss 2.927648\n",
      "    Params: tensor([  5.3666, -17.2986])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4681, Loss 2.927648\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4682, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4683, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4684, Loss 2.927648\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4685, Loss 2.927650\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4686, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4687, Loss 2.927651\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4688, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4689, Loss 2.927649\n",
      "    Params: tensor([  5.3666, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4690, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4691, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2987])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4692, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4693, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4694, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4695, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4696, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4697, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4698, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4699, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4700, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4701, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2988])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4702, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4703, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4704, Loss 2.927647\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4705, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4706, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4707, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4708, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4709, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4710, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4711, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4712, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2989])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4713, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4714, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4715, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4716, Loss 2.927647\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4717, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4718, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4719, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4720, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4721, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4722, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2990])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4723, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4724, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4725, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4726, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4727, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4728, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4729, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4730, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4731, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4732, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4733, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2991])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4734, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4735, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4736, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4737, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4738, Loss 2.927650\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0010])\n",
      "Epoch 4739, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4740, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4741, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4742, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4743, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2992])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4744, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4745, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4746, Loss 2.927648\n",
      "    Params: tensor([  5.3667, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4747, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4748, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4749, Loss 2.927649\n",
      "    Params: tensor([  5.3667, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4750, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4751, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4752, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4753, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4754, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2993])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4755, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4756, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4757, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4758, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4759, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4760, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4761, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4762, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4763, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4764, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2994])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4765, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4766, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4767, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4768, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4769, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4770, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4771, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4772, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4773, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4774, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4775, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2995])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4776, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4777, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4778, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4779, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4780, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4781, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4782, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4783, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4784, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4785, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2996])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4786, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4787, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4788, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4789, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4790, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4791, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4792, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4793, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4794, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4795, Loss 2.927650\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4796, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4797, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2997])\n",
      "    Grad:   tensor([-0.0002,  0.0009])\n",
      "Epoch 4798, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4799, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4800, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4801, Loss 2.927646\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4802, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4803, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0009])\n",
      "Epoch 4804, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4805, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4806, Loss 2.927647\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4807, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4808, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4809, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4810, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2998])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4811, Loss 2.927648\n",
      "    Params: tensor([  5.3668, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4812, Loss 2.927649\n",
      "    Params: tensor([  5.3668, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4813, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4814, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4815, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4816, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4817, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4818, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4819, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4820, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4821, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4822, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4823, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.2999])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4824, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4825, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4826, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4827, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4828, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4829, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4830, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4831, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4832, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4833, Loss 2.927646\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4834, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4835, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4836, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3000])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4837, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4838, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4839, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4840, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4841, Loss 2.927650\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4842, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4843, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4844, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4845, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4846, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4847, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4848, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4849, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3001])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4850, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4851, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4852, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4853, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4854, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4855, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4856, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4857, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4858, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4859, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4860, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4861, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4862, Loss 2.927645\n",
      "    Params: tensor([  5.3669, -17.3002])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4863, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4864, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4865, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4866, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4867, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4868, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4869, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4870, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4871, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4872, Loss 2.927646\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4873, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4874, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4875, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3003])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4876, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4877, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4878, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4879, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4880, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4881, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0008])\n",
      "Epoch 4882, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4883, Loss 2.927648\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4884, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4885, Loss 2.927647\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4886, Loss 2.927649\n",
      "    Params: tensor([  5.3669, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4887, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4888, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3004])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4889, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4890, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4891, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4892, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4893, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4894, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4895, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4896, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4897, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4898, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4899, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4900, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4901, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3005])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4902, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4903, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4904, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4905, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4906, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4907, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4908, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4909, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4910, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4911, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4912, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4913, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4914, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4915, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3006])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4916, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4917, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4918, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4919, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4920, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4921, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4922, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4923, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4924, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4925, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4926, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4927, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4928, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3007])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4929, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4930, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4931, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4932, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4933, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4934, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4935, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4936, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4937, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4938, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4939, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4940, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4941, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3008])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4942, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4943, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4944, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4945, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4946, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4947, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4948, Loss 2.927649\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4949, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4950, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch 4951, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch 4952, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch 4953, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4954, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4955, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4956, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4957, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4958, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3009])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4959, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch 4960, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4961, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch 4962, Loss 2.927646\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0007])\n",
      "Epoch 4963, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4964, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4965, Loss 2.927647\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4966, Loss 2.927648\n",
      "    Params: tensor([  5.3670, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4967, Loss 2.927646\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch 4968, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch 4969, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4970, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4971, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4972, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4973, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4974, Loss 2.927646\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4975, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3010])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4976, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4977, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch 4978, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch 4979, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4980, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4981, Loss 2.927646\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4982, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4983, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4984, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4985, Loss 2.927646\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4986, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch 4987, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4988, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4989, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4990, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4991, Loss 2.927646\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4992, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4993, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3011])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4994, Loss 2.927646\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch 4995, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch 4996, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4997, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4998, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 4999, Loss 2.927647\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n",
      "Epoch 5000, Loss 2.927648\n",
      "    Params: tensor([  5.3671, -17.3012])\n",
      "    Grad:   tensor([-0.0001,  0.0006])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1119572d0>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3840x2880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "t_p = model(t_un, *params)\n",
    "\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adafactor',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_adafactor',\n",
       " '_functional',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7761, 0.1064], requires_grad=True)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860120\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612900\n",
      "Epoch 1000, Loss 3.086698\n",
      "Epoch 1500, Loss 2.928578\n",
      "Epoch 2000, Loss 2.927646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 2000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6,  2,  7,  4, 10,  3,  9,  1,  0]), tensor([8, 5]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]\n",
    "train_t_un = 0.1 * train_t_u\n",
    "val_t_un = 0.1 * val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, train_t_u, val_t_u,\n",
    "                  train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "\n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch <= 3 or epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Training loss {train_loss.item():.4f},\"\n",
    "                  f\" Validation loss {val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 32.3407, Validation loss 296.4708\n",
      "Epoch 2, Training loss 21.5723, Validation loss 206.2375\n",
      "Epoch 3, Training loss 18.6824, Validation loss 166.0221\n",
      "Epoch 500, Training loss 6.6598, Validation loss 57.5818\n",
      "Epoch 1000, Training loss 3.6123, Validation loss 31.5658\n",
      "Epoch 1500, Training loss 2.7763, Validation loss 21.0658\n",
      "Epoch 2000, Training loss 2.5469, Validation loss 16.4239\n",
      "Epoch 2500, Training loss 2.4840, Validation loss 14.2279\n",
      "Epoch 3000, Training loss 2.4668, Validation loss 13.1422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  4.7232, -14.6063], requires_grad=True)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "training_loop(\n",
    "    n_epochs = 3000,\n",
    "    optimizer = optimizer,\n",
    "    params = params,\n",
    "    train_t_u = train_t_un,\n",
    "    val_t_u = val_t_un,\n",
    "    train_t_c = train_t_c,\n",
    "    val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
